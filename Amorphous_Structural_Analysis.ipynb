{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451afa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# ------ SIMULATION VALIDATOR ------\n",
    "# Before running analysis it's important to check that all simulations have run as expected\n",
    "# 1. Check expected number of dump files\n",
    "# 2. Checks log files for ERRORs and WARNINGs\n",
    "# 3. Check that all expected simulations exist\n",
    "# ----------------------------------\n",
    "\n",
    "expected_number_of_dump_files_per_sim = 96             # =(total number of timesteps + 1)/100\n",
    "expected_densities = [1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5]\n",
    "expected_runs = [1,2,3,4,5,6,7,8,9,10]\n",
    "list_files_with_warnings = False\n",
    "\n",
    "# This expects file structure:\n",
    "# <unique_key>/                     \n",
    "#         └── NVT/                       \n",
    "#               ├── dump_custom.C.00000.dat  \n",
    "#               ├── dump_custom.C.00001.dat\n",
    "\n",
    "def simulation_validator(directory):\n",
    "\n",
    "    directory = Path(directory)  \n",
    "\n",
    "    # Check number of files and file types\n",
    "    errors = set()\n",
    "    warnings = set()\n",
    "    invalid_sims = 0\n",
    "\n",
    "    \n",
    "    for nvt_dir in directory.rglob(\"NVT\"):\n",
    "        if nvt_dir.is_dir():\n",
    "\n",
    "            # All files in the NVT directory\n",
    "            all_files = [f for f in nvt_dir.iterdir() if f.is_file()]\n",
    "            total = len(all_files)\n",
    "\n",
    "            unique_key = nvt_dir.parent.name\n",
    "            # Check number of files matches expected number\n",
    "            if total != expected_number_of_dump_files_per_sim:\n",
    "                print(f\"{total} files found in {unique_key}\")\n",
    "                invalid_sims += 1\n",
    "\n",
    "            # Files that start with 'dump_custom'\n",
    "            unrecognized_files = [f for f in all_files if not f.name.startswith(\"dump_custom\")]\n",
    "            if unrecognized_files:\n",
    "                print(f\"Unrecognized files found in {unique_key}\")\n",
    "                print(unrecognized_files)\n",
    "                invalid_sims += 1\n",
    "    \n",
    "    # Check for ERROR and WARNING messages in .log files\n",
    "    for log_file in directory.rglob(\"*.log\"):\n",
    "        \n",
    "        has_error = False\n",
    "        has_warning = False\n",
    "\n",
    "        unique_key = log_file.parent.name \n",
    "        \n",
    "        with log_file.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                if \"ERROR\" in line:\n",
    "                    has_error = True\n",
    "                    break  # stop reading; error takes priority\n",
    "                elif \"WARNING\" in line:\n",
    "                    has_warning = True\n",
    "\n",
    "        # Decide classification\n",
    "        if has_error:\n",
    "            errors.add(unique_key)\n",
    "            invalid_sims += 1\n",
    "        elif has_warning:\n",
    "            warnings.add(unique_key)\n",
    "\n",
    "    if errors:\n",
    "        print(f\"{len(errors)} Simulations displaying errors\")\n",
    "        for e in sorted(errors):\n",
    "            print(f\"   - {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo simulations displaying errors.\")\n",
    "\n",
    "    if warnings:\n",
    "        print(f\"\\n{len(warnings)} Simulations displaying warnings\")\n",
    "        if list_files_with_warnings:\n",
    "            for w in sorted(warnings):\n",
    "                print(f\"   - {w}\")\n",
    "    else:\n",
    "        print(\"\\nNo simulations displaying warnings.\")\n",
    "\n",
    "    print(f\"\\n{invalid_sims} Invalid simulations\")\n",
    "\n",
    "    \n",
    "    # ------ Missing Simulation Check -----\n",
    "    simulations = []\n",
    "\n",
    "    unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)$'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "\n",
    "    # Generate df of simulations\n",
    "    for unique_key in directory.rglob('*'):\n",
    "\n",
    "        if not unique_key.is_dir(): # must be a directory\n",
    "            continue\n",
    "\n",
    "        # Only matches unique_key dirs\n",
    "        m = unique_key_pattern.fullmatch(unique_key.name)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        element_symbol = m.group(1)\n",
    "        potential_name = m.group(2)\n",
    "        simulation_type = m.group(3)\n",
    "        num_atoms = m.group(4)\n",
    "        density = m.group(5)\n",
    "        run = m.group(6)\n",
    "\n",
    "        simulations.append((element_symbol, potential_name, simulation_type, num_atoms, density, run))\n",
    "\n",
    "    simulations_df = pd.DataFrame(\n",
    "                    simulations,\n",
    "                    columns=['element_symbol', 'potential_name', 'simulation_type', 'num_atoms', 'density', 'run'])\n",
    "\n",
    "    simulations_df = simulations_df.astype({\n",
    "    'num_atoms': 'int64',\n",
    "    'density': 'float64',\n",
    "    'run': 'int64'\n",
    "    })\n",
    "\n",
    "    # Generate expected dataframe\n",
    "    unique_sim_params_df = simulations_df[['element_symbol', 'potential_name', 'simulation_type', 'num_atoms']].drop_duplicates()\n",
    "    expected_sims_df = pd.DataFrame(columns=['element_symbol', 'potential_name', 'simulation_type', 'num_atoms', 'density', 'run'])\n",
    "    \n",
    "    for _, row in unique_sim_params_df.iterrows():\n",
    "        for density in expected_densities:\n",
    "            for run in expected_runs:\n",
    "                expected_sims_df.loc[len(expected_sims_df)] = {'element_symbol': row['element_symbol'],\n",
    "                                                                'potential_name': row['potential_name'],\n",
    "                                                                'simulation_type': row['simulation_type'],\n",
    "                                                                'num_atoms': row['num_atoms'],\n",
    "                                                                'density': density,\n",
    "                                                                'run': run}\n",
    "\n",
    "    expected_sims_df = expected_sims_df.astype({\n",
    "    'num_atoms': 'int64',\n",
    "    'density': 'float64',\n",
    "    'run': 'int64'\n",
    "    })\n",
    "    \n",
    "    # Find diffs between expected and found\n",
    "    diff = simulations_df.merge(expected_sims_df, how = 'outer', indicator=True)\n",
    "    missing_sims = diff[diff['_merge'] == 'right_only']\n",
    "     \n",
    "    if missing_sims.empty:\n",
    "        print(\"\\nNo missing simulations\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nWARNING: {len(missing_sims)} Missing simulation(s) (based on expected number of runs and densities)\")\n",
    "        print(\"\\nMissing Simulations:\")\n",
    "        display(missing_sims)\n",
    "\n",
    "directory_to_validate = \"LAMMPS_simulations/Element: Carbon/Potential: GAP17/Type: NVT/Atoms: 64\"\n",
    "\n",
    "simulation_validator(directory_to_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- ANALYSIS SCRIPT---------------\n",
    "\n",
    "# This script searches the LAMMPS_simulations directory and \n",
    "# 1. Identifies simulations that have not yet been analysed\n",
    "# 2. Creates ovito files and renderings\n",
    "# 3. Runs the following analyses with Ovito:\n",
    "#   a) % of sp, sp2, sp3 environments \n",
    "#   b) Radial Distribution Functions\n",
    "#   c) Potential Energy\n",
    "#   d) Bond Length\n",
    "#   e) Force Magnitude\n",
    "#   f) Angular Distribution functions\n",
    "# 4. Runs the following analyses with MatSciPy\n",
    "#   a) Ring size histogram\n",
    "#   b) n-membered rings against density\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ovito\n",
    "import ovito\n",
    "from ovito.io import import_file\n",
    "from ovito.modifiers import CreateBondsModifier, CoordinationAnalysisModifier, ColorCodingModifier, BondAnalysisModifier\n",
    "from ovito.vis import Viewport, TachyonRenderer, ColorLegendOverlay, BondsVis\n",
    "from ovito.qt_compat import QtCore\n",
    "\n",
    "# MatSciPy\n",
    "from matscipy.rings import ring_statistics\n",
    "from ase.io import read\n",
    "\n",
    "# ------ MAKE NEW DIRECTORIES ------\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "structural_analysis_dir = analysis_dir / \"Amorphous Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "data_dir = structural_analysis_dir / \"Raw Data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ovito_dir = structural_analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(exist_ok=True)\n",
    "# ----------------------------------\n",
    "\n",
    "# ------  IMPORT SIMULATION DATA ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dictionary sorted_imported_simulation_files = {unique_key: [sorted list of dump_file path objects]} \n",
    "# 3. This can be loaded like so: \n",
    "#   a) first item: unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "#   b) loop through all items: for unique_key, dump_files in imported_simulation_files.items():\n",
    "\n",
    "# NOTE: The unique_key is generated from the grandparent of the dumpfiles\n",
    "# This function expects the following file structure, \"dump_custom.C.00000\" regex and unique_key regex:\n",
    "#\n",
    "# <unique_key>/                     \n",
    "#         └── NVT/                       \n",
    "#               ├── dump_custom.C.00000.dat  \n",
    "#               ├── dump_custom.C.00001.dat\n",
    "\n",
    "def import_simulation_data(directory):\n",
    "\n",
    "    dump_file_name = re.compile(r\"^dump_custom\\.C\\.(\\d+)\\.dat$\") # Dump file regex\n",
    "    unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "     \n",
    "    directory = Path(directory)\n",
    "\n",
    "    imported_simulation_files = defaultdict(list) # Imported files dictionary\n",
    "\n",
    "    imported_files_counter = 0\n",
    "    skipped_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "        \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = dump_file_name.match(path.name) # Enforce dump_file file naming\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        parent = path.parent\n",
    "        \n",
    "        if parent.name != \"NVT\": # Enforce NVT file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Parent directory for {path}, {parent} is not equal to NVT\")\n",
    "            continue\n",
    "\n",
    "        grandparent = parent.parent\n",
    "    \n",
    "        if not unique_key_pattern.match(grandparent.name): # Enforce unique_key file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Invalid unique_key name format '{grandparent.name}'\")\n",
    "            continue\n",
    "\n",
    "        if not grandparent.name: # Protect against missing grandparent\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: No grandparent directory for {path}\")\n",
    "            continue\n",
    "\n",
    "        unique_key = grandparent.name\n",
    "        numeric_index = int(m.group(1))\n",
    "\n",
    "        imported_simulation_files[unique_key].append((numeric_index, path))\n",
    "        imported_files_counter += 1\n",
    "\n",
    "    # sort each list by numeric index and drop the numeric index in final structure\n",
    "    sorted_imported_simulation_files = {}\n",
    "\n",
    "    for key, items in imported_simulation_files.items():\n",
    "        items.sort(key=lambda pair: pair[0])  # sort by numeric_index\n",
    "        paths_sorted = [p for _, p in items]\n",
    "        sorted_imported_simulation_files[key] = paths_sorted\n",
    "\n",
    "    if imported_files_counter:\n",
    "        print(f\"Imported {imported_files_counter} dump files\")\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} dump files due to errors\")\n",
    "\n",
    "    return sorted_imported_simulation_files\n",
    "\n",
    "imported_simulation_files = import_simulation_data(\"LAMMPS_simulations\")\n",
    "print(f\"Imported {len(imported_simulation_files)} LAMMPS simulation files\")\n",
    "\n",
    "# Sets up an empty pipeline for each successive function to use\n",
    "def empty_ovito_pipeline(imported_simulation_files):\n",
    "\n",
    "    # Clear existing pipeline\n",
    "    for p in list(ovito.scene.pipelines):\n",
    "        p.remove_from_scene()\n",
    "\n",
    "    if not imported_simulation_files:\n",
    "        raise ValueError(\"No datafiles provided to empty_ovito_pipeline()\")\n",
    "    \n",
    "    # Load the first item in the dictionary \n",
    "    unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "\n",
    "    if not dump_file:\n",
    "        raise ValueError(f\"No dump files found for simulation '{unique_key}'\")\n",
    "    \n",
    "    pipeline = import_file(dump_file)\n",
    "    \n",
    "    return pipeline\n",
    "pipeline = empty_ovito_pipeline(imported_simulation_files)\n",
    "\n",
    "# Data visualisation in Ovito\n",
    "def ovito_analysis(data_dict, pipeline):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided to ovito_analysis()\")\n",
    "\n",
    "    # ------- ANALYSIS OF IMPORTED FILES ------------\n",
    "    # BUG: Image and video renderers error with: \n",
    "    # \"RuntimeError: Visual element 'Rings' reported an error:Failed to build non-periodic representation of periodic surface mesh. Periodic domain might be too small.\" if ring mod is included.\n",
    "\n",
    "    # Bond Modifier and Visuals \n",
    "    bond_modifier = CreateBondsModifier(cutoff=1.85)\n",
    "    bond_modifier.vis.width = 0.15\n",
    "    bond_modifier.vis.coloring_mode = BondsVis.ColoringMode.Uniform\n",
    "    bond_modifier.vis.color = (0.5, 0.5, 0.5)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Coordination Modifier and Colour Coding\n",
    "    pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff=1.85))\n",
    "    colour_coding_mod = ColorCodingModifier(property=\"Coordination\",start_value=1.0,end_value=4.0,gradient=ColorCodingModifier.Viridis(),discretize_color_map=True)\n",
    "    pipeline.modifiers.append(colour_coding_mod)\n",
    "\n",
    "    # Add to Scene\n",
    "    pipeline.add_to_scene()\n",
    "\n",
    "    # Viewing settings\n",
    "    vp = Viewport()\n",
    "    vp.type = Viewport.Type.Perspective\n",
    "\n",
    "    # Coordination Legend\n",
    "    legend = ColorLegendOverlay(\n",
    "        title = \"Coordination\",\n",
    "        modifier = colour_coding_mod,\n",
    "        alignment = QtCore.Qt.AlignmentFlag.AlignHCenter | QtCore.Qt.AlignmentFlag.AlignBottom,\n",
    "        orientation = QtCore.Qt.Orientation.Horizontal,\n",
    "        font_size = 0.1,\n",
    "        format_string = '%.0f' \n",
    "        )\n",
    "    vp.overlays.append(legend)\n",
    "\n",
    "    # Note: this function only renders for the first repeat \n",
    "    def is_run_1_(run_file_name):\n",
    "        return re.match(r\".*1$\", run_file_name) is not None\n",
    "\n",
    "    # Skipped/Created file counters\n",
    "    skipped_ovito_files_counter = 0\n",
    "    skipped_png_files_counter = 0\n",
    "    skipped_avi_files_counter = 0\n",
    "    created_ovito_files_counter = 0\n",
    "    created_png_files_counter = 0\n",
    "    created_avi_files_counter = 0\n",
    "    \n",
    "\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "\n",
    "        m = unique_key_pattern.match(unique_key)\n",
    "        if not m:\n",
    "            print(f\"Invalid unique_key name format: {unique_key}\")\n",
    "            \n",
    "        element_symbol = m.group(1)\n",
    "        potential_name = m.group(2)\n",
    "        simulation_type = m.group(3)\n",
    "        num_atoms = m.group(4)\n",
    "        density = m.group(5)\n",
    "        run = m.group(6)\n",
    "\n",
    "        if element_symbol == \"C\":\n",
    "            element = \"Carbon\"\n",
    "        elif element_symbol == \"Si\":\n",
    "            element = \"Silicon\"\n",
    "        else:\n",
    "            raise ValueError (f\"Unrecognized element_symbol: {element_symbol}. Update symbol --> element mapping\")\n",
    "\n",
    "        # File Name\n",
    "        ovito_file_dir = ovito_dir / f\"Element: {element}\" / f\"Potential: {potential_name}\" / f\"Type: {simulation_type}\" / f\"Atoms: {num_atoms}\" / f\"Density: {density}\" \n",
    "        ovito_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        tachyon = TachyonRenderer(shadows=False, direct_light_intensity=1.1)\n",
    "\n",
    "        # Only does analysis for run_1_\n",
    "        if not is_run_1_(unique_key):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        ovito_save_file = ovito_file_dir / f\"{unique_key}.ovito\"\n",
    "        \n",
    "        # Ovito File Existance-Checker\n",
    "        ovito_exists = any(ovito_dir.rglob(ovito_save_file.name))\n",
    "\n",
    "        if ovito_exists:\n",
    "            skipped_ovito_files_counter += 1\n",
    "            continue\n",
    "        \n",
    "        pipeline.source.load(dump_files)\n",
    "\n",
    "        # Set particle scaling (datafile specific)\n",
    "        n_frames = pipeline.source.num_frames\n",
    "        final_frame = max(0, n_frames - 1)\n",
    "        data = pipeline.compute(frame = final_frame)\n",
    "        data.particles.vis.scaling = 0.3\n",
    "\n",
    "        # Set Zoom\n",
    "        vp.zoom_all()\n",
    "\n",
    "        ovito.scene.save(ovito_save_file)\n",
    "        created_ovito_files_counter += 1   \n",
    "\n",
    "        # Create Images    \n",
    "        img_save_file = ovito_file_dir / f\"{unique_key}.png\"\n",
    "        img_save_file_str = str(img_save_file)\n",
    "        \n",
    "        # Ovito File Existance-Checker\n",
    "        img_exists = any(ovito_dir.rglob(img_save_file.name))\n",
    "\n",
    "        if img_exists:\n",
    "            skipped_png_files_counter += 1\n",
    "            continue\n",
    "        \n",
    "        vp.render_image(size=(1920,1080),\n",
    "                        filename=img_save_file_str,\n",
    "                        background=(1,1,1),\n",
    "                        frame=final_frame,\n",
    "                        renderer=tachyon)\n",
    "        created_png_files_counter += 1 \n",
    "              \n",
    "        # Create Videos\n",
    "        vid_save_file   = ovito_file_dir / f\"{unique_key}.avi\"\n",
    "        vid_save_file_str = str(vid_save_file)                                     \n",
    "\n",
    "        # File Existance-Checker\n",
    "        vid_exists   = any(ovito_dir.rglob(vid_save_file.name))\n",
    "        if vid_exists:\n",
    "            skipped_avi_files_counter += 1\n",
    "            continue\n",
    "\n",
    "        vp.render_anim(size=(1920,1080), \n",
    "                    filename=vid_save_file_str, \n",
    "                    fps=10,\n",
    "                    renderer=tachyon)\n",
    "        created_avi_files_counter += 1  \n",
    "\n",
    "\n",
    "    # Print Skipped/Created files\n",
    "    if skipped_ovito_files_counter:\n",
    "        print(f\"Skipped {skipped_ovito_files_counter} existing .ovito files\")\n",
    "    if skipped_png_files_counter:\n",
    "        print(f\"Skipped {skipped_png_files_counter} existing image files\")\n",
    "    if skipped_avi_files_counter:\n",
    "        print(f\"Skipped {skipped_avi_files_counter} existing .avi files\")\n",
    "    \n",
    "    if created_ovito_files_counter:\n",
    "        print(f\"Created {created_ovito_files_counter} .ovito files\")\n",
    "    if created_png_files_counter:\n",
    "        print(f\"Created {created_png_files_counter} image files\")\n",
    "    if created_avi_files_counter:\n",
    "        print(f\"Created {created_avi_files_counter} .avi files\")\n",
    "    \n",
    "    # Remove modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "\n",
    "# ------ DATA GENERATION FUNCTIONS ------\n",
    "# file_analysis(): \n",
    "#   1. uses imported_simulation_files from import_simulation_data()\n",
    "#   2. uses the pipeline from empty_ovito_pipeline(): no modifiers by default\n",
    "#   3. checks if files already exist in \"Structural Analysis\"\n",
    "#   4. loads each file in datafiles into the existing pipeline\n",
    "#   5. computes a specified data object for the given pipeline on each file and saves to a file name given by the \"unique_key\" + \"data_tag\"\n",
    "#   NOTE:\n",
    "#       a) requires \"data_tag\": e.g. \"bond_length_data.txt\" or \"RDF_data.txt\" (include file suffix, e.g. \".txt\")\n",
    "#       b) \"data_function\" refers to the ovito function that return the desired data object \n",
    "#               e.g. \"data.particles['Coordination']\" or \"data.tables['coordination-rdf'].xy()\" or \"data.particles[\"c_pea\"]\" \n",
    "#       c) requires use of the \"lambda data:\" syntax for creating a throwaway function\n",
    "#               e.g. When calling this func, use \"file_analysis_and_existance_checker(datafiles,\"ring_data\",lambda data: data.tables[\"ring-size-histogram\"].xy())\"\"\n",
    "#   6. Set_frame allows specification of which dump file to use (set_frame = 0 is the first file)\n",
    "#   7. Set_frame_temp is used to name the files based on the temperature in the specified frame\n",
    "\n",
    "analysis_tools = [\"ovito\", \"matscipy\"]\n",
    "def file_analysis(data_dict, pipeline, data_tag, data_function, analysis_tool, set_frame, set_frame_temp):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided\")\n",
    "    \n",
    "    if analysis_tool not in analysis_tools:\n",
    "        raise AttributeError(f\"{analysis_tool} not recognized in analysis_tools\")\n",
    "\n",
    "    # Skipped file counter\n",
    "    skipped_files_counter = 0\n",
    "    created_files_counter = 0\n",
    "\n",
    "    # ----- STRUCTURAL ANALYSIS -----\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        # Check that set_frame is within range\n",
    "        if set_frame > (len(dump_files) -1) or set_frame < 0:\n",
    "            print(f\"Specified frame out of range: 0 - {len(dump_files)-1}\")\n",
    "            return\n",
    "\n",
    "        unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "\n",
    "        m = unique_key_pattern.match(unique_key)\n",
    "        if not m:\n",
    "            print(f\"Invalid unique_key name format: {unique_key}\")\n",
    "            \n",
    "        element_symbol = m.group(1)\n",
    "        potential_name = m.group(2)\n",
    "        simulation_type = m.group(3)\n",
    "        num_atoms = m.group(4)\n",
    "        density = m.group(5)\n",
    "        run = m.group(6)\n",
    "\n",
    "        if element_symbol == \"C\":\n",
    "            element = \"Carbon\"\n",
    "        elif element_symbol == \"Si\":\n",
    "            element = \"Silicon\"\n",
    "        else:\n",
    "            raise ValueError (f\"Unrecognized element_symbol: {element_symbol}. Update symbol --> element mapping\")\n",
    "\n",
    "        # File Name\n",
    "        data_file_dir = data_dir / f\"Element: {element}\" / f\"Potential: {potential_name}\" / f\"Type: {simulation_type}\" / f\"Atoms: {num_atoms}\" / f\"Density: {density}\" / f\"Run: {run}\"\n",
    "        data_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "        data_file_name = data_file_dir / f\"{unique_key}_{set_frame_temp}K_{data_tag}\"\n",
    "\n",
    "        # Structural Analysis File Existance-Checker\n",
    "        data_exists = any(data_dir.rglob(data_file_name.name))\n",
    "        if data_exists and not REPLACE_OLD_FILES:\n",
    "            skipped_files_counter += 1\n",
    "            continue \n",
    "\n",
    "        # Read files based on which tool is being used:\n",
    "        # ovito favours LAMMPS\n",
    "        if analysis_tool == \"ovito\":\n",
    "\n",
    "            # Load new file into the pipeline and compute data for the specified frame        \n",
    "            pipeline.source.load(dump_files)\n",
    "            data = pipeline.compute(frame = set_frame)\n",
    "        \n",
    "        # matscipy favours ase\n",
    "        if analysis_tool == \"matscipy\":\n",
    "\n",
    "            # Load new file into ase\n",
    "            data = read(dump_files[set_frame], format=\"lammps-dump-text\")\n",
    "\n",
    "        # Data function\n",
    "        specific_data = data_function(data)\n",
    "        np.savetxt(data_file_name, specific_data, delimiter=\",\", fmt=\"%.6f\")\n",
    "        created_files_counter += 1\n",
    "\n",
    "    # Print Skipped/Created Files\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} existing {data_tag} files\")\n",
    "    if created_files_counter:\n",
    "        print(f\"Created {created_files_counter} {data_tag} files\")    \n",
    "\n",
    "# ------ Ovito Analysis ------\n",
    "def coordination_analysis(data_dict, pipeline, coordination_cutoff, set_frame, set_frame_temp):\n",
    "    \n",
    "    # Coordination Analysis Modfier\n",
    "    coord_mod = CoordinationAnalysisModifier(cutoff=coordination_cutoff)\n",
    "    pipeline.modifiers.append(coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"coordination.txt\", \n",
    "                  lambda data: data.particles['Coordination'], \n",
    "                  \"ovito\", set_frame, set_frame_temp)\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()\n",
    "    \n",
    "def energy_analysis(data_dict, pipeline, set_frame, set_frame_temp):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"potential_energy.txt\", \n",
    "                  lambda data: data.particles[\"c_pea\"], \n",
    "                  \"ovito\", set_frame, set_frame_temp)\n",
    "\n",
    "def RDF_analysis(data_dict, pipeline, RDF_cutoff, bins, set_frame, set_frame_temp):\n",
    "    \n",
    "    # Coordination Analysis Modfier for RDF\n",
    "    RDF_coord_mod = CoordinationAnalysisModifier(cutoff=RDF_cutoff, number_of_bins=bins)\n",
    "    pipeline.modifiers.append(RDF_coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"RDF.txt\", \n",
    "                  lambda data: data.tables['coordination-rdf'].xy(), \n",
    "                  \"ovito\", set_frame, set_frame_temp)\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()    \n",
    "\n",
    "def bond_length_analysis(data_dict, pipeline, bins, bond_length, \n",
    "                         bond_length_analysis_cutoff, set_frame, set_frame_temp):\n",
    "\n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Bond Analysis Modifier\n",
    "    bond_analysis_mod = BondAnalysisModifier(bins = bins, length_cutoff=bond_length_analysis_cutoff)\n",
    "    pipeline.modifiers.append(bond_analysis_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"bond_length.txt\", \n",
    "                  lambda data: data.tables[\"bond-length-distr\"].xy(), \n",
    "                  \"ovito\", set_frame, set_frame_temp)\n",
    "  \n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    " \n",
    "def force_analysis(data_dict, pipeline, set_frame, set_frame_temp):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"forces.txt\", \n",
    "                  lambda data: data.particles[\"Force\"],\n",
    "                  \"ovito\", set_frame, set_frame_temp)\n",
    "\n",
    "def bond_angle_analysis(data_dict, pipeline, bins, bond_length, set_frame, set_frame_temp):\n",
    "    \n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Bond Analysis Modifier\n",
    "    bond_analysis_mod = BondAnalysisModifier(bins = bins)\n",
    "    pipeline.modifiers.append(bond_analysis_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"bond_angle.txt\", \n",
    "                  lambda data: data.tables[\"bond-angle-distr\"].xy(),\n",
    "                    \"ovito\", set_frame, set_frame_temp)\n",
    "  \n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "# --------------------------------------------\n",
    "\n",
    "# ------ MatSciPy Analysis ------\n",
    "def ring_analysis(data_dict, max_ring_size, bond_length, set_frame, set_frame_temp):\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"ring.txt\", \n",
    "                  lambda data: ring_statistics(data, cutoff=bond_length, maxlength=max_ring_size), \n",
    "                  \"matscipy\", set_frame, set_frame_temp)\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will regenerate ALL files (apart from renders)\n",
    "REPLACE_OLD_FILES = True\n",
    "\n",
    "if REPLACE_OLD_FILES:\n",
    "    confirm = input(\"Are you sure you want to replace old files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "final_frame = 95\n",
    "final_temp = 300\n",
    "\n",
    "final_liquid_frame = 60\n",
    "liquid_temp = 5000\n",
    "\n",
    "force_analysis(imported_simulation_files, pipeline, final_frame, final_temp)\n",
    "\n",
    "bond_length_analysis(imported_simulation_files, pipeline, bins=1000, \n",
    "                     bond_length = 1.85, bond_length_analysis_cutoff=2.0,\n",
    "                     set_frame=final_frame, set_frame_temp=final_temp)\n",
    "\n",
    "RDF_analysis(imported_simulation_files, pipeline, \n",
    "             RDF_cutoff=6.0, bins=200, \n",
    "             set_frame=final_frame, set_frame_temp=final_temp)\n",
    "\n",
    "RDF_analysis(imported_simulation_files, pipeline, \n",
    "             RDF_cutoff=6.0, bins=200, set_frame=final_liquid_frame, \n",
    "             set_frame_temp=liquid_temp)\n",
    "\n",
    "ring_analysis(imported_simulation_files, max_ring_size=24, \n",
    "              bond_length=1.85, set_frame=final_frame, \n",
    "              set_frame_temp=final_temp)\n",
    "\n",
    "coordination_analysis(imported_simulation_files, pipeline, \n",
    "                      coordination_cutoff=1.85, set_frame=final_frame, \n",
    "                      set_frame_temp=final_temp)\n",
    "\n",
    "energy_analysis(imported_simulation_files, pipeline,\n",
    "                set_frame=final_frame, set_frame_temp=final_temp)\n",
    "\n",
    "bond_angle_analysis(imported_simulation_files, pipeline, bins=40, \n",
    "                    bond_length = 1.85, set_frame=final_frame, \n",
    "                    set_frame_temp=final_temp)\n",
    "\n",
    "bond_angle_analysis(imported_simulation_files, pipeline, bins=40, \n",
    "                    bond_length = 1.85, set_frame=final_liquid_frame, \n",
    "                    set_frame_temp=liquid_temp)\n",
    "\n",
    "ovito_analysis(imported_simulation_files, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ GRAPHICAL ANALYSIS --------\n",
    "\n",
    "# Graphical data points are means of all repeat runs with errors given as 1 standard deviation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create Graphical Analysis Directories\n",
    "from pathlib import Path\n",
    "\n",
    "# ------ FIGURE FORMATTING ------\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('1_column_fig.mplstyle')\n",
    "# -------------------------------\n",
    "\n",
    "# ------ IMPORT DATA FILES ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dataframe: \n",
    "#    imported_data_files_df, with columns:\n",
    "#        element, potential, simulation type, atoms, density, run number, data tag, file_data\n",
    "# 3. Only includes files with the specified \"data_tag\" \n",
    "\n",
    "def import_data_files(directory, data_tag):\n",
    "    \n",
    "    directory = Path(directory)\n",
    "\n",
    "    # Regex pattern for reading \"unique_key\" + \"data_tag\"\"\n",
    "    data_file_name = re.compile(\n",
    "    r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "    r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "    r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "    r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "    r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "    r'(?P<run>\\d+)_'                                # e.g. 1 (run number)\n",
    "    r'(?P<temperature>\\d+)K'                       # e.g. 300 or 5000\n",
    "    r'(?:_(?P<data_tag>.+)|(?P<data_tag2>\\..+))$'   # e.g. ring.txt or .png (allows underscore after run_number or .avi etc...)   \n",
    "    )  \n",
    "\n",
    "    imported_data_files_rows = []\n",
    "\n",
    "    skipped_data_files_counter = 0\n",
    "    imported_data_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "    \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = data_file_name.match(path.name) # Enforce data file naming\n",
    "        if not m:\n",
    "            print(f\"ERROR: Skipped {path.name}. Invalid data file name\")\n",
    "            skipped_data_files_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Parse \"unique_key\" + \"data_tag\" components\n",
    "        element_symbol    = m.group(\"element_symbol\")\n",
    "        potential_name    = m.group(\"potential_name\")\n",
    "        simulation_type   = m.group(\"simulation_type\")\n",
    "        num_atoms         = m.group(\"num_atoms\")\n",
    "        density           = m.group(\"density\")\n",
    "        run_number        = m.group(\"run\")\n",
    "        temperature       = m.group(\"temperature\")\n",
    "        file_data_tag     = (m.group(\"data_tag\") or m.group(\"data_tag2\") or \"\").lstrip(\"._\")\n",
    "\n",
    "\n",
    "        # Only import files with the correct data_tag\n",
    "        if file_data_tag != data_tag:\n",
    "            continue\n",
    "        \n",
    "        # Load data from each file\n",
    "        try:\n",
    "            file_data = np.loadtxt(path, delimiter = ',')\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to load {path}: {e}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Check that data files aren't empty\n",
    "        if file_data is None or getattr(file_data, \"size\", 0) == 0:\n",
    "            print(f\"No data found in {path}\")\n",
    "            continue\n",
    "\n",
    "        imported_data_files_rows.append({\"element\": element_symbol, \"potential\" : potential_name,\n",
    "                                          \"simulation_type\": simulation_type, \"num_atoms\": num_atoms, \n",
    "                                          \"density\": density, \"run_number\": run_number, \n",
    "                                          \"temperature\": temperature,\n",
    "                                          \"data_tag\": file_data_tag, \"file_data\": file_data})\n",
    "        \n",
    "        imported_data_files_counter += 1\n",
    "        \n",
    "    imported_data_files_df = pd.DataFrame(imported_data_files_rows)\n",
    "\n",
    "    print()\n",
    "    if imported_data_files_counter:\n",
    "        print(f\"Imported {imported_data_files_counter} {data_tag} files\")\n",
    "    if skipped_data_files_counter:\n",
    "        print(f\"Skipped {skipped_data_files_counter} {data_tag} files\")\n",
    "    \n",
    "    return imported_data_files_df\n",
    "#-------------------------------\n",
    "\n",
    "# ------ DATA ANALYSIS ----------\n",
    "# 1. Imports data using import_data_files()\n",
    "# 2. If independent_var=False: independent variable is defaulted to float(density)\n",
    "#       a) Applies unique_data_function to file_data to generate scalar values\n",
    "#       b) Returns dataframe with scalar values in column: dep_var\n",
    "#       c) Transforms density column into column: ind_var\n",
    "#    If independent_var=True:\n",
    "#       a) Creates 2 columns from file_data: ind_var, dep_var (explodes arrays into scalar values)\n",
    "#       b) If the array is 1D (in the case of MatSciPy ring data), the index is taken as the ind_var\n",
    "# 4. Takes the mean and std of dep_var for each run number\n",
    "# 5. Returns analysed_data_files_df with 'mean', 'std' columns for dep var\n",
    "\n",
    "# Note: Assumes no analysis required when both independent and dependant variables are imported\n",
    "\n",
    "def data_analysis(df, unique_data_function, independent_var):\n",
    "\n",
    "    # Analyse data files\n",
    "    analysed_data_files_df = df\n",
    "\n",
    "    # For independent_var=True, explode the arrays into scalar values \n",
    "\n",
    "    if independent_var:\n",
    "        # Split file_data into ind_var and dep_var\n",
    "        pieces = []\n",
    "        meta_cols = ['element','potential','simulation_type','num_atoms','density','data_tag','run_number', 'temperature']\n",
    "\n",
    "        # Check if the data is 2D or 1D\n",
    "        for _, row in analysed_data_files_df.iterrows():\n",
    "            \n",
    "            arr = np.asarray(row['file_data'])\n",
    "            \n",
    "            # Skip empty arrays\n",
    "            if arr.size == 0:\n",
    "                continue  \n",
    "            \n",
    "            # Add an index to 1D arrays before expanding into 2 columns (for matsci ring files)\n",
    "            # The index == ring size\n",
    "            if arr.ndim == 1:\n",
    "                arr = np.column_stack((np.arange(arr.size), arr))\n",
    "\n",
    "            # Verify that 1D array has been converted to 2D, or input file was originally 2D\n",
    "            if arr.ndim != 2 or arr.shape[1] != 2:\n",
    "                raise ValueError(\"Imported dataframe not recognized as accepted 1D or 2D. Check data shape\"\n",
    "                                 \"\\n If importing 1D data, verify that the data is indexable (this generates the second column\")\n",
    "\n",
    "            piece = pd.DataFrame(arr, columns=['ind_var', 'dep_var'])\n",
    "            \n",
    "            # Attatch other column data\n",
    "            for k in meta_cols:\n",
    "                piece[k] = row[k]\n",
    "                \n",
    "            pieces.append(piece)\n",
    "\n",
    "        if not pieces:\n",
    "            print(f\"No valid arrays to explode for {analysed_data_files_df['data_tag']}\")\n",
    "            return None\n",
    "\n",
    "        # concatenate into one DataFrame\n",
    "        analysed_data_files_df = pd.concat(pieces, ignore_index=True)\n",
    "  \n",
    "    \n",
    "    else:\n",
    "        # Perform unique_data_function to generate scalar values\n",
    "        try:\n",
    "            analysed_data_files_df['dep_var'] = analysed_data_files_df['file_data'].apply(unique_data_function)\n",
    "             # Ind_var is a copy of density by default (formated as float)\n",
    "            analysed_data_files_df['ind_var'] = analysed_data_files_df['density'].copy().astype(float)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed unique_data_function for {analysed_data_files_df['data_tag']}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Find mean/std of each scalar value in dep_var\n",
    "    # Groups columns by simulation runs\n",
    "    grouping_keys = [\"element\",\"potential\",\"simulation_type\",\"num_atoms\",\"density\", \"data_tag\", \"temperature\", \"ind_var\"]\n",
    "    grouped_runs = [c for c in grouping_keys if c in analysed_data_files_df.columns]\n",
    "    if not grouped_runs:\n",
    "        raise ValueError(\"No grouping keys found in imported DataFrame. Check import_data_files output.\")\n",
    "\n",
    "    analysed_data_files_df = analysed_data_files_df.groupby(grouped_runs).agg(mean=('dep_var', 'mean'),\n",
    "                                                                            std=('dep_var', 'std')\n",
    "                                                                            ).reset_index()\n",
    "\n",
    "    return analysed_data_files_df \n",
    "# ---------------------------------\n",
    "\n",
    "# ------ PLOT ------ \n",
    "\n",
    "# Additional file tag: for adding sp/sp2/sp3 or 5/6/7 ring labels\n",
    "def single_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag):\n",
    "\n",
    "    def make_single_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path):\n",
    "\n",
    "        graph_save_path = Path(graph_save_path)\n",
    "        \n",
    "        # Skip if file exists\n",
    "        if graph_save_path.exists():\n",
    "            if OVERWRITE_GRAPH:\n",
    "                graph_save_path.unlink()\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        # Local figure size for each plot            \n",
    "        fig, ax = plt.subplots()  \n",
    "        x, mean, std = group[\"ind_var\"], group[\"mean\"], group[\"std\"]\n",
    "\n",
    "\n",
    "        if plot_type == \"marker\":\n",
    "            ax.errorbar(x, mean, yerr=std, capthick=0.5, elinewidth=0.5)\n",
    "        elif plot_type == \"line\":\n",
    "            alpha_fill = 0.25\n",
    "            ax.plot(x, mean, label=\"Mean\", marker = \"\")\n",
    "            ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MaxNLocator(number_of_x_axis_ticks))\n",
    "\n",
    "        ax.set(xlabel=x_label, ylabel=y_label, title=chart_title)\n",
    "\n",
    "        # Save Plot \n",
    "        fig.savefig(graph_save_path)\n",
    "        plt.close(fig)  # Close figure to free memory\n",
    "\n",
    "        print(f\"{graph_save_path.name} created\")\n",
    "        return True\n",
    "\n",
    "    skipped_graphs = 0\n",
    "\n",
    "    if independent_var:\n",
    "        # Group by unique_data_key including density for each plot\n",
    "        group_cols = ['element','potential','simulation_type','num_atoms', 'density', 'data_tag', \"temperature\"]\n",
    "        for (e,p,s,n,d,da,t), group in df.groupby(group_cols):\n",
    "            \n",
    "            # Only analyse key analysis densities\n",
    "            if float(d) not in key_analysis_densities:\n",
    "                continue\n",
    "\n",
    "            # Naming and folder\n",
    "            # File Name\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = single_plot_dir / f\"Element: {e}\" / f\"Potential: {p}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / f\"Plot Type: {da}\" / f\"Temperature: {t}\"\n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{d}_{t}K_{da}.{set_global_file_format}\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{d}_{t}K_{da}_{additional_file_tag}.{set_global_file_format}\"\n",
    "            \n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            # Add density and temp to the chart title\n",
    "            d_chart_title = f\"{chart_title} {d} g/cm³ {t} K\"\n",
    "\n",
    "            make_graph = make_single_plot(group, plot_type, x_label, y_label, d_chart_title, graph_save_path)\n",
    "            if not make_graph:\n",
    "                skipped_graphs += 1\n",
    "    \n",
    "    else:\n",
    "        # Group by unique_data_key excluding density for each vs. density plot\n",
    "        group_cols = ['element','potential','simulation_type','num_atoms','data_tag', \"temperature\"]\n",
    "        for (e,p,s,n,da,t), group in df.groupby(group_cols):\n",
    "\n",
    "            # Naming and folder\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = single_plot_dir / f\"Element: {e}\" / f\"Potential: {p}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / f\"Plot Type: Density\" / f\"Temperature: {t}\"\n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{t}K_{da}.{set_global_file_format}\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{t}K_{da}_{additional_file_tag}.{set_global_file_format}\" \n",
    "\n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            make_graph = make_single_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path)\n",
    "            if not make_graph:\n",
    "                skipped_graphs += 1\n",
    "    \n",
    "    if skipped_graphs:\n",
    "        print(f\"Skipped single plots: {skipped_graphs}\")\n",
    "\n",
    "def potential_comparison_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag):\n",
    "\n",
    "    specific_potential_comparison_dir_name = \"_\".join(potential_comparison_list)    \n",
    "\n",
    "    def make_comparison_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path):\n",
    "        for potential, subdf in group.groupby('potential'):\n",
    "\n",
    "            graph_save_path = Path(graph_save_path)\n",
    "            \n",
    "            # Skip if file exists\n",
    "            if graph_save_path.exists():\n",
    "                if OVERWRITE_GRAPH:\n",
    "                    graph_save_path.unlink()\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "            # Only use potentials in potential_comparison_list\n",
    "            if potential not in potential_comparison_list:\n",
    "                continue\n",
    "\n",
    "            x, mean, std = subdf[\"ind_var\"],subdf[\"mean\"], subdf[\"std\"]\n",
    "\n",
    "            if plot_type == \"marker\":\n",
    "                ax.errorbar(x, mean, yerr=std, capthick=0.5, elinewidth=0.5, label = potential)\n",
    "            elif plot_type == \"line\":\n",
    "                alpha_fill = 0.15\n",
    "                ax.plot(x, mean, label=potential, marker = \"\")\n",
    "                ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "        ax.set(xlabel=x_label, ylabel=y_label, title=chart_title)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save Plot \n",
    "        fig.savefig(graph_save_path)\n",
    "        plt.close(fig)  # Close figure to free memory\n",
    "        print(f\"{graph_save_path.name} created\")\n",
    "        return True\n",
    "        \n",
    "\n",
    "    potentials_required = set(potential_comparison_list)\n",
    "\n",
    "    skipped_graphs = 0\n",
    "    \n",
    "    if independent_var:\n",
    "\n",
    "        # Group by unique_data_key including density and potential for each plot\n",
    "        group_cols = ['element','simulation_type','num_atoms', 'density', 'data_tag', 'temperature']\n",
    "        for (e,s,n,d,da,t), group in df.groupby(group_cols):\n",
    "\n",
    "            # Only analyse key analysis densities\n",
    "            if float(d) not in key_analysis_densities: \n",
    "                continue\n",
    "\n",
    "            # Only compare plots if there is data for all specified densities\n",
    "            available = set(group['potential'].unique())\n",
    "            if not potentials_required.issubset(available):\n",
    "                continue\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(number_of_x_axis_ticks))\n",
    "\n",
    "            # Naming and folder\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = potential_comparison_dir / f\"Element: {e}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / specific_potential_comparison_dir_name / f\"Plot Type: {da}\"/  f\"Temperature: {t}\"  \n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{d}_{t}K_{da}_{specific_potential_comparison_dir_name}.{set_global_file_format}\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{d}_{t}K_{da}_{additional_file_tag}_{specific_potential_comparison_dir_name}.{set_global_file_format}\"\n",
    "            \n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            # Add density to the chart title\n",
    "            d_chart_title = f\"{chart_title} {d} g/cm³ {t} K\"\n",
    "\n",
    "            make_graph = make_comparison_plot(group, plot_type, x_label, y_label, d_chart_title, graph_save_path)\n",
    "\n",
    "            if not make_graph:\n",
    "                plt.close(fig)\n",
    "                skipped_graphs += 1\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Group by unique_data_key excluding density and potential for each vs. density plot\n",
    "        group_cols = ['element','simulation_type','num_atoms','data_tag', 'temperature']\n",
    "        for (e,s,n,da,t), group in df.groupby(group_cols):\n",
    "\n",
    "            # Only compare plots if there is data for all specified densities\n",
    "            available = set(group['potential'].unique())\n",
    "            if not potentials_required.issubset(available):\n",
    "                continue\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(number_of_x_axis_ticks))\n",
    "            \n",
    "            # Naming and folder\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = potential_comparison_dir / f\"Element: {e}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / specific_potential_comparison_dir_name / f\"Plot Type: Density\" /  f\"Temperature: {t}\"\n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{t}K_{da}_{specific_potential_comparison_dir_name}.{set_global_file_format}\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{t}K_{da}_{additional_file_tag}_{specific_potential_comparison_dir_name}.{set_global_file_format}\"\n",
    "            \n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            make_graph = make_comparison_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path)\n",
    "            \n",
    "            if not make_graph:\n",
    "                plt.close(fig)\n",
    "                skipped_graphs += 1\n",
    "    \n",
    "    if skipped_graphs:\n",
    "        print(f\"Skipped potential comparison plots: {skipped_graphs}\")\n",
    "# ------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Instructions: \n",
    "# 1. Assign data_tag, chat_title, save_file_name (RAW TEXT ONLY, NO PATHS), and y_label \n",
    "# 2. Create a function that returns a scalar from a given structure file (e.g. mean bond length))\n",
    "# 3. Call import_analyse_plot, using your new function as its unique_data_function\n",
    "# 4. Set independent_var=True to plot for only 1 density (e.g. RDF, ring histogram, bond angle distribution)\n",
    "#   a) This requires both an independent and dependant variable, sometimes included in structural_analysis_file, otherwise must be generated\n",
    "#   b) Default generation for a 1D numpy array returns the array index as the independent variable\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function Wrapper\n",
    "def import_analyse_plot(directory, data_tag, unique_data_function, \n",
    "                        plot_type, x_label, y_label, chart_title, \n",
    "                        independent_var, additional_file_tag):\n",
    "    \n",
    "    # Import Data Files\n",
    "    df = import_data_files(directory, data_tag)\n",
    "    if df.empty:\n",
    "        print(f\"No data files imported for data_analysis()\")\n",
    "        return None\n",
    "\n",
    "    # Analyse Data Files\n",
    "    df = data_analysis(df, unique_data_function, independent_var)\n",
    "\n",
    "    # Plot\n",
    "    single_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag)\n",
    "    potential_comparison_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag)\n",
    "\n",
    "# Coordination analysis \n",
    "def coordination_analysis(directory, coordination_number):\n",
    "    \n",
    "    # Label coordination number \n",
    "    mapping = {\n",
    "        2: (\"sp\", \"sp Carbon Proportion\"),\n",
    "        3: (\"sp2\", \"sp2 Carbon Proportion\"),\n",
    "        4: (\"sp3\", \"sp3 Carbon Proportion\")\n",
    "    }\n",
    "    env, y_label = mapping.get(coordination_number, (None, None))\n",
    "\n",
    "    if env is None:\n",
    "        print(\"ERROR: Coordination number should be between 2 and 4\")\n",
    "        env = f\"{coordination_number}_coordinate\"\n",
    "        y_label = f\"{coordination_number} coordinate atoms\"\n",
    "        \n",
    "    data_tag = \"coordination.txt\"\n",
    "    chart_title = f\"Coordination vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    \n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def coord_function(data: np.ndarray):\n",
    "        return float((np.count_nonzero(data == coordination_number) / data.size))\n",
    "\n",
    "    import_analyse_plot(directory, data_tag, coord_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False, additional_file_tag= env)\n",
    "\n",
    "# Ring Size analysis\n",
    "def ring_analysis(directory, ring_size):\n",
    "        \n",
    "    data_tag = \"ring.txt\"\n",
    "    chart_title = f\"Number of {ring_size} Membered Rings vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = f\"{ring_size} Membered Rings\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def ring_function(data: np.ndarray):\n",
    "        if ring_size < len(data):\n",
    "            return float(data[ring_size])\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, ring_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False, additional_file_tag=ring_size)\n",
    "   \n",
    "# Potential energy analysis \n",
    "def potential_energy_analysis(directory):\n",
    "\n",
    "    data_tag = \"potential_energy.txt\"\n",
    "    chart_title = f\"Mean Potential Energy vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = 'Mean Potential Energy (eV)'\n",
    "    \n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def PE_function(data: np.ndarray):\n",
    "        \n",
    "        return np.mean(data)\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, PE_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False, additional_file_tag=None)\n",
    "\n",
    "# Bond Length analysis\n",
    "def bond_length_analysis(directory):\n",
    "    \n",
    "    data_tag = \"bond_length.txt\"\n",
    "    chart_title = f\"Mean Bond Length vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = 'Mean Bond Length (Å)'\n",
    "\n",
    "    def bond_length_function (data: np.array):\n",
    "        return np.average(data[:, 0], weights=data[:, 1])\n",
    "\n",
    "    import_analyse_plot(directory, data_tag, bond_length_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False,additional_file_tag=None)\n",
    "\n",
    "# Force Analysis\n",
    "def force_analysis(directory):\n",
    "        \n",
    "    data_tag = \"forces.txt\"\n",
    "    chart_title = \"Mean Force Magnitude vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = \"Mean Force Magnitude (eV/Å)\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def force_function(data: np.ndarray):\n",
    "        return np.mean(np.linalg.norm(data, axis=1))\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, force_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False,additional_file_tag=None)\n",
    "\n",
    "# Bond Angle Analysis\n",
    "def bond_angle_analysis(directory):\n",
    "    \n",
    "    data_tag = \"bond_angle.txt\"\n",
    "    chart_title = f\"Mean Bond Angle\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = \"Degrees (°)\"\n",
    "\n",
    "    def bond_angle_function (data: np.array):\n",
    "        return np.average(data[:, 0], weights=data[:, 1])\n",
    "\n",
    "    import_analyse_plot(directory, data_tag, bond_angle_function, \n",
    "                    \"line\", x_label, y_label, chart_title, \n",
    "                    independent_var=False, additional_file_tag=None)\n",
    "\n",
    "# RDF Analysis\n",
    "def RDF_analysis(directory):\n",
    "    \n",
    "    data_tag = \"RDF.txt\"\n",
    "    chart_title = \"Radial Distribution Function\"\n",
    "    y_label = \"g(r)\"\n",
    "    x_label = \"r (Å)\"\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, None, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=True, additional_file_tag=None)\n",
    "\n",
    "# Ring Histogram Analysis\n",
    "def ring_size_distribution_analysis(directory):\n",
    "    \n",
    "    data_tag = \"ring.txt\"\n",
    "    chart_title = \"Ring Size Distribution\"\n",
    "    y_label = \"Frequency\"\n",
    "    x_label = \"Ring Size\"\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, None, \n",
    "                        \"marker\", x_label, y_label, chart_title, \n",
    "                        independent_var=True,additional_file_tag=None)\n",
    "\n",
    "# Bond Angle distribution analysis\n",
    "def bond_angle_distribution_analysis(directory):\n",
    "\n",
    "    data_tag = \"bond_angle.txt\"\n",
    "    chart_title = f\"Bond Angle Distribution\"\n",
    "    x_label = \"Degrees (°)\"\n",
    "    y_label = \"Frequency\"\n",
    "\n",
    "    import_analyse_plot(directory, data_tag, None, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=True,additional_file_tag=None)\n",
    "\n",
    "# Wrapper for all functions \n",
    "# (excluding energy/forces becuase these conflate the structural and energetic differences between the models)\n",
    "def graphical_analysis_wrapper(OVERWRITE_GRAPH, set_data_dir):\n",
    "\n",
    "    # AttributeError: 'NoneType' object has no attribute 'groupby' \n",
    "    #   From mis-assigning independent-var = False when it is actually true\n",
    "\n",
    "    if OVERWRITE_GRAPH:\n",
    "        confirm = input(\"Are you sure you want to replace graphs? (y/n): \").strip().lower()\n",
    "        if confirm != \"y\":\n",
    "            OVERWRITE_GRAPH = False\n",
    "\n",
    "    for i in range (2,5):\n",
    "        coordination_analysis(set_data_dir, coordination_number=i)\n",
    "\n",
    "    bond_length_analysis(set_data_dir)\n",
    "\n",
    "    # potential_energy_analysis(set_data_dir)\n",
    "\n",
    "    for i in range (5,8):\n",
    "        ring_analysis(set_data_dir, ring_size=i)\n",
    "\n",
    "    # force_analysis(set_data_dir\n",
    "\n",
    "    RDF_analysis(set_data_dir)\n",
    "\n",
    "    ring_size_distribution_analysis(set_data_dir)\n",
    "\n",
    "    bond_angle_distribution_analysis(set_data_dir)\n",
    "\n",
    "    bond_angle_analysis(set_data_dir)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------- ANALYSIS PARAMETERS ------------------------------------\n",
    "all_potentials = [\"GAP17\", \"mace-mp-0b3\", \"mace-mpa-0\", \"mace-omat-0\"]\n",
    "\n",
    "# List the potentials to compare (Will not attempt to make graphs for non-existant potentials)\n",
    "potential_comparison_list = [\"GAP17\", \"mace-mp-0b3\", \"mace-mpa-0\", \"mace-omat-0\"]\n",
    "\n",
    "# List the densities in analyse (Will not attempt to make graphs for non-existant densities)\n",
    "key_analysis_densities = [1.5, 2.0, 2.5, 3.0, 3.5]\n",
    "\n",
    "number_of_x_axis_ticks = len(key_analysis_densities) # Set number of x axis ticks \n",
    "\n",
    "OVERWRITE_GRAPH = False\n",
    "\n",
    "# File format: png for viewing, pdf for saving and publishing\n",
    "set_global_file_format = \"pdf\"\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------ FILE STORAGE ----------------------------------------\n",
    "cwd = Path.cwd()\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "structural_analysis_dir = analysis_dir / \"Amorphous Structural Analysis\"\n",
    "graphs_dir = structural_analysis_dir / \"Graphs\"\n",
    "graph_dir = graphs_dir / f\"{set_global_file_format} Graphs\"\n",
    "single_plot_dir = graph_dir / \"Single Plots\"\n",
    "potential_comparison_dir = graph_dir / \"Potential Comparison Plots\"\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "graphical_analysis_wrapper(OVERWRITE_GRAPH, data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structure_analyser_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
