{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb0dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470 LAMMPS simulation files imported\n",
      "Skipped 470 existing bond_length files\n",
      "Skipped 470 existing RDF files\n",
      "Skipped 470 existing ring files\n",
      "Skipped 470 existing coordination files\n",
      "Skipped 470 existing potential_energy files\n"
     ]
    }
   ],
   "source": [
    "#--------- ANALYSIS SCRIPT---------------\n",
    "\n",
    "# This script searches the LAMMPS_simulations directory and \n",
    "# 1. Identifies simulations that have not yet been analysed\n",
    "# 2. Creates ovito files and renderings\n",
    "# 3. Runs the following analyses as functions of density or cooling rate:\n",
    "#   a) % of sp, sp2, sp3 environments \n",
    "#   c) Histogram of ring sizes for a given density\n",
    "#   d) Frequency plots for n-mem rings\n",
    "#   e) Radial Distribution Functions\n",
    "#   f) Potential Energy\n",
    "#   g) Bond Length\n",
    "\n",
    "# # -------- OPTIONAL WAIT FUNCTION TO ALLOW FOR AUTOMATED RUNNING ---------\n",
    "# import subprocess\n",
    "# import time\n",
    "# WAIT_TIME = 600  # seconds\n",
    "# USER = \"scat9451\"\n",
    "\n",
    "# while True:\n",
    "#     job_status = subprocess.run([\"qstat\", \"-u\", USER], capture_output=True, text=True)\n",
    "#     if not job_status.stdout.strip():  # empty means no jobs\n",
    "#         break\n",
    "#     print(\"Active jobs found - waiting ...\")\n",
    "#     time.sleep(WAIT_TIME)\n",
    "\n",
    "# print(\"No active jobs found - proceeding ...\")\n",
    "# # ---------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import ovito\n",
    "from ovito.io import import_file\n",
    "from ovito.modifiers import CreateBondsModifier, FindRingsModifier, CoordinationAnalysisModifier, ColorCodingModifier, BondAnalysisModifier\n",
    "from ovito.vis import Viewport, TachyonRenderer, ColorLegendOverlay, BondsVis\n",
    "from ovito.qt_compat import QtCore\n",
    "\n",
    "# ------ MAKE NEW DIRECTORIES ------\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ovito_dir = analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(exist_ok=True)\n",
    "\n",
    "structural_analysis_dir = analysis_dir / \"Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(exist_ok=True)\n",
    "# ----------------------------------\n",
    "\n",
    "# ------  IMPORT SIMULATION DATA ------\n",
    "# Searches through the specified directory and:\n",
    "# Returns \"datafiles\" which is a list of 2 element tuples; \n",
    "# 1 is the run dir name (containing all MD run information) (str) - This is used to generate corresponding file names\n",
    "# 2 is a list of dump file paths (Path objects)\n",
    "# \n",
    "# e.g. \n",
    "# datafiles = [\n",
    "#   ( \"run1\", [Path(\".../dump_custom.C.0.dat\"), Path(\".../dump_custom.C.1.dat\"), …] ),\n",
    "#   ( \"run2\", [Path(\".../dump_custom.C.0.dat\"), Path(\".../dump_custom.C.1.dat\"), …] ),\n",
    "#   …\n",
    "# ]\n",
    "# \n",
    "# Ensure that the directory structure matches:\n",
    "# <directory>                                  # Base directory passed to import_simulation_data()\n",
    "#   ├── <run_...>/                             # Directory containing all repeats of  a given set of params\n",
    "#   │     ├── <run_1_...>/                     # Second level: each individual MD repeat \n",
    "#   │     │     └── NVT/                       # Inside each run directory a folder named “NVT” (generated by LAMMPS)\n",
    "#   │     │           ├── dump_custom.C.0.dat  # List of dump files\n",
    "#   │     │           ├── dump_custom.C.1.dat\n",
    "#   │     │           └── …                \n",
    "#   │     └── <run_2_...>/\n",
    "#   │           └── NVT/\n",
    "#   │                ├── dump_custom.C.0.dat\n",
    "#   │                └── …\n",
    "#   └── <run...>/\n",
    "#         ├── <run_1_...>/\n",
    "#         │     └── NVT/\n",
    "#         │        ├── dump_custom.C.0.dat\n",
    "#         │        └── …\n",
    "#         └── …\n",
    "\n",
    "\n",
    "def import_simulation_data(directory):\n",
    "    directory = Path(directory)\n",
    "    datafiles = []\n",
    "    dump_file_name = re.compile(r\"^dump_custom\\.C\\.(\\d+)\\.dat$\")\n",
    "\n",
    "    for simulation_dir in directory.iterdir():  # For each subdirectory in LAMMPS_simulations\n",
    "        if not simulation_dir.is_dir():\n",
    "            print(f\"ERROR: {simulation_dir} is not a directory\")\n",
    "            continue\n",
    "\n",
    "        for run_dir in simulation_dir.iterdir():  # For each individual MD run within each subdirectory\n",
    "            if not run_dir.is_dir():\n",
    "                print(f\"ERROR: {run_dir} is not a directory\")\n",
    "                continue\n",
    "\n",
    "            # Import all data dumps in numerical order\n",
    "            matches = []\n",
    "            nvt_dir = run_dir / \"NVT\"\n",
    "            \n",
    "            if not nvt_dir.is_dir(): # checks that there aren't any files named \"NVT\"\n",
    "                print(f\"ERROR: {nvt_dir} is not a directory\")\n",
    "                continue\n",
    "\n",
    "            for fname in nvt_dir.iterdir():\n",
    "                \n",
    "                if not fname.is_file(): # checks that everything inside NVT is a file\n",
    "                    print(f\"ERROR: {fname} is not a file\")\n",
    "                    continue\n",
    "\n",
    "                m = dump_file_name.match(fname.name)\n",
    "                if m:\n",
    "                    num = int(m.group(1))\n",
    "                    matches.append((num, fname))\n",
    "            \n",
    "            if not matches: # Notifies user if there are no dump files in an NVT folder\n",
    "                print(f\"No dump files found in {run_dir.name}\")\n",
    "                continue\n",
    "\n",
    "            matches.sort(key=lambda x: x[0])\n",
    "            sorted_files = [p for _, p in matches]\n",
    "            datafiles.append((run_dir.name, sorted_files))\n",
    "\n",
    "    if not datafiles:\n",
    "        print(\"No LAMMPS datafiles found\")\n",
    "        return\n",
    "\n",
    "    return datafiles\n",
    "\n",
    "datafiles = import_simulation_data(\"LAMMPS_simulations\")\n",
    "print(f\"{len(datafiles)} LAMMPS simulation files imported\")\n",
    "\n",
    "# Sets up an empty pipeline for each successive function to use\n",
    "def empty_ovito_pipeline(datafiles):\n",
    "\n",
    "    # Clear existing pipeline\n",
    "    for p in list(ovito.scene.pipelines):\n",
    "        p.remove_from_scene()\n",
    "\n",
    "    if not datafiles:\n",
    "        raise ValueError(\"No datafiles provided to basic_ovito_pipeline()\")\n",
    "   \n",
    "    basename, files = datafiles[0]\n",
    "    first_file = files[0]\n",
    "    pipeline = import_file(first_file)\n",
    "    \n",
    "    return pipeline\n",
    "pipeline = empty_ovito_pipeline(datafiles)\n",
    "\n",
    "# Data visualisation in Ovito\n",
    "def ovito_analysis(datafiles, pipeline):\n",
    "\n",
    "    if not datafiles:\n",
    "        raise ValueError(\"No datafiles provided to ovito_analysis()\")\n",
    "\n",
    "    # ------- ANALYSIS OF IMPORTED FILES ------------\n",
    "    # BUG: Image and video renderers error with: \n",
    "    # \"RuntimeError: Visual element 'Rings' reported an error:Failed to build non-periodic representation of periodic surface mesh. Periodic domain might be too small.\" if ring mod is included.\n",
    "\n",
    "    # Bond Modifier and Visuals \n",
    "    bond_modifier = CreateBondsModifier(cutoff=1.85)\n",
    "    bond_modifier.vis.width = 0.15\n",
    "    bond_modifier.vis.coloring_mode = BondsVis.ColoringMode.Uniform\n",
    "    bond_modifier.vis.color = (0.5, 0.5, 0.5)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Coordination Modifier and Colour Coding\n",
    "    pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff=1.85))\n",
    "    colour_coding_mod = ColorCodingModifier(property=\"Coordination\",start_value=1.0,end_value=4.0,gradient=ColorCodingModifier.Viridis(),discretize_color_map=True)\n",
    "    pipeline.modifiers.append(colour_coding_mod)\n",
    "\n",
    "    # Add to Scene\n",
    "    pipeline.add_to_scene()\n",
    "\n",
    "    # Viewing settings\n",
    "    vp = Viewport()\n",
    "    vp.type = Viewport.Type.Perspective\n",
    "\n",
    "    # Coordination Legend\n",
    "    legend = ColorLegendOverlay(\n",
    "        title = \"Coordination\",\n",
    "        modifier = colour_coding_mod,\n",
    "        alignment = QtCore.Qt.AlignmentFlag.AlignHCenter | QtCore.Qt.AlignmentFlag.AlignBottom,\n",
    "        orientation = QtCore.Qt.Orientation.Horizontal,\n",
    "        font_size = 0.1,\n",
    "        format_string = '%.0f' \n",
    "        )\n",
    "    vp.overlays.append(legend)\n",
    "\n",
    "    # Note: this function only renders for the first repeat \n",
    "    def is_run_1_(run_file_name):\n",
    "        return re.match(r\"^run_1_C\", run_file_name) is not None\n",
    "\n",
    "    # Skipped file counters\n",
    "    skipped_ovito_files = 0\n",
    "    skipped_png_files = 0\n",
    "    skipped_avi_files = 0\n",
    "    \n",
    "    # Image and Video Rendering\n",
    "    for basename, files in datafiles:\n",
    "\n",
    "        pipeline.source.load(files)\n",
    "\n",
    "        if is_run_1_(basename): # Only does analysis for run_1_\n",
    "            \n",
    "            ovito_save_file = ovito_dir / f\"{basename}.ovito\"\n",
    "            img_save_file   = ovito_dir / f\"{basename}.png\"\n",
    "            vid_save_file   = ovito_dir / f\"{basename}.avi\"                                     \n",
    "\n",
    "            # Ovito File Existance-Checker\n",
    "            ovito_exists = any(ovito_dir.rglob(ovito_save_file.name))\n",
    "            img_exists   = any(ovito_dir.rglob(img_save_file.name))\n",
    "            vid_exists   = any(ovito_dir.rglob(vid_save_file.name))\n",
    "            \n",
    "            # Set particle scaling (datafile specific)\n",
    "            n_frames = pipeline.source.num_frames\n",
    "            final_frame = max(0, n_frames - 1)\n",
    "            data = pipeline.compute(frame = final_frame)\n",
    "            data.particles.vis.scaling = 0.3\n",
    "\n",
    "            # Set Zoom\n",
    "            vp.zoom_all()\n",
    "\n",
    "            # Save .ovito scene\n",
    "            if not ovito_exists:\n",
    "                ovito.scene.save(ovito_save_file)\n",
    "                print(f\"Created: {basename}.ovito\") \n",
    "            else: skipped_ovito_files +=1\n",
    "\n",
    "            # Image and animation Render \n",
    "            tachyon = TachyonRenderer(shadows=False, direct_light_intensity=1.1)\n",
    "            if not img_exists:\n",
    "                vp.render_image(size=(1920,1080),\n",
    "                                filename=img_save_file,\n",
    "                                background=(1,1,1),\n",
    "                                frame=final_frame,\n",
    "                                renderer=tachyon)\n",
    "                print(f\"Created: {basename}.png\") \n",
    "            else: skipped_png_files += 1\n",
    "\n",
    "\n",
    "            if not vid_exists:        \n",
    "                vp.render_anim(size=(1920,1080), \n",
    "                            filename=vid_save_file, \n",
    "                            fps=10,\n",
    "                            renderer=tachyon)\n",
    "                print(f\"Created {basename}.avi\")   \n",
    "            else: skipped_avi_files += 1\n",
    "\n",
    "    # Print Skipped files\n",
    "    if skipped_ovito_files:\n",
    "        print(f\"Skipped {skipped_ovito_files} existing .ovito files\")\n",
    "    if skipped_png_files:\n",
    "        print(f\"Skipped {skipped_png_files} existing .png files\")\n",
    "    if skipped_avi_files:\n",
    "        print(f\"Skipped {skipped_avi_files} existing .avi files\")\n",
    "    \n",
    "    # Remove modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "\n",
    "# ------ DATA GENERATION FUNCTIONS ------\n",
    "# file_analysis(): \n",
    "#   1. uses datafiles from import_simulation_data()\n",
    "#   2. uses the pipeline from empty_ovito_pipeline(): no modifiers by default\n",
    "#   3. checks if files already exist in \"Structural Analysis\"\n",
    "#   4. loads each file in datafiles into the existing pipeline\n",
    "#   5. computes a specified data object for the given pipeline on each file and saves to a file name given by \"unique_file_identifier\" and the run-file name\n",
    "#   NOTE:\n",
    "#       a) requires \"unique_file_identifier\": e.g. \"bond_length_data\" or \"RDF\" (don't include leading underscore or .txt)\n",
    "#       b) \"data_function\" refers to the ovito function that return the desired data object \n",
    "#               e.g. \"data.particles['Coordination']\" or \"data.tables['coordination-rdf'].xy()\" or \"data.particles[\"c_pea\"]\" \n",
    "#       c) requires use of the \"lambda data:\" syntax for creating a throwaway function\n",
    "#               e.g. When calling this func, use \"file_analysis_and_existance_checker(datafiles,\"ring_data\",lambda data: data.tables[\"ring-size-histogram\"].xy())\"\"\n",
    "\n",
    "def file_analysis(datafiles, pipeline, unique_file_identifier, data_function):\n",
    "\n",
    "    if not datafiles:\n",
    "        raise ValueError(\"No datafiles provided\")\n",
    "    \n",
    "    # Skipped file counter\n",
    "    skipped_files = 0\n",
    "\n",
    "    # ----- STRUCTURAL ANALYSIS -----\n",
    "    for basename, files in datafiles:\n",
    "                \n",
    "        pipeline.source.load(files)\n",
    "        n_frames = pipeline.source.num_frames\n",
    "        final_frame = max(0, n_frames - 1)\n",
    "        data = pipeline.compute(frame = final_frame)\n",
    "\n",
    "        # File Name\n",
    "        data_file_name = structural_analysis_dir / f\"{basename}_{unique_file_identifier}.txt\"\n",
    "\n",
    "        # Structural Analysis File Existance-Checker\n",
    "        data_exists = any(structural_analysis_dir.rglob(data_file_name.name))\n",
    "        if data_exists and not REPLACE_OLD_FILES:\n",
    "            skipped_files += 1\n",
    "            continue \n",
    "\n",
    "        # Data\n",
    "        specific_data = data_function(data)\n",
    "        np.savetxt(data_file_name, specific_data, delimiter=\",\", fmt=\"%.6f\")\n",
    "        print(f\"Created {basename}_{unique_file_identifier}.txt\")\n",
    "\n",
    "    # Print Skipped Files\n",
    "    if skipped_files:\n",
    "        print(f\"Skipped {skipped_files} existing {unique_file_identifier} files\")  \n",
    "\n",
    "def ring_analysis(datafiles, pipeline, min_ring_size, max_ring_size, bond_length):\n",
    "    \n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "    \n",
    "    # Ring Analysis Modifier\n",
    "    ring_mod = FindRingsModifier(minimum_ring_size=min_ring_size, maximum_ring_size=max_ring_size)\n",
    "    pipeline.modifiers.append(ring_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(datafiles, pipeline, \"ring\", lambda data: data.tables[\"ring-size-histogram\"].xy())\n",
    "\n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "def coordination_analysis(datafiles, pipeline, coordination_cutoff):\n",
    "    \n",
    "    # Coordination Analysis Modfier\n",
    "    coord_mod = CoordinationAnalysisModifier(cutoff=coordination_cutoff)\n",
    "    pipeline.modifiers.append(coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(datafiles, pipeline, \"coordination\", lambda data: data.particles['Coordination'])\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()\n",
    "    \n",
    "def energy_analysis(datafiles, pipeline):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(datafiles, pipeline, \"potential_energy\", lambda data: data.particles[\"c_pea\"])\n",
    "\n",
    "def RDF_analysis(datafiles, pipeline, RDF_cutoff, bins):\n",
    "    \n",
    "    # Coordination Analysis Modfier for RDF\n",
    "    RDF_coord_mod = CoordinationAnalysisModifier(cutoff=RDF_cutoff, number_of_bins=bins)\n",
    "    pipeline.modifiers.append(RDF_coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(datafiles, pipeline, \"RDF\", lambda data: data.tables['coordination-rdf'].xy())\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()    \n",
    "\n",
    "def bond_length_analysis(datafiles, pipeline, bins, bond_length, bond_length_analysis_cutoff):\n",
    "\n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Bond Analysis Modifier\n",
    "    bond_analysis_mod = BondAnalysisModifier(bins = bins, length_cutoff=bond_length_analysis_cutoff)\n",
    "    pipeline.modifiers.append(bond_analysis_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(datafiles, pipeline, \"bond_length\", lambda data: data.tables[\"bond-length-distr\"].xy())\n",
    "  \n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    " \n",
    "\n",
    "# ----- POSSIBLE ANALYSIS ------\n",
    "# 1. Forces\n",
    "# 2. Bond Angle\n",
    "# 3. Conditional analysis (i.e. for sp/sp2/sp3 individually)\n",
    "# 4. Young's Modulus\n",
    "# 5. Coordination for each frame in a given sim plotted against simulation time\n",
    "# ------------------------------\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will regenerate ALL files (apart from renders)\n",
    "REPLACE_OLD_FILES = True\n",
    "\n",
    "if REPLACE_OLD_FILES:\n",
    "    confirm = input(\"Are you sure you want to replace old files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "bond_length_analysis(datafiles, pipeline, bins=1000, bond_length = 1.85, bond_length_analysis_cutoff=2.0)\n",
    "RDF_analysis(datafiles, pipeline, RDF_cutoff=6.0, bins=200)\n",
    "ring_analysis(datafiles, pipeline, min_ring_size=3, max_ring_size=24, bond_length=1.85)\n",
    "coordination_analysis(datafiles, pipeline, coordination_cutoff=1.85)\n",
    "energy_analysis(datafiles, pipeline)\n",
    "#ovito_analysis(datafiles, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd55289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FILE ORGANISER -----\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Assign Directories\n",
    "cwd = Path.cwd()\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "ovito_dir = analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(exist_ok=True)\n",
    "structural_analysis_dir = analysis_dir / \"Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Regex for capturing the MD and structural parameters\n",
    "LAMMPS_regex_pattern = re.compile(\n",
    "    r'^run_\\d+_'                                 # captures run number\n",
    "    r'(C_nvt_\\d+atoms_[0-9]+(?:\\.[0-9]+)?gcm_[0-9]+(?:\\.[0-9]+)?minsep.data\\_m\\d+_c\\d+)'  # structural and melt/quench parameters\n",
    "    r'(?:[._].*|$)'                              # captures anything after the \"key\"\n",
    ")\n",
    "\n",
    "# Groups files with matching structural and melt/quench parameters \n",
    "def file_organiser(dir_path, OVERWRITE=False):\n",
    "    \n",
    "    for file in dir_path.iterdir():\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        base = file.stem\n",
    "        match = LAMMPS_regex_pattern.match(base)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        simulation_parameters = match.group(1)  # Matches simulation and structural parameters\n",
    "        simulation_dir = dir_path / f\"run_{simulation_parameters}\"\n",
    "        simulation_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        destination = simulation_dir / file.name\n",
    "\n",
    "        if destination.exists():\n",
    "            if OVERWRITE:\n",
    "                print(f\"Overwritten: {file.name}\")\n",
    "            else:\n",
    "                print(f\"Skipped Overwrite of {file.name}\")\n",
    "                continue\n",
    "        \n",
    "        # Move (and overwrite if allowed)\n",
    "        file.replace(destination)\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will replace ALL existing files \n",
    "OVERWRITE = True\n",
    "\n",
    "if OVERWRITE:\n",
    "    confirm = input(\"Are you sure you want to overwrite existing files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "# Organise ovito files and structural_analysis files\n",
    "file_organiser(ovito_dir, OVERWRITE=OVERWRITE)\n",
    "file_organiser(structural_analysis_dir, OVERWRITE=OVERWRITE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a3a4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 matching data files found\n",
      "sp3 Carbon Proportion_coordination_density_plot_m5000_c10000.png created\n",
      "210 matching data files found\n",
      "bond_length_density_plot_m5000_c10000.png created\n",
      "210 matching data files found\n",
      "mean_potential_energy_density_plot_m5000_c10000.png created\n",
      "210 matching data files found\n",
      "6_ring_density_plot_m5000_c10000.png created\n",
      "10 matching data files found\n",
      "10 matching data files found\n",
      "ring_histogram_m5000_c10000_2.00-3.00gcm.png created\n",
      "10 matching data files found\n",
      "10 matching data files found\n",
      "10 matching data files found\n",
      "RDF_m5000_c10000_2.00-3.50-1.50gcm.png created\n"
     ]
    }
   ],
   "source": [
    "# ------ GRAPHICAL ANALYSIS --------\n",
    "\n",
    "# Graphical data points are means of all repeat runs with errors given as 1 standard deviation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create Graphical Analysis Directories\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_dir = analysis_dir / \"Graphical Analysis\"\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# ------ FIGURE FORMATTING ------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('1_column_fig.mplstyle')\n",
    "# -------------------------------\n",
    "\n",
    "# ------ DATA IMPORTING------\n",
    "# analysis_type: density, quench, single_value \n",
    "# data_name: identifying name in data file name\n",
    "# Simulation params melt_time, quench_time, density: int or None\n",
    "# Returns a list of tuples (filepath, density, melt_time, quench_time)\n",
    "def import_data_files(directory, data_name, analysis_type, density, melt_time, quench_time):\n",
    "    \n",
    "    directory = Path(directory)\n",
    "\n",
    "    # Normalise analysis type to be non-case sensitive\n",
    "    analysis_type_norm = str(analysis_type).lower()\n",
    "\n",
    "    # Regex for data types\n",
    "    data_file_regex = re.compile(fr\".*({re.escape(data_name)}).*\") #regex for finding coordination data\n",
    "    density_regex = re.compile(r'_(\\d+(?:\\.\\d+)?)\\s*(?=gcm)') #regex for finding density data\n",
    "    melt_quench_regex =  re.compile(r'm(\\d+)_c(\\d+)')  # regex for finding melt/quench parameters    \n",
    "\n",
    "    imported_data_files = [] # in the tuple format: (filepath, density, melt, quench)\n",
    "\n",
    "    run_folders = [d for d in directory.iterdir() if d.is_dir()] # list all run folders\n",
    "\n",
    "    # Type checks for correct inputs\n",
    "    try:\n",
    "        melt_time_filter = int(melt_time) if melt_time is not None else None\n",
    "    except (ValueError, TypeError):\n",
    "        raise ValueError(\"melt_time must be an int or None\")\n",
    "    try:\n",
    "        quench_time_filter = int(quench_time) if quench_time is not None else None\n",
    "    except (ValueError, TypeError):\n",
    "        raise ValueError(\"quench_time must be an int or None\")\n",
    "    try:\n",
    "        density_filter = float(density) if density is not None else None\n",
    "    except (ValueError, TypeError):\n",
    "        raise ValueError(\"density must be a number or None\")\n",
    "\n",
    "    # Further Checks for correct for a given analysis\n",
    "    if analysis_type_norm == \"density\":\n",
    "        if melt_time_filter is None or quench_time_filter is None:\n",
    "            raise ValueError(\"For density analysis, melt_time and quench_time must be provided (not None).\")\n",
    "        if density_filter is not None:\n",
    "            raise ValueError(\"For density analysis, the 'density' parameter should be None (it's used as a filter for quench analysis).\")\n",
    "    elif analysis_type_norm == \"quench\":\n",
    "        if melt_time_filter is None or density_filter is None:\n",
    "            raise ValueError(\"For quench analysis, melt_time and density must be provided (not None).\")\n",
    "        if quench_time_filter is not None:\n",
    "            raise ValueError(\"For quench analysis, the 'quench_time' parameter should be None (it's used as a filter for density analysis).\")  \n",
    "    elif analysis_type_norm == \"single_value\":\n",
    "        if melt_time_filter is None or density_filter is None or quench_time_filter is None:\n",
    "            raise ValueError(\"For single value analysis, all args must be specified (melt, quench, density)\")\n",
    "    \n",
    "    if analysis_type_norm not in existing_analysis_types:\n",
    "        raise ValueError(f\"Unknown analysis_type: {analysis_type!r}\")\n",
    "\n",
    "    # Density analysis vs. Quench time analysis vs. variable density analysis\n",
    "    for run_folder in run_folders:\n",
    "        run_folder_name = run_folder.name\n",
    "        \n",
    "        # Exract density\n",
    "        d = density_regex.search(run_folder_name)\n",
    "        if d is None:\n",
    "            continue                            \n",
    "        try:\n",
    "            file_density = float(d.group(1))   \n",
    "        except ValueError:\n",
    "            print(f\"ERROR: Failed to extract {run_folder} density\")\n",
    "            continue\n",
    "\n",
    "        # Extract melt/quench params\n",
    "        m = melt_quench_regex.search(run_folder_name)\n",
    "        if m is None:\n",
    "            continue\n",
    "        try:\n",
    "            file_melt_time = int(m.group(1))    \n",
    "            file_quench_time = int(m.group(2))  \n",
    "        except ValueError:\n",
    "            print(f\"ERROR: Failed to extract {run_folder} melt/quench parameters\")\n",
    "            continue\n",
    "\n",
    "        # List datafiles in each run_folder\n",
    "        data_files = [f for f in run_folder.iterdir() if f.is_file()]\n",
    "        \n",
    "        for data_file in data_files:\n",
    "            \n",
    "            # Extract data file type\n",
    "            ft = data_file_regex.search(data_file.name)\n",
    "            if ft is None: \n",
    "                continue\n",
    "            file_type = ft.group(1)\n",
    "\n",
    "            if file_type == data_name: \n",
    "            \n",
    "                # Density Analysis (returns files with matching melt and quench params)\n",
    "                if analysis_type_norm == \"density\":\n",
    "                    if (file_melt_time == melt_time_filter\n",
    "                        and file_quench_time == quench_time_filter):\n",
    "                        imported_data_files.append((data_file, file_density, file_melt_time, file_quench_time))\n",
    "\n",
    "                # Quench Time Analysis (returns files with matching density and melt params)\n",
    "                elif analysis_type_norm == \"quench\":\n",
    "                    if (file_melt_time == melt_time_filter\n",
    "                        and (abs(file_density - density_filter) < 1e-6)):\n",
    "                        imported_data_files.append((data_file, file_density, file_melt_time, file_quench_time))\n",
    "\n",
    "                # Single Density Analysis (returns files with matching density, melt and quench params)\n",
    "                elif analysis_type_norm == \"single_value\":\n",
    "                    if (file_melt_time == melt_time_filter\n",
    "                        and file_quench_time == quench_time_filter\n",
    "                        and (abs(file_density - density_filter) < 1e-6)):\n",
    "                        imported_data_files.append((data_file, file_density, file_melt_time, file_quench_time))\n",
    "\n",
    "    \n",
    "    if not imported_data_files:\n",
    "        print(\"No datafiles found for the current selection\")\n",
    "    else:\n",
    "        num_imported_files = len(imported_data_files)\n",
    "        print(f\"{num_imported_files} matching data files found\")\n",
    "    return imported_data_files\n",
    "# ----------------------------\n",
    "\n",
    "# ------ PLOTTING DATA ------\n",
    "# Plot_type: marker (with error bars), line (with shaded regions) \n",
    "# Note: only works with density or quench analysis\n",
    "def plot(plot_type, data, analysis_type, x_label, y_label, chart_title, save_file_name):\n",
    "\n",
    "    if plot_type not in existing_plot_types:\n",
    "        raise ValueError(f\"Unknown plot_type: {plot_type!r}\")\n",
    "\n",
    "    data = data.sort_values(f\"{analysis_type}\")\n",
    "    fig, ax = plt.subplots()  # Local figure size for each plot\n",
    "    \n",
    "    x = data[f\"{analysis_type}\"].values\n",
    "    mean = data[\"mean\"].values\n",
    "    std = data[\"std\"].values\n",
    "    \n",
    "    if plot_type == \"marker\":\n",
    "        ax.errorbar(x, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5)\n",
    "\n",
    "    elif plot_type == \"line\":\n",
    "        alpha_fill = 0.25\n",
    "        ax.plot(x, mean, label=\"Mean\")\n",
    "        ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "                \n",
    "    # Labels and Titles\n",
    "    ax.set_xlabel(f\"{x_label}\")\n",
    "    ax.set_ylabel(f'{y_label}')\n",
    "    ax.set_title(f\"{chart_title}\")\n",
    "\n",
    "    # Ensure directory exists\n",
    "    graph_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    filepath = graph_dir / save_file_name\n",
    "\n",
    "    # Save Plot to Graphical Analysis\n",
    "    plt.savefig(filepath)\n",
    "    print(f\"{filepath.name} created\")\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "# ---------------------------\n",
    "            \n",
    "# Coordination analysis \n",
    "def coordination_analysis(analysis_type, directory, density, melt_time, quench_time, coordination_number):\n",
    "    \n",
    "    data_name = \"coordination\"\n",
    "\n",
    "    # Label coordination number \n",
    "    if coordination_number == 2:\n",
    "        y_label = \"sp Carbon Proportion\"\n",
    "    elif coordination_number == 3:\n",
    "        y_label = \"sp2 Carbon Proportion\"\n",
    "    elif coordination_number == 4:\n",
    "        y_label = \"sp3 Carbon Proportion\"\n",
    "    else:\n",
    "        print(\"ERROR: Coordination number should be between 2 and 4\")\n",
    "        y_label == f\"{coordination_number} coordinate atoms\"\n",
    "        return\n",
    "\n",
    "    if analysis_type == \"density\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, None, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        coordination_density_results = [] # In the form: (density, coordination_proportion)\n",
    "\n",
    "        for file, density, _, _ in datafiles: \n",
    "                        \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                coordination_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "                        \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if coordination_data.size == 0:\n",
    "                continue\n",
    "        \n",
    "            # Calculate coordination proportions\n",
    "            coordination_proportion = (np.count_nonzero(coordination_data == coordination_number) / coordination_data.size)\n",
    "            coordination_density_results.append((density, coordination_proportion))\n",
    "        \n",
    "\n",
    "        x_label = \"Density (g/cm³)\"\n",
    "        chart_title = f\"Coordination vs. Density\\nMelt Time = {melt_time} fs, Quench Time = {quench_time} fs\"\n",
    "        save_file_name = f\"{y_label}_coordination_density_plot_m{melt_time}_c{quench_time}.png\"\n",
    "\n",
    "\n",
    "    elif analysis_type == \"quench\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, density, melt_time, None)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        coordination_density_results = [] # In the form: (density, coordination_proportion)\n",
    "\n",
    "        for file, _, _, quench in datafiles: \n",
    "                        \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                coordination_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "                        \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if coordination_data.size == 0:\n",
    "                continue\n",
    "        \n",
    "            # Calculate coordination proportions\n",
    "            coordination_proportion = (np.count_nonzero(coordination_data == coordination_number) / coordination_data.size)\n",
    "            coordination_density_results.append((quench, coordination_proportion))\n",
    "        \n",
    "        x_label = \"Quench Time (fs)\"\n",
    "        chart_title = f\"Coordination vs. Quench Time\\nMelt Time = {melt_time} fs  Density = {density} (g/cm³)\"\n",
    "        save_file_name = f\"{y_label}_coordination_quench_plot_m{melt_time}_{density}gcm.png\"\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(coordination_density_results, columns=[f\"{analysis_type}\", \"coordination_proportions\"])\n",
    "    coordination_df = df.groupby(f\"{analysis_type}\")[\"coordination_proportions\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index()  \n",
    "\n",
    "\n",
    "    plot(plot_type = set_plot_type, data = coordination_df,\n",
    "        analysis_type = analysis_type, x_label = x_label, y_label = y_label, \n",
    "        chart_title = chart_title,\n",
    "        save_file_name = save_file_name)\n",
    "\n",
    "# Ring Size analysis \n",
    "def ring_analysis(analysis_type, directory, density, melt_time, quench_time, ring_size):\n",
    "    data_name = \"ring\"\n",
    "\n",
    "    if analysis_type == \"density\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, None, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        ring_num_results = [] # In the form: (density, ring_num)\n",
    "\n",
    "        for file, density, _, _ in datafiles: \n",
    "                        \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                ring_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "                        \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if ring_data.size == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate number of rings\n",
    "            num_rings = ring_data[ring_data[:, 0] == ring_size, 1][0]\n",
    "            ring_num_results.append((density, num_rings))\n",
    "        \n",
    "        x_label = \"Density (g/cm³)\"\n",
    "        chart_title = f\"Number of {ring_size} Membered Rings vs. Density\\nMelt Time = {melt_time} fs  Quench Time = {quench_time} fs\"\n",
    "        save_file_name = f\"{ring_size}_ring_density_plot_m{melt_time}_c{quench_time}.png\"\n",
    "\n",
    "    elif analysis_type == \"quench\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, density, melt_time, None)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        ring_num_results = [] # In the form: (density, ring_num)\n",
    "\n",
    "        for file, _, _, quench in datafiles: \n",
    "                        \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                ring_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "                        \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if ring_data.size == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate number of rings\n",
    "            num_rings = ring_data[ring_data[:, 0] == ring_size, 1][0]\n",
    "            ring_num_results.append((quench, num_rings))\n",
    "        \n",
    "        x_label = \"Quench Time (fs)\"\n",
    "        chart_title = f\"Number of {ring_size} Membered Rings vs. Quench Time\\nMelt Time = {melt_time} fs  Density = {density} (g/cm³)\"\n",
    "        save_file_name = f\"{ring_size}_ring_quench_plot_m{melt_time}_{density}gcm.png\"\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(ring_num_results, columns=[f\"{analysis_type}\", \"num_rings\"])\n",
    "    coordination_df = df.groupby(f\"{analysis_type}\")[\"num_rings\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index()  \n",
    "\n",
    "\n",
    "    plot(plot_type = set_plot_type, data = coordination_df,\n",
    "        analysis_type = analysis_type, x_label = x_label, y_label = f\"{ring_size} Membered Rings\", \n",
    "        chart_title = chart_title,\n",
    "        save_file_name = save_file_name)\n",
    "\n",
    "# Potential energy analysis \n",
    "def potential_energy_analysis(analysis_type, directory, density, melt_time, quench_time):\n",
    "\n",
    "    data_name = \"potential_energy\"\n",
    "\n",
    "    if analysis_type == \"density\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, None, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        PE_results = [] # In the form: (density, PE)\n",
    "\n",
    "        for file, density, _, _ in datafiles: \n",
    "                        \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                energy_data = np.loadtxt(file)\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if energy_data.size == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate number of rings\n",
    "            mean_potential_energy = np.mean(energy_data)\n",
    "            PE_results.append((density, mean_potential_energy))\n",
    "        \n",
    "        x_label = \"Density (g/cm³)\"\n",
    "        chart_title = f\"Mean Potential Energy vs. Density\\nMelt Time = {melt_time} fs  Quench Time = {quench_time} fs\"\n",
    "        save_file_name = f\"mean_potential_energy_density_plot_m{melt_time}_c{quench_time}.png\"\n",
    "\n",
    "    elif analysis_type == \"quench\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, density, melt_time, None)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        PE_results = [] # In the form: (density, PE)\n",
    "\n",
    "        for file, _, _, quench in datafiles: \n",
    "                        \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                energy_data = np.loadtxt(file)\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            # Fail-safe incase there's no data in the file\n",
    "            if energy_data.size == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate mean potential energy\n",
    "            mean_potential_energy = np.mean(energy_data)\n",
    "            PE_results.append((quench, mean_potential_energy))\n",
    "        \n",
    "        x_label = \"Quench Time (fs)\"\n",
    "        chart_title = f\"Mean Potential Energy vs. Quench Time\\nMelt Time = {melt_time} fs  Density = {density} (g/cm³)\"\n",
    "        save_file_name = f\"mean_potential_energy_quench_plot_m{melt_time}_{density}gcm.png\"\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(PE_results, columns=[f\"{analysis_type}\", \"mean_potential_energy\"])\n",
    "    coordination_df = df.groupby(f\"{analysis_type}\")[\"mean_potential_energy\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index()  \n",
    "\n",
    "\n",
    "    plot(plot_type = set_plot_type, data = coordination_df,\n",
    "        analysis_type = analysis_type, x_label = x_label, y_label = 'Mean Potential Energy (eV)', \n",
    "        chart_title = chart_title,\n",
    "        save_file_name = save_file_name)\n",
    "\n",
    "# Bond Length analysis\n",
    "def bond_length_analysis(analysis_type, directory, density, melt_time, quench_time):\n",
    "\n",
    "    data_name = \"bond_length\"\n",
    "\n",
    "    if analysis_type == \"density\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, None, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        bond_length_results = [] # In the form: (density, mean_bond_length)\n",
    "\n",
    "        for file, density, _, _ in datafiles: \n",
    "                        \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                bond_length_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if bond_length_data.size == 0:\n",
    "                continue\n",
    "        \n",
    "            # Split columns\n",
    "            x = bond_length_data[:, 0]   # bond length\n",
    "            y = bond_length_data[:, 1]   # frequency\n",
    "\n",
    "            # Calculate mean bond length w.r.t. density\n",
    "            mean_bond_length = np.average(x, weights=y)\n",
    "            bond_length_results.append((density, mean_bond_length))\n",
    "        \n",
    "        x_label = \"Density (g/cm³)\"\n",
    "        chart_title = f'Mean Bond Length vs. Density\\nMelt Time = {melt_time} fs, Quench Time = {quench_time} fs'\n",
    "        save_file_name = f\"bond_length_density_plot_m{melt_time}_c{quench_time}.png\"\n",
    "\n",
    "    elif analysis_type == \"quench\":\n",
    "        \n",
    "        datafiles = import_data_files(directory, data_name, analysis_type, density, melt_time, None)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        bond_length_results = [] # In the form: (density, mean_bond_length)\n",
    "\n",
    "        for file, _, _, quench in datafiles:\n",
    "\n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                bond_length_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if bond_length_data.size == 0:\n",
    "                continue\n",
    "        \n",
    "            # Split columns\n",
    "            x = bond_length_data[:, 0]   # bond length\n",
    "            y = bond_length_data[:, 1]   # frequency\n",
    "\n",
    "            # Calculate mean bond length w.r.t. quench time\n",
    "            mean_bond_length = np.average(x, weights=y)\n",
    "            bond_length_results.append((quench, mean_bond_length))\n",
    "        \n",
    "        x_label = \"Quench Time (fs)\"\n",
    "        chart_title = f\"Mean Bond Length vs. Quench Time\\nMelt Time = {melt_time} fs  Density = {density} (g/cm³)\"\n",
    "        save_file_name = f\"bond_length_quench_plot_m{melt_time}_{density}gcm.png\"\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(bond_length_results, columns=[f\"{analysis_type}\", \"mean_bond_length\"])\n",
    "    bond_length_df = df.groupby(f\"{analysis_type}\")[\"mean_bond_length\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index()  \n",
    "\n",
    "\n",
    "    plot(plot_type = set_plot_type, data = bond_length_df,\n",
    "        analysis_type = analysis_type, x_label = x_label, y_label = 'Mean Bond Length (Å)', \n",
    "        chart_title = chart_title,\n",
    "        save_file_name = save_file_name)\n",
    "\n",
    "# ------ RDF/DENSITY ANALYSIS ------\n",
    "# Allows comparison of different densities\n",
    "# The RDF is given as the mean of all repeats\n",
    "# Shading represents the standard deviation\n",
    "# Note: this assumes the same x values in the RDFS for the data, which is only true\n",
    "# for the same number of atoms and bins --> specified in structure generator and earlier structural analysis\n",
    "# for different analysis types it takes the first value of densities/quench_times if there is more than one\n",
    "def RDF_density_analysis(directory, melt_time, quench_time, densities):\n",
    "       \n",
    "    data_name = \"RDF\"\n",
    "\n",
    "    per_density_dataframes = {}\n",
    "\n",
    "    for density in densities:\n",
    "        datafiles = import_data_files(directory, data_name, \"single_value\", density, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            continue\n",
    "\n",
    "        RDF_results = [] # in the form (density, r_array, g_r_array)\n",
    "\n",
    "        for file, file_density, _, _ in datafiles:\n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                RDF_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if RDF_data.size == 0:\n",
    "                continue\n",
    "            \n",
    "            r_array = RDF_data[:, 0]\n",
    "            g_r_array = RDF_data[:, 1]\n",
    "\n",
    "            RDF_results.append((file_density, r_array, g_r_array))\n",
    "            \n",
    "        # skip if no files found \n",
    "        if len(RDF_results) == 0:\n",
    "            continue\n",
    "\n",
    "        # Check that the r scales are the same across all repeats\n",
    "        ref_r = RDF_results[0][1] #first r scale\n",
    "        consistent_r = True\n",
    "        for _, r_arr, _ in RDF_results[1:]:\n",
    "            if not np.allclose(ref_r, r_arr, atol=1e-6):\n",
    "                print(f\"ERROR: RDF data not consistent in r scale — skipping density {density}\")\n",
    "                consistent_r = False\n",
    "                break\n",
    "        if not consistent_r:\n",
    "            continue  # skip this density entirely\n",
    "\n",
    "        # Expand each tuple (density, r_array, g_r_array) into rows to allow for aggregation\n",
    "        rows = []\n",
    "        for d, r_arr, g_arr in RDF_results:\n",
    "            for r_val, g_val in zip(r_arr, g_arr):\n",
    "                rows.append((d, r_val, g_val))\n",
    "        \n",
    "        # Dataframe\n",
    "        df = pd.DataFrame(rows, columns=[\"density\", \"r\", \"g_r\"])\n",
    "        RDF_df = (df.groupby([\"density\", \"r\"])[\"g_r\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index())\n",
    "    \n",
    "        # Saved to the dictionary\n",
    "        per_density_dataframes[density] = RDF_df\n",
    "\n",
    "    if not per_density_dataframes:\n",
    "        print(\"ERROR: No data found for RDF plot\")\n",
    "        return\n",
    "\n",
    "    # Plot Data\n",
    "    # Shaded areas given by +- 1 standard deviation\n",
    "    def plot_RDFs(rdf_dict, title, alpha_fill=0.25):\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlabel(\"r (Å)\")\n",
    "        ax.set_ylabel(\"g(r)\")\n",
    "        ax.set_title(f\"{title}\")\n",
    "        \n",
    "        # sort densities for consistent legend order\n",
    "        densities_sorted = sorted(rdf_dict.keys())\n",
    "\n",
    "        for density in densities_sorted:\n",
    "            df = rdf_dict[density]\n",
    "            r = df['r'].values\n",
    "            mean = df['mean'].values\n",
    "            std = df['std'].values\n",
    "\n",
    "            ax.plot(r, mean, label=f\"{density:.2f} g/cm³\")\n",
    "            ax.fill_between(r, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "        ax.legend(title=\"Density\")\n",
    "\n",
    "        # Save image\n",
    "        dens_str = \"-\".join(f\"{d:.2f}\" for d in densities)\n",
    "        save_file_name = f\"RDF_m{melt_time}_c{quench_time}_{dens_str}gcm.png\"\n",
    "        filepath = graph_dir / save_file_name\n",
    "        plt.savefig(filepath)\n",
    "        plt.close(fig)\n",
    "        print(f\"{filepath.name} created\")\n",
    "\n",
    "    plot_RDFs(per_density_dataframes, title=f\"Radial Distribution Function\\nMelt Time = {melt_time} fs, Quench Time = {quench_time} fs\")\n",
    "#-----------------------------------\n",
    "\n",
    "# ------ RING SIZE/ DENSITY HISTOGRAM ------\n",
    "# Allows comparison of different densities\n",
    "# Shading represents the standard deviation\n",
    "def ring_histogram_density_analysis(directory, melt_time, quench_time, densities):\n",
    "    \n",
    "    data_name = \"ring\"\n",
    "\n",
    "    per_density_dataframes = {}\n",
    "\n",
    "    for density in densities:\n",
    "        datafiles = import_data_files(directory, data_name, \"single_value\", density, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        ring_results = [] # in the form (density, ring_size_array, frequency_array) \n",
    "\n",
    "        for file, file_density, _, _ in datafiles:\n",
    "                    \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                ring_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if ring_data.size == 0:\n",
    "                continue\n",
    "            \n",
    "            ring_size_array = ring_data[:,0]\n",
    "            frequency_array = ring_data[:,1]\n",
    "\n",
    "            ring_results.append((file_density, ring_size_array, frequency_array))\n",
    "            \n",
    "        # skip if no files found \n",
    "        if len(ring_results) == 0:\n",
    "            continue\n",
    "\n",
    "        # Expand each tuple (density, ring_size_array, frequency_array) into rows to allow for aggregation\n",
    "        rows = []\n",
    "        for d, ring_size_arr, frequency_array in ring_results:\n",
    "            for r_val, f_val in zip(ring_size_arr, frequency_array):\n",
    "                rows.append((d, r_val, f_val))        \n",
    "\n",
    "        # Dataframe\n",
    "        df = pd.DataFrame(rows, columns=[\"density\", \"ring_size\", \"frequency\"])\n",
    "        ring_df = (df.groupby([\"density\", \"ring_size\"])[\"frequency\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index())\n",
    "        \n",
    "        # Saved to the dictionary\n",
    "        per_density_dataframes[density] = ring_df\n",
    "\n",
    "    if not per_density_dataframes:\n",
    "        print(\"ERROR: No data found for ring histogram plot\")\n",
    "        return\n",
    "\n",
    "    # Plot Data\n",
    "    # Shaded areas given by +- 1 standard deviation\n",
    "    def plot_ring_hist(rdf_dict, title, alpha_fill=0.25):\n",
    "        \n",
    "        fig, ax = plt.subplots() \n",
    "        ax.set_xlabel(\"Ring Size\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"{title}\")\n",
    "        \n",
    "        # sort densities for consistent legend order\n",
    "        densities_sorted = sorted(rdf_dict.keys())\n",
    "\n",
    "        for density in densities_sorted:\n",
    "            df = rdf_dict[density]\n",
    "            r = df['ring_size'].values\n",
    "            mean = df['mean'].values\n",
    "            std = df['std'].values\n",
    "\n",
    "            ax.errorbar(r, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5, label=f\"{density} g/cm³\")\n",
    "\n",
    "            #ax.fill_between(r, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "        ax.legend(title=\"Density\")\n",
    "\n",
    "        # Save image\n",
    "        dens_str = \"-\".join(f\"{d:.2f}\" for d in densities)\n",
    "        save_file_name = f\"ring_histogram_m{melt_time}_c{quench_time}_{dens_str}gcm.png\"\n",
    "        filepath = graph_dir / save_file_name\n",
    "        plt.savefig(filepath)\n",
    "        plt.close(fig)\n",
    "        print(f\"{filepath.name} created\")\n",
    "\n",
    "    plot_ring_hist(per_density_dataframes, title=f\"Ring Size Histogram\\nMelt Time = {melt_time} fs, Quench Time = {quench_time} fs\")\n",
    "# ------------------------------------------\n",
    "\n",
    "# ------ ANALYSIS PARAMETERS ------\n",
    "existing_analysis_types = [\"density\", \"quench\", \"single_value\"]\n",
    "existing_plot_types = [\"marker\", \"line\"]\n",
    "\n",
    "set_analysis_type = \"density\"  # NOTE: \"single_value\" analysis is only valid for multi-density plots - do not use here\n",
    "set_plot_type = \"line\"\n",
    "\n",
    "set_density = 2.5 # for quench analysis\n",
    "set_melt_time = 5000\n",
    "set_quench_time = 10000 # for density analysis\n",
    "set_analysis_directory = \"Analysis/Structural Analysis\"\n",
    "\n",
    "set_coordination_number = 4\n",
    "set_ring_size = 6\n",
    "# ---------------------------------\n",
    "\n",
    "coordination_analysis(analysis_type = set_analysis_type, directory=set_analysis_directory, \n",
    "                      density = set_density, melt_time=set_melt_time, quench_time=set_quench_time, \n",
    "                      coordination_number=set_coordination_number)\n",
    "\n",
    "bond_length_analysis(analysis_type = set_analysis_type, directory=set_analysis_directory, \n",
    "                     density = set_density, melt_time=set_melt_time, quench_time=set_quench_time)\n",
    "\n",
    "potential_energy_analysis(analysis_type=set_analysis_type, directory=set_analysis_directory,\n",
    "                           density = set_density, melt_time=set_melt_time, quench_time=set_quench_time)\n",
    "\n",
    "ring_analysis(analysis_type = set_analysis_type, directory= set_analysis_directory, \n",
    "              density = set_density, melt_time=set_melt_time, quench_time=set_quench_time, \n",
    "              ring_size=set_ring_size)\n",
    "\n",
    "\n",
    "#Multi density plots\n",
    "ring_histogram_density_analysis('Analysis/Structural Analysis', \n",
    "                                melt_time=5000, quench_time=10000, densities=(2.0,3.0))\n",
    "\n",
    "RDF_density_analysis('Analysis/Structural Analysis', \n",
    "                     melt_time=5000, quench_time=10000, densities=(2.0,3.5,1.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ovito",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
