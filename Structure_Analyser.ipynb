{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451afa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SIMULATION VALIDATOR ------\n",
    "# Before running analysis it's important to check that all simulations have run as expected\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "# # -------- OPTIONAL WAIT FUNCTION ---------\n",
    "import subprocess\n",
    "import time\n",
    "WAIT_TIME = 600  # seconds\n",
    "USER = \"scat9451\"\n",
    "\n",
    "while True:\n",
    "    job_status = subprocess.run([\"qstat\", \"-u\", USER], capture_output=True, text=True)\n",
    "    if not job_status.stdout.strip():  # empty means no jobs\n",
    "        break\n",
    "    print(\"Active jobs found - waiting ...\")\n",
    "    time.sleep(WAIT_TIME)\n",
    "\n",
    "print(\"No active jobs found - proceeding ...\")\n",
    "# # ---------------------------------------------------------------------\n",
    "\n",
    "expected_number_of_dump_files_per_sim = 96             # =(total number of timesteps + 1)/100\n",
    "\n",
    "# This expects file structure:\n",
    "# <unique_key>/                     \n",
    "#         ‚îî‚îÄ‚îÄ NVT/                       \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00000.dat  \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00001.dat\n",
    "from pathlib import Path\n",
    "\n",
    "root_directory = Path(\"/u/vld/scat9451/main_project/\")\n",
    "def simulation_validator(directory):\n",
    "\n",
    "    directory = Path(directory)  \n",
    "\n",
    "    # Check number of files and file types\n",
    "    errors = set()\n",
    "    warnings = set()\n",
    "    invalid_sims = 0\n",
    "\n",
    "    \n",
    "    for nvt_dir in directory.rglob(\"NVT\"):\n",
    "        if nvt_dir.is_dir():\n",
    "\n",
    "            # All files in the NVT directory\n",
    "            all_files = [f for f in nvt_dir.iterdir() if f.is_file()]\n",
    "            total = len(all_files)\n",
    "\n",
    "            unique_key = nvt_dir.parent.name\n",
    "            # Check number of files matches expected number\n",
    "            if total != expected_number_of_dump_files_per_sim:\n",
    "                print(f\"{total} files found in {unique_key}\")\n",
    "                invalid_sims += 1\n",
    "\n",
    "            # Files that start with 'dump_custom'\n",
    "            unrecognized_files = [f for f in all_files if not f.name.startswith(\"dump_custom\")]\n",
    "            if unrecognized_files:\n",
    "                print(f\"Unrecognized files found in {unique_key}\")\n",
    "                print(unrecognized_files)\n",
    "                invalid_sims += 1\n",
    "    \n",
    "    # Check for ERROR and WARNING messages in .log files\n",
    "    for log_file in directory.rglob(\"*.log\"):\n",
    "        \n",
    "        has_error = False\n",
    "        has_warning = False\n",
    "\n",
    "        unique_key = log_file.parent.name \n",
    "        \n",
    "        with log_file.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                if \"ERROR\" in line:\n",
    "                    has_error = True\n",
    "                    break  # stop reading; error takes priority\n",
    "                elif \"WARNING\" in line:\n",
    "                    has_warning = True\n",
    "\n",
    "        # Decide classification\n",
    "        if has_error:\n",
    "            errors.add(unique_key)\n",
    "            invalid_sims += 1\n",
    "        elif has_warning:\n",
    "            warnings.add(unique_key)\n",
    "        \n",
    "\n",
    "    # --- Print results nicely ---\n",
    "    print(\"\\nüîç Log scan summary\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "\n",
    "    if errors:\n",
    "        print(f\"‚ùå Simulations displaying errors ({len(errors)}):\")\n",
    "        for e in sorted(errors):\n",
    "            print(f\"   - {e}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No simulations displaying errors.\")\n",
    "\n",
    "    if warnings:\n",
    "        print(f\"\\n‚ö†Ô∏è  Simulations displaying warnings ({len(warnings)}):\")\n",
    "        for w in sorted(warnings):\n",
    "            print(f\"   - {w}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No simulations displaying warnings.\")\n",
    "\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"Total invalid simulations: {invalid_sims}\")\n",
    "\n",
    "simulation_validator(\"/u/vld/scat9451/main_project/LAMMPS_simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- ANALYSIS SCRIPT---------------\n",
    "\n",
    "# This script searches the LAMMPS_simulations directory and \n",
    "# 1. Identifies simulations that have not yet been analysed\n",
    "# 2. Creates ovito files and renderings\n",
    "# 3. Runs the following analyses as functions of density :\n",
    "#   a) % of sp, sp2, sp3 environments \n",
    "#   c) Histogram of ring sizes for a given density\n",
    "#   d) Frequency plots for n-mem rings\n",
    "#   e) Radial Distribution Functions\n",
    "#   f) Potential Energy\n",
    "#   g) Bond Length\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import ovito\n",
    "from ovito.io import import_file\n",
    "from ovito.modifiers import CreateBondsModifier, FindRingsModifier, CoordinationAnalysisModifier, ColorCodingModifier, BondAnalysisModifier\n",
    "from ovito.vis import Viewport, TachyonRenderer, ColorLegendOverlay, BondsVis\n",
    "from ovito.qt_compat import QtCore\n",
    "\n",
    "# ------ MAKE NEW DIRECTORIES ------\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ovito_dir = analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(exist_ok=True)\n",
    "\n",
    "structural_analysis_dir = analysis_dir / \"Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(exist_ok=True)\n",
    "# ----------------------------------\n",
    "\n",
    "# ------  IMPORT SIMULATION DATA ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dictionary sorted_imported_simulation_files = {unique_key: [sorted list of dump_file path objects]} \n",
    "# 3. This can be loaded like so: \n",
    "#   a) first item: unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "#   b) loop through all items: for unique_key, dump_files in imported_simulation_files.items():\n",
    "\n",
    "# NOTE: The unique_key is generated from the grandparent of the dumpfiles\n",
    "# This function expects the following file structure, \"dump_custom.C.00000\" regex and unique_key regex:\n",
    "#\n",
    "# <unique_key>/                     \n",
    "#         ‚îî‚îÄ‚îÄ NVT/                       \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00000.dat  \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00001.dat\n",
    "root_directory = Path(\"/u/vld/scat9451/main_project/\")\n",
    "def import_simulation_data(directory):\n",
    "\n",
    "    dump_file_name = re.compile(r\"^dump_custom\\.C\\.(\\d+)\\.dat$\") # Dump file regex\n",
    "    unique_key_pattern = re.compile(r\"^[A-Za-z]+_[A-Za-z0-9]+_[A-Za-z]+_\\d+_\\d+(?:\\.\\d+)?_\\d+$\") # Unique key regex\n",
    "    \n",
    "    directory = Path(directory)\n",
    "\n",
    "    imported_simulation_files = defaultdict(list) # Imported files dictionary\n",
    "\n",
    "    imported_files_counter = 0\n",
    "    skipped_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "        \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = dump_file_name.match(path.name) # Enforce dump_file file naming\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        parent = path.parent\n",
    "        \n",
    "        if parent.name != \"NVT\": # Enforce NVT file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Parent directory for {path}, {parent} is not equal to NVT\")\n",
    "            continue\n",
    "\n",
    "        grandparent = parent.parent\n",
    "        if not unique_key_pattern.match(grandparent.name): # Enforce unique_key file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Invalid unique_key name format '{grandparent.name}'\")\n",
    "            continue\n",
    "        if not grandparent.name: # Protect against missing grandparent\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: No grandparent directory for {path}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        unique_key = grandparent.name\n",
    "        numeric_index = int(m.group(1))\n",
    "\n",
    "        imported_simulation_files[unique_key].append((numeric_index, path))\n",
    "        imported_files_counter += 1\n",
    "\n",
    "    # sort each list by numeric index and drop the numeric index in final structure\n",
    "    sorted_imported_simulation_files = {}\n",
    "    for key, items in imported_simulation_files.items():\n",
    "        items.sort(key=lambda pair: pair[0])  # sort by numeric_index\n",
    "        paths_sorted = [p for _, p in items]\n",
    "        sorted_imported_simulation_files[key] = paths_sorted\n",
    "\n",
    "    if imported_files_counter:\n",
    "        print(f\"Imported {imported_files_counter} dump files\")\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} dump files due to errors\")\n",
    "\n",
    "    return sorted_imported_simulation_files\n",
    "\n",
    "imported_simulation_files = import_simulation_data(\"LAMMPS_simulations\")\n",
    "print(f\"Imported {len(imported_simulation_files)} LAMMPS simulation files\")\n",
    "\n",
    "# Sets up an empty pipeline for each successive function to use\n",
    "def empty_ovito_pipeline(imported_simulation_files):\n",
    "\n",
    "    # Clear existing pipeline\n",
    "    for p in list(ovito.scene.pipelines):\n",
    "        p.remove_from_scene()\n",
    "\n",
    "    if not imported_simulation_files:\n",
    "        raise ValueError(\"No datafiles provided to empty_ovito_pipeline()\")\n",
    "    \n",
    "    # Load the first item in the dictionary \n",
    "    unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "\n",
    "    if not dump_file:\n",
    "        raise ValueError(f\"No dump files found for simulation '{unique_key}'\")\n",
    "    \n",
    "    pipeline = import_file(dump_file)\n",
    "    \n",
    "    return pipeline\n",
    "pipeline = empty_ovito_pipeline(imported_simulation_files)\n",
    "\n",
    "# Data visualisation in Ovito\n",
    "def ovito_analysis(data_dict, pipeline):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided to ovito_analysis()\")\n",
    "\n",
    "    # ------- ANALYSIS OF IMPORTED FILES ------------\n",
    "    # BUG: Image and video renderers error with: \n",
    "    # \"RuntimeError: Visual element 'Rings' reported an error:Failed to build non-periodic representation of periodic surface mesh. Periodic domain might be too small.\" if ring mod is included.\n",
    "\n",
    "    # Bond Modifier and Visuals \n",
    "    bond_modifier = CreateBondsModifier(cutoff=1.85)\n",
    "    bond_modifier.vis.width = 0.15\n",
    "    bond_modifier.vis.coloring_mode = BondsVis.ColoringMode.Uniform\n",
    "    bond_modifier.vis.color = (0.5, 0.5, 0.5)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Coordination Modifier and Colour Coding\n",
    "    pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff=1.85))\n",
    "    colour_coding_mod = ColorCodingModifier(property=\"Coordination\",start_value=1.0,end_value=4.0,gradient=ColorCodingModifier.Viridis(),discretize_color_map=True)\n",
    "    pipeline.modifiers.append(colour_coding_mod)\n",
    "\n",
    "    # Add to Scene\n",
    "    pipeline.add_to_scene()\n",
    "\n",
    "    # Viewing settings\n",
    "    vp = Viewport()\n",
    "    vp.type = Viewport.Type.Perspective\n",
    "\n",
    "    # Coordination Legend\n",
    "    legend = ColorLegendOverlay(\n",
    "        title = \"Coordination\",\n",
    "        modifier = colour_coding_mod,\n",
    "        alignment = QtCore.Qt.AlignmentFlag.AlignHCenter | QtCore.Qt.AlignmentFlag.AlignBottom,\n",
    "        orientation = QtCore.Qt.Orientation.Horizontal,\n",
    "        font_size = 0.1,\n",
    "        format_string = '%.0f' \n",
    "        )\n",
    "    vp.overlays.append(legend)\n",
    "\n",
    "    # Note: this function only renders for the first repeat \n",
    "    def is_run_1_(run_file_name):\n",
    "        return re.match(r\".*1$\", run_file_name) is not None\n",
    "\n",
    "    # Skipped/Created file counters\n",
    "    skipped_ovito_files_counter = 0\n",
    "    skipped_png_files_counter = 0\n",
    "    skipped_avi_files_counter = 0\n",
    "    created_ovito_files_counter = 0\n",
    "    created_png_files_counter = 0\n",
    "    created_avi_files_counter = 0\n",
    "    \n",
    "\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        tachyon = TachyonRenderer(shadows=False, direct_light_intensity=1.1)\n",
    "\n",
    "\n",
    "        if is_run_1_(unique_key): # Only does analysis for run_1_\n",
    "            \n",
    "            ovito_save_file = analysis_dir / f\"{unique_key}.ovito\"\n",
    "            \n",
    "            # Ovito File Existance-Checker\n",
    "            ovito_exists = any(analysis_dir.rglob(ovito_save_file.name))\n",
    "\n",
    "            if ovito_exists:\n",
    "                skipped_ovito_files_counter += 1\n",
    "                continue\n",
    "            \n",
    "            pipeline.source.load(dump_files)\n",
    "\n",
    "            # Set particle scaling (datafile specific)\n",
    "            n_frames = pipeline.source.num_frames\n",
    "            final_frame = max(0, n_frames - 1)\n",
    "            data = pipeline.compute(frame = final_frame)\n",
    "            data.particles.vis.scaling = 0.3\n",
    "\n",
    "            # Set Zoom\n",
    "            vp.zoom_all()\n",
    "\n",
    "            ovito.scene.save(ovito_save_file)\n",
    "            created_ovito_files_counter += 1   \n",
    "\n",
    "        if is_run_1_(unique_key): # Only does analysis for run_1_\n",
    "             \n",
    "            img_save_file = analysis_dir / f\"{unique_key}.png\"\n",
    "            img_save_file_str = str(img_save_file)\n",
    "            \n",
    "            # Ovito File Existance-Checker\n",
    "            img_exists = any(analysis_dir.rglob(img_save_file.name))\n",
    "\n",
    "            if img_exists:\n",
    "                skipped_png_files_counter += 1\n",
    "                continue\n",
    "            \n",
    "            pipeline.source.load(dump_files)\n",
    "\n",
    "            # Set particle scaling (datafile specific)\n",
    "            n_frames = pipeline.source.num_frames\n",
    "            final_frame = max(0, n_frames - 1)\n",
    "            data = pipeline.compute(frame = final_frame)\n",
    "            data.particles.vis.scaling = 0.3\n",
    "\n",
    "            # Set Zoom\n",
    "            vp.zoom_all()\n",
    "            \n",
    "            vp.render_image(size=(1920,1080),\n",
    "                            filename=img_save_file_str,\n",
    "                            background=(1,1,1),\n",
    "                            frame=final_frame,\n",
    "                            renderer=tachyon)\n",
    "            created_png_files_counter += 1 \n",
    "              \n",
    "\n",
    "        if is_run_1_(unique_key): # Only does analysis for run_1_\n",
    "            \n",
    "            vid_save_file   = analysis_dir / f\"{unique_key}.avi\"\n",
    "            vid_save_file_str = str(vid_save_file)                                     \n",
    "\n",
    "            # File Existance-Checker\n",
    "            vid_exists   = any(analysis_dir.rglob(vid_save_file.name))\n",
    "            if vid_exists:\n",
    "                skipped_avi_files_counter += 1\n",
    "                continue\n",
    "\n",
    "            pipeline.source.load(dump_files)\n",
    "\n",
    "            # Set particle scaling (datafile specific)\n",
    "            n_frames = pipeline.source.num_frames\n",
    "            final_frame = max(0, n_frames - 1)\n",
    "            data = pipeline.compute(frame = final_frame)\n",
    "            data.particles.vis.scaling = 0.3\n",
    "\n",
    "            # Set Zoom\n",
    "            vp.zoom_all()\n",
    "\n",
    "            vp.render_anim(size=(1920,1080), \n",
    "                        filename=vid_save_file_str, \n",
    "                        fps=10,\n",
    "                        renderer=tachyon)\n",
    "            created_avi_files_counter += 1  \n",
    "\n",
    "\n",
    "    # Print Skipped/Created files\n",
    "    if skipped_ovito_files_counter:\n",
    "        print(f\"Skipped {skipped_ovito_files_counter} existing .ovito files\")\n",
    "    if skipped_png_files_counter:\n",
    "        print(f\"Skipped {skipped_png_files_counter} existing .png files\")\n",
    "    if skipped_avi_files_counter:\n",
    "        print(f\"Skipped {skipped_avi_files_counter} existing .avi files\")\n",
    "    \n",
    "    if created_ovito_files_counter:\n",
    "        print(f\"Created {created_ovito_files_counter} .ovito files\")\n",
    "    if created_png_files_counter:\n",
    "        print(f\"Created {created_png_files_counter} .png files\")\n",
    "    if created_avi_files_counter:\n",
    "        print(f\"Created {created_avi_files_counter} .avi files\")\n",
    "    \n",
    "    # Remove modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "\n",
    "# ------ DATA GENERATION FUNCTIONS ------\n",
    "# file_analysis(): \n",
    "#   1. uses imported_simulation_files from import_simulation_data()\n",
    "#   2. uses the pipeline from empty_ovito_pipeline(): no modifiers by default\n",
    "#   3. checks if files already exist in \"Structural Analysis\"\n",
    "#   4. loads each file in datafiles into the existing pipeline\n",
    "#   5. computes a specified data object for the given pipeline on each file and saves to a file name given by the \"unique_key\" + \"data_tag\"\n",
    "#   NOTE:\n",
    "#       a) requires \"data_tag\": e.g. \"bond_length_data.txt\" or \"RDF_data.txt\" (include file suffix, e.g. \".txt\")\n",
    "#       b) \"data_function\" refers to the ovito function that return the desired data object \n",
    "#               e.g. \"data.particles['Coordination']\" or \"data.tables['coordination-rdf'].xy()\" or \"data.particles[\"c_pea\"]\" \n",
    "#       c) requires use of the \"lambda data:\" syntax for creating a throwaway function\n",
    "#               e.g. When calling this func, use \"file_analysis_and_existance_checker(datafiles,\"ring_data\",lambda data: data.tables[\"ring-size-histogram\"].xy())\"\"\n",
    "\n",
    "def file_analysis(data_dict, pipeline, data_tag, data_function):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided\")\n",
    "    \n",
    "    # Skipped file counter\n",
    "    skipped_files_counter = 0\n",
    "    created_files_counter = 0\n",
    "\n",
    "    # ----- STRUCTURAL ANALYSIS -----\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        # File Name\n",
    "        data_file_name = analysis_dir / f\"{unique_key}_{data_tag}\"\n",
    "\n",
    "        # Structural Analysis File Existance-Checker\n",
    "        data_exists = any(analysis_dir.rglob(data_file_name.name))\n",
    "        if data_exists and not REPLACE_OLD_FILES:\n",
    "            skipped_files_counter += 1\n",
    "            continue \n",
    "\n",
    "        # Load new file into the pipeline and compute data for final frame        \n",
    "        pipeline.source.load(dump_files)\n",
    "        n_frames = pipeline.source.num_frames\n",
    "        final_frame = max(0, n_frames - 1)\n",
    "        data = pipeline.compute(frame = final_frame)\n",
    "\n",
    "        # Data\n",
    "        specific_data = data_function(data)\n",
    "        np.savetxt(data_file_name, specific_data, delimiter=\",\", fmt=\"%.6f\")\n",
    "        created_files_counter += 1\n",
    "\n",
    "    # Print Skipped/Created Files\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} existing {data_tag} files\")\n",
    "    if created_files_counter:\n",
    "        print(f\"Created {created_files_counter} {data_tag} files\")    \n",
    "\n",
    "def list_attributes(pipeline):\n",
    "    data = pipeline.compute()\n",
    "    print(\"Per Particle Attributes:\")\n",
    "    for prop in data.particles.keys():\n",
    "        print(prop)\n",
    "\n",
    "def ring_analysis(data_dict, pipeline, min_ring_size, max_ring_size, bond_length):\n",
    "    \n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "    \n",
    "    # Ring Analysis Modifier\n",
    "    ring_mod = FindRingsModifier(minimum_ring_size=min_ring_size, maximum_ring_size=max_ring_size)\n",
    "    pipeline.modifiers.append(ring_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"ring.txt\", lambda data: data.tables[\"ring-size-histogram\"].xy())\n",
    "\n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "def coordination_analysis(data_dict, pipeline, coordination_cutoff):\n",
    "    \n",
    "    # Coordination Analysis Modfier\n",
    "    coord_mod = CoordinationAnalysisModifier(cutoff=coordination_cutoff)\n",
    "    pipeline.modifiers.append(coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"coordination.txt\", lambda data: data.particles['Coordination'])\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()\n",
    "    \n",
    "def energy_analysis(data_dict, pipeline):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"potential_energy.txt\", lambda data: data.particles[\"c_pea\"])\n",
    "\n",
    "def RDF_analysis(data_dict, pipeline, RDF_cutoff, bins):\n",
    "    \n",
    "    # Coordination Analysis Modfier for RDF\n",
    "    RDF_coord_mod = CoordinationAnalysisModifier(cutoff=RDF_cutoff, number_of_bins=bins)\n",
    "    pipeline.modifiers.append(RDF_coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"RDF.txt\", lambda data: data.tables['coordination-rdf'].xy())\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()    \n",
    "\n",
    "def bond_length_analysis(data_dict, pipeline, bins, bond_length, bond_length_analysis_cutoff):\n",
    "\n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Bond Analysis Modifier\n",
    "    bond_analysis_mod = BondAnalysisModifier(bins = bins, length_cutoff=bond_length_analysis_cutoff)\n",
    "    pipeline.modifiers.append(bond_analysis_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"bond_length.txt\", lambda data: data.tables[\"bond-length-distr\"].xy())\n",
    "  \n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    " \n",
    "def force_analysis(data_dict, pipeline):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"forces.txt\", lambda data: data.particles[\"Force\"])\n",
    "\n",
    "# ----- POSSIBLE ANALYSIS ------\n",
    "# 2. Bond Angle\n",
    "# 3. Conditional analysis (i.e. for sp/sp2/sp3 individually)\n",
    "# 4. Young's Modulus\n",
    "# 5. Coordination for each frame in a given sim plotted against simulation time\n",
    "# ------------------------------\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will regenerate ALL files (apart from renders)\n",
    "REPLACE_OLD_FILES = False\n",
    "\n",
    "if REPLACE_OLD_FILES:\n",
    "    confirm = input(\"Are you sure you want to replace old files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "#list_attributes(pipeline)\n",
    "force_analysis(imported_simulation_files, pipeline)\n",
    "bond_length_analysis(imported_simulation_files, pipeline, bins=1000, bond_length = 1.85, bond_length_analysis_cutoff=2.0)\n",
    "RDF_analysis(imported_simulation_files, pipeline, RDF_cutoff=6.0, bins=200)\n",
    "ring_analysis(imported_simulation_files, pipeline, min_ring_size=3, max_ring_size=24, bond_length=1.85)\n",
    "coordination_analysis(imported_simulation_files, pipeline, coordination_cutoff=1.85)\n",
    "energy_analysis(imported_simulation_files, pipeline)\n",
    "# ovito_analysis(imported_simulation_files, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FILE ORGANISER -----\n",
    "import re, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Assign Directories\n",
    "cwd = Path.cwd()\n",
    "\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "ovito_dir = analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "ovito_file_data_tags = [\".ovito\", \".png\", \".avi\"]\n",
    "structural_analysis_file_data_tags = [\"bond_length.txt\", \"coordination.txt\", \"potential_energy.txt\", \"RDF.txt\", \"ring.txt\", \"forces.txt\"]\n",
    "\n",
    "structural_analysis_dir = analysis_dir / \"Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "#structural_analysis_file_tags = [\"\"] <--- This may be worth implementing if more than 2 folders are used in the future\n",
    "\n",
    "# Regex pattern for reading \"unique_key\" + \"data_tag\"\"\n",
    "data_file_name = re.compile(\n",
    "    r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "    r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "    r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "    r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "    r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "    r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "    r'(?:_(?P<data_tag>.+)|(?P<data_tag2>\\..+))$'   # e.g. ring.txt or .png (allows underscore after run_number or .avi etc...)   \n",
    ")\n",
    "\n",
    "# General function for moving a file with overwrite function\n",
    "def directory_move(file, destination_dir):\n",
    "\n",
    "    file=Path(file)\n",
    "    if not file.exists():\n",
    "        print(f\"ERROR: {file} does not exist\")\n",
    "        return \"missing\"\n",
    "    \n",
    "    # Make Destination Directory\n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # New Path with existance check\n",
    "    moved_file = destination_dir / file.name\n",
    "    if moved_file.exists(): \n",
    "        if not OVERWRITE:\n",
    "            return \"skipped\"\n",
    "        else:\n",
    "            moved_file.unlink()\n",
    "            shutil.move(str(file), str(moved_file))\n",
    "            return \"overwritten\"\n",
    "\n",
    "    else:\n",
    "        shutil.move(str(file), str(moved_file))\n",
    "        return \"success\"\n",
    "\n",
    "# Robust, general function for sorting all files using the unique_key and directory_move()\n",
    "def sort_directory(working_directory):\n",
    "\n",
    "    sorted_files = 0\n",
    "    skipped_files = 0\n",
    "    overwritten_files = 0\n",
    "    missing_files = 0\n",
    "    unrecognized_data_tags = 0\n",
    "\n",
    "    for file in working_directory.rglob(\"*\"): # searches working directory for directories contained in it\n",
    "        \n",
    "        directory = Path(file)\n",
    "        \n",
    "        if not directory.is_file(): # select for files only (data files)\n",
    "            continue\n",
    "\n",
    "        m = data_file_name.match(directory.name)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        # Parse each element of the unique key\n",
    "        element_symbol = m.group(\"element_symbol\")\n",
    "        if element_symbol == \"C\":\n",
    "            element_name = \"Carbon\"\n",
    "        else:\n",
    "            print(f\"Unrecognized element symbol for {directory}. Skipping file. \\nAdd element_symbol --> element_name mapping\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        potential_name = m.group(\"potential_name\")\n",
    "        simulation_type = m.group(\"simulation_type\")\n",
    "        num_atoms = int(m.group(\"num_atoms\"))\n",
    "        density = m.group(\"density\")\n",
    "        \n",
    "        data_tag =  (m.group(\"data_tag\") or m.group(\"data_tag2\") or \"\")\n",
    "\n",
    "        # Defaults to not moving files\n",
    "        destination_dir = None\n",
    "        \n",
    "        # Moves ovito files into ovito_dir\n",
    "        if data_tag in ovito_file_data_tags:   \n",
    "            # Destination: evaluated using the unique key\n",
    "            destination_dir = (\n",
    "                ovito_dir\n",
    "                / f\"Element: {element_name}\"\n",
    "                / f\"Potential: {potential_name}\"\n",
    "                / f\"Type: {simulation_type}\"\n",
    "                / f\"Atoms: {num_atoms}\"\n",
    "                / f\"Density: {density}\"\n",
    "            )\n",
    "        \n",
    "        # Moves structural data files to structural_analysis_dir \n",
    "        elif data_tag in structural_analysis_file_data_tags:\n",
    "            # Destination: evaluated using the unique key\n",
    "            destination_dir = (\n",
    "                structural_analysis_dir\n",
    "                / f\"Element: {element_name}\"\n",
    "                / f\"Potential: {potential_name}\"\n",
    "                / f\"Type: {simulation_type}\"\n",
    "                / f\"Atoms: {num_atoms}\"\n",
    "                / f\"Density: {density}\"\n",
    "            )\n",
    "        \n",
    "        if destination_dir is None:\n",
    "            unrecognized_data_tags +=1\n",
    "            print(f\"Unrecognized data_tag: {data_tag}\")\n",
    "            continue\n",
    "        \n",
    "        status = directory_move(directory, destination_dir)\n",
    "        \n",
    "        if status == \"success\":\n",
    "            sorted_files += 1\n",
    "        elif status == \"skipped\":\n",
    "            skipped_files += 1\n",
    "        elif status == \"overwritten\":\n",
    "            overwritten_files += 1\n",
    "        elif status == \"missing\":\n",
    "            missing_files += 1\n",
    "    \n",
    "    if sorted_files:\n",
    "        print(f\"Sorted {sorted_files} files\")\n",
    "    if skipped_files:\n",
    "        print(f\"Skipped {skipped_files} existing files\")\n",
    "    if missing_files:\n",
    "        print(f\"{missing_files} missing files\")\n",
    "    if overwritten_files:\n",
    "        print(f\"{overwritten_files} failed files\")\n",
    "    if unrecognized_data_tags:\n",
    "        print(f\"{unrecognized_data_tags} unrecognized data tags\")\n",
    "\n",
    "    if not sorted_files and not skipped_files and not missing_files and not failed_files:\n",
    "        print(f\"No matching run files found in {working_directory}. Change run file regex if required.\")\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will replace ALL existing files \n",
    "OVERWRITE = False\n",
    "\n",
    "if OVERWRITE:\n",
    "    confirm = input(\"Are you sure you want to overwrite existing files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "# Organise ovito files and structural_analysis files\n",
    "sort_directory(analysis_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ GRAPHICAL ANALYSIS --------\n",
    "\n",
    "# Graphical data points are means of all repeat runs with errors given as 1 standard deviation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Create Graphical Analysis Directories\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "cwd = Path.cwd()\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_dir = analysis_dir / \"Graphical Analysis\"\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "potential_comparison_dir = graph_dir / \"Potential Comparison Plots\"\n",
    "potential_comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ------ FIGURE FORMATTING ------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('1_column_fig.mplstyle')\n",
    "# -------------------------------\n",
    "\n",
    "# ------ IMPORT DATA FILES ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dictionary:\n",
    "#    imported_data_files = {unique_data_key : [list of (density, Path), ...]} \n",
    "#    only includes files with the specified \"data_tag\" \n",
    "#    Where unique_data_key = \"unique_key (without density or repeat) e.g. C_GAP17_NVT_64\n",
    "#    Therefore only imports files with the corresponding data_tag\n",
    "# 3. If specify_density = True, it will only import files with density in key_analysis_densities\n",
    "key_analysis_densities = [1.5, 2.0, 2.5, 3.0, 3.5]\n",
    "def import_data_files(directory, data_tag, specify_density):\n",
    "    \n",
    "    directory = Path(directory)\n",
    "\n",
    "    # Regex pattern for reading \"unique_key\" + \"data_tag\"\"\n",
    "    data_file_name = re.compile(\n",
    "    r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "    r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "    r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "    r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "    r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "    r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "    r'(?:_(?P<data_tag>.+)|(?P<data_tag2>\\..+))$'   # e.g. ring.txt or .png (allows underscore after run_number or .avi etc...)   \n",
    "    )  \n",
    "\n",
    "    imported_data_files = defaultdict(list) # Imported files dictionary\n",
    "\n",
    "    skipped_data_files_counter = 0\n",
    "    imported_data_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "    \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = data_file_name.match(path.name) # Enforce data file naming\n",
    "        if not m:\n",
    "            print(f\"ERROR: Skipped {path}. Invalid data file name\")\n",
    "            skipped_data_files_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Parse \"unique_key\" + \"data_tag\" components\n",
    "        element_symbol    = m.group(\"element_symbol\")\n",
    "        potential_name    = m.group(\"potential_name\")\n",
    "        simulation_type   = m.group(\"simulation_type\")\n",
    "        num_atoms         = m.group(\"num_atoms\")\n",
    "        density           = m.group(\"density\")\n",
    "        run_number        = m.group(\"run\")\n",
    "        file_data_tag     = m.group(\"data_tag\")\n",
    "\n",
    "        # only import files with the correct data_tag\n",
    "        if file_data_tag != data_tag:\n",
    "            continue\n",
    "        \n",
    "        # Functionality for only importing files with a specific density\n",
    "        density = float(density)\n",
    "        if specify_density and density not in key_analysis_densities:\n",
    "            continue\n",
    "\n",
    "        # Construct unique_data_key\n",
    "        unique_data_key = f\"{element_symbol}_{potential_name}_{simulation_type}_{num_atoms}\"\n",
    "\n",
    "        # Append the density to the list\n",
    "        imported_data_files[unique_data_key].append((density, path))\n",
    "        imported_data_files_counter += 1\n",
    "\n",
    "    # Sort by density for each key\n",
    "    for key, items in imported_data_files.items():\n",
    "        try:\n",
    "            items_sorted = sorted(items, key=lambda pair: pair[0])\n",
    "            imported_data_files[key] = items_sorted\n",
    "        except Exception:\n",
    "            print(f\"ERROR: Failed to sort density list for {key}\")\n",
    "\n",
    "    if import_data_files:\n",
    "        print(f\"Imported {imported_data_files_counter} {data_tag} files\")\n",
    "    if skipped_data_files_counter:\n",
    "        print(f\"Skipped {skipped_data_files_counter} {data_tag} files\")\n",
    "    return imported_data_files\n",
    "#-------------------------------\n",
    "\n",
    "# ------ DATA ANALYSIS ----------\n",
    "# Imports data using import_data_files()\n",
    "# Manipulates data using specified unique_data_function to return dependent variables\n",
    "# OPTIONS:\n",
    "# 1. potential_comaparison = False:\n",
    "#   a) specify_density = False: returns plots of scalar values (e.g. potential energy) against density for each unique key\n",
    "#   b) specify_density = True: returns plots of arrays (e.g. g(r) vs r) for each density in \"key_analysis_densities\" for each (element, potential, sim_type, num_atoms) key\n",
    "# 2. Potential comparison = True: \n",
    "#   a) performs the same analysis but overlays plots of each different potential\n",
    "# Default is specify_density=False, which then runs a analysis with Density as the default independent variable\n",
    "# Allows you to specify_density will will return 1 graph per density in key_analysis_densities\n",
    "def by_density_data_analysis(directory, data_tag, unique_data_function, save_file_name, chart_title, x_label, y_label, specify_density=False):\n",
    "    \n",
    "    # Import Data Files\n",
    "    imported_data_files = import_data_files(directory, data_tag, specify_density=specify_density)\n",
    "    if not imported_data_files:\n",
    "        return None   \n",
    "\n",
    "    # Safety check to ensure not parsing paths when trying to name graphs\n",
    "    base = Path(save_file_name).name\n",
    "\n",
    "    # Used for potential comparison\n",
    "    unique_data_key_dict = {}         # Dictionary of {unique_data_key: agg_df}, where agg_df is a processed data_frame including mean and std\n",
    "\n",
    "    # Loop over all unique keys and create per_density and specific density plots. Adds processed data to the unique_data_key_dict\n",
    "    for unique_data_key, entries in imported_data_files.items():\n",
    "\n",
    "        results = [] # In the form: (density (unique_data_function_output: (independent_variable (None by default), dependent_variable)))\n",
    "        \n",
    "        # Loop over all MD runs for different densities and repeats that match the unique key\n",
    "        for density, path in entries: \n",
    "                        \n",
    "            try:\n",
    "                data = np.loadtxt(path, delimiter=',')\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {path}; unable to load: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Failsafe in case there's no data in the file\n",
    "            if data is not None and getattr(data, \"size\", None) == 0:\n",
    "                print(f\"Data file {path} is empty\")\n",
    "                continue\n",
    "\n",
    "            # Performs data analysis function\n",
    "            try:\n",
    "                unique_data_function_output = unique_data_function(data)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: failed unique_data_function for {path}:\")\n",
    "                unique_data_function_output = None\n",
    "            \n",
    "            if unique_data_function_output is None:\n",
    "                continue            \n",
    "\n",
    "            if specify_density:\n",
    "\n",
    "                # Check that the unique data_function is returning correct data format: (independent_variable, dependent_variable)\n",
    "                if not (hasattr(unique_data_function_output, \"__iter__\") and not isinstance(unique_data_function_output, (str, bytes))):\n",
    "                    print(f\"Invalid output for {path}: expected (independent, dependent), got {type(unique_data_function_output)}\")\n",
    "                    continue\n",
    "                if not hasattr(unique_data_function_output, \"__len__\") or len(unique_data_function_output) != 2:\n",
    "                    print(f\"Invalid output length for {path}: expected 2, got {len(unique_data_function_output) if hasattr(unique_data_function_output, '__len__') else 'N/A'}\")\n",
    "                    continue\n",
    "\n",
    "                independent_variable, dependent_variable = unique_data_function_output\n",
    "                results.append((float(density), independent_variable, dependent_variable))\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # Check that the unique data_function is returning correct data format: scalar value (dependent_variable)\n",
    "                if not np.isscalar(unique_data_function_output):\n",
    "                    print(\"Error: function should return a single number.\")\n",
    "                    continue\n",
    "\n",
    "                dependent_variable = unique_data_function_output\n",
    "                results.append((float(density), float(dependent_variable)))\n",
    "\n",
    "        if not results:\n",
    "            print(f\"Failed to analyse data for {unique_data_key}\")\n",
    "            continue\n",
    "        \n",
    "        save_file_dir = graph_dir / Path(unique_data_key)\n",
    "        save_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        final_name = f\"{unique_data_key}_{base}\"\n",
    "        save_path = save_file_dir / final_name\n",
    "        \n",
    "        # Handling returning arrays/lists or scalars from the unique_data_function\n",
    "        if specify_density:\n",
    "\n",
    "            # Sample first value\n",
    "            first_ind = results[0][1]   # independent_variable from first result\n",
    "            first_dep = results[0][2]   # dependent_variable from first result\n",
    "            \n",
    "            #Lists/ Arrays\n",
    "            if (isinstance(first_ind, (list, np.ndarray)) and isinstance(first_dep, (list, np.ndarray))):\n",
    "                \n",
    "                frames = []\n",
    "                for density, independent_variable, dependent_variable in results:\n",
    "                    \n",
    "                    ind_arr = np.asarray(independent_variable)\n",
    "                    dep_arr = np.asarray(dependent_variable)\n",
    "\n",
    "                    # Check for length mismatch    \n",
    "                    if ind_arr.shape[0] != dep_arr.shape[0]:\n",
    "                        print(f\"ERROR: Length mismatch for density {density}: indep {ind_arr.shape} vs dep {dep_arr.shape}; skipping this entry.\")\n",
    "                        continue\n",
    "                    \n",
    "                    df_local = pd.DataFrame({\n",
    "                        \"density\": [float(density)] * ind_arr.shape[0],\n",
    "                        \"independent_variable\": ind_arr,\n",
    "                        \"dependent_variable\": dep_arr\n",
    "                    })\n",
    "                    frames.append(df_local)\n",
    "                \n",
    "                if not frames:\n",
    "                    print(f\"No valid array entries for {unique_data_key}\")\n",
    "                    continue\n",
    "\n",
    "                df = pd.concat(frames, ignore_index=True)\n",
    "                agg_df = (df.groupby([\"density\", \"independent_variable\"])[\"dependent_variable\"].agg(mean=\"mean\", std=\"std\").fillna(0.0).reset_index())\n",
    "          \n",
    "            else:\n",
    "                # Scalar values\n",
    "                df = pd.DataFrame(results, columns=[\"density\", \"independent_variable\", \"dependent_variable\"])\n",
    "                agg_df = (df.groupby([\"density\", \"independent_variable\"])[\"dependent_variable\"].agg(mean=\"mean\", std=\"std\").fillna(0.0).reset_index())\n",
    "\n",
    "\n",
    "            for density in agg_df[\"density\"].unique():\n",
    "                slice_df = agg_df[agg_df[\"density\"] == density].reset_index(drop=True)\n",
    "                per_density_chart_title = f\"{density}gcm {chart_title}\"\n",
    "                per_density_save_path = save_file_dir / f\"{density}_{final_name}\"\n",
    "                plot(set_plot_type, slice_df, x_label, y_label, per_density_chart_title, per_density_save_path, specify_density=specify_density)\n",
    "                print(f\"{final_name} created\")\n",
    "\n",
    "            # Organise Plots into subfolders\n",
    "            for path in graph_dir.rglob(f'*{save_file_name}'):\n",
    "                \n",
    "                parent = path.parent\n",
    "                sf = Path(save_file_name)\n",
    "                target_stem = sf.stem   # e.g. 'RDF'\n",
    "                sub_dir = parent / f\"{target_stem} plots\" \n",
    "                \n",
    "                # Don't move files once moved (avoid nesting)\n",
    "                if f\"{target_stem} plots\" in path.parts:\n",
    "                    continue\n",
    "\n",
    "                sub_dir.mkdir(parents = True, exist_ok=True)\n",
    "                dest = sub_dir / path.name\n",
    "                shutil.move(str(path), str(dest))       \n",
    "        \n",
    "        else:\n",
    "            df = pd.DataFrame(results, columns=[\"density\", \"dependent_variable\"])\n",
    "            agg_df = df.groupby(\"density\")[\"dependent_variable\"].agg(mean=\"mean\", std=\"std\").fillna(0.0).reset_index() \n",
    "\n",
    "            plot(set_plot_type, agg_df,x_label, y_label, chart_title, save_path, specify_density=specify_density)\n",
    "            print(f\"{final_name} created\")\n",
    "\n",
    "        \n",
    "        # Add agg_df to the unique_data_key_dict\n",
    "        unique_data_key_dict[unique_data_key] = agg_df\n",
    "\n",
    "    # Parsing unique_data_key_information\n",
    "    unique_data_key_name = re.compile(\n",
    "    r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "    r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "    r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "    r'(?P<num_atoms>\\d+)$'                          # e.g. 64\n",
    "    )\n",
    "\n",
    "    # Creates a new dictionary of {element, simulation_type, num_atoms : (potential, agg_df)}\n",
    "    per_potential_dict = {} \n",
    "    for unique_data_key, agg_df in unique_data_key_dict.items():\n",
    "        \n",
    "        match = unique_data_key_name.match(unique_data_key)\n",
    "        if not match:\n",
    "            print(f\"Unique_data_key name: {unique_data_key} does not match expected format. Change unique_data_key_name regex or file naming\")\n",
    "            continue\n",
    "\n",
    "        key_information = match.groupdict()\n",
    "        element = key_information[\"element_symbol\"]\n",
    "        potential = key_information[\"potential_name\"]\n",
    "        simulation_type = key_information[\"simulation_type\"]\n",
    "        num_atoms = int(key_information[\"num_atoms\"])\n",
    "\n",
    "        per_potential_key = (element, simulation_type, num_atoms)\n",
    "        per_potential_dict.setdefault(per_potential_key, {})\n",
    "\n",
    "        if potential in per_potential_dict[per_potential_key]:\n",
    "            per_potential_dict[per_potential_key][potential] = pd.concat(\n",
    "            [per_potential_dict[per_potential_key][potential], agg_df], ignore_index=True\n",
    "        )\n",
    "        else:\n",
    "            per_potential_dict[per_potential_key][potential] = agg_df.copy()\n",
    "\n",
    "    # Plots multi line comparison plots for each potential\n",
    "    for (element, simulation_type, num_atoms), value in per_potential_dict.items():\n",
    "        \n",
    "        # Print all different potentials found\n",
    "        if len(value) > 1:\n",
    "            print(f\"{element}_{simulation_type}_{num_atoms}: Datafiles found for {len(value)} different potentials\")\n",
    "            potentials = []\n",
    "            for potentials_dict in per_potential_dict.values():\n",
    "                potentials.extend(potentials_dict.keys())\n",
    "\n",
    "            print(sorted(set(potentials)))\n",
    "        else:\n",
    "            print(f\"{element}_{simulation_type}_{num_atoms}: Only 1 potential found, skipping potential comparison analysis\")\n",
    "            continue\n",
    "        \n",
    "        for potential, agg_df in value.items():\n",
    "\n",
    "            save_file_dir = potential_comparison_dir\n",
    "\n",
    "            final_name = f\"{element}_{simulation_type}_{num_atoms}_{base}\"\n",
    "            save_path = save_file_dir / final_name\n",
    "\n",
    "            if specify_density:\n",
    "                \n",
    "                for density in agg_df[\"density\"].unique():\n",
    "                    slice_df = agg_df[agg_df[\"density\"] == density].reset_index(drop=True)\n",
    "                    per_density_chart_title = f\"{density}gcm {chart_title}\"\n",
    "                    per_density_save_path = save_file_dir / f\"{density}_{final_name}\"\n",
    "                    plot(set_plot_type, slice_df, x_label, y_label, per_density_chart_title, \n",
    "                        per_density_save_path, specify_density=specify_density)\n",
    "                    print(f\"{final_name} created\")\n",
    "\n",
    "                # Organise Plots into subfolders\n",
    "                for path in graph_dir.rglob(f'*{save_file_name}'):\n",
    "                    \n",
    "                    parent = path.parent\n",
    "                    sf = Path(save_file_name)\n",
    "                    target_stem = sf.stem   # e.g. 'RDF'\n",
    "                    sub_dir = parent / f\"{target_stem} plots\" \n",
    "                    \n",
    "                    # Don't move files once moved (avoid nesting)\n",
    "                    if f\"{target_stem} plots\" in path.parts:\n",
    "                        continue\n",
    "\n",
    "                    sub_dir.mkdir(parents = True, exist_ok=True)\n",
    "                    dest = sub_dir / path.name\n",
    "                    shutil.move(str(path), str(dest))      \n",
    "            \n",
    "            else:\n",
    "                plot(set_plot_type, agg_df,x_label, y_label, chart_title, \n",
    "                    save_path, specify_density=specify_density)\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# ------ PLOTTING DATA ---------\n",
    "# Plot Types: marker (with error bars), line (with shaded regions) \n",
    "def plot(plot_type, data, x_label, y_label, chart_title, save_path, specify_density):\n",
    "\n",
    "    if plot_type not in existing_plot_types:\n",
    "        raise ValueError(f\"Unknown plot_type: {plot_type!r}\")\n",
    "\n",
    "    # Ensure Parent Directory Exists\n",
    "    save_path = Path(save_path)\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig, ax = plt.subplots()  # Local figure size for each plot\n",
    "    \n",
    "\n",
    "    if \"independent_variable\" in data.columns and specify_density:\n",
    "        x = data[\"independent_variable\"].values\n",
    "    else:\n",
    "        x = data[\"density\"].values\n",
    "\n",
    "    mean = data[\"mean\"].values\n",
    "    std = data[\"std\"].values\n",
    "    \n",
    "    if plot_type == \"marker\":\n",
    "        ax.errorbar(x, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5)\n",
    "    elif plot_type == \"line\":\n",
    "        alpha_fill = 0.25\n",
    "        ax.plot(x, mean, label=\"Mean\")\n",
    "        ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "                \n",
    "    # Labels and Titles\n",
    "    ax.set_xlabel(f\"{x_label}\")\n",
    "    ax.set_ylabel(f'{y_label}')\n",
    "    ax.set_title(f\"{chart_title}\")\n",
    "\n",
    "    # Save Plot to Graphical Analysis\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "\n",
    "\n",
    "def multi_plot(plot_type, per_potential_dict, x_label, y_label, chart_title, save_path, specify_density):\n",
    "\n",
    "    save_path = Path(save_path)\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for potential, agg_df in per_potential_dict.items():\n",
    "\n",
    "        if \"independent_variable\" in agg_df.columns and specify_density:\n",
    "            x = agg_df[\"independent_variable\"].values\n",
    "        else:\n",
    "            x = agg_df[\"density\"].values\n",
    "\n",
    "\n",
    "        mean = np.asarray(agg_df[\"mean\"])\n",
    "        std = np.asarray(agg_df[\"std\"])\n",
    "\n",
    "        if plot_type == \"marker\":\n",
    "            ax.errorbar(x, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5, label = potential)\n",
    "        \n",
    "        elif plot_type == \"line\":\n",
    "            alpha_fill = 0.25\n",
    "            ax.plot(x, mean, label=potential)\n",
    "            ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(chart_title)\n",
    "    ax.legend(title = \"Potential\")\n",
    "\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "# ------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Instructions: \n",
    "# 1. Assign data_tag, chat_title, save_file_name and y_label (RAW TEXT ONLY, NO PATHS)\n",
    "# 2. Create a function that computes a desired value per structure file and returns this value\n",
    "# 3. Call by_density_data_analysis, using your new function as its unique_data_function\n",
    "# 4. Set specify_density=True to plot for only 1 density (e.g. RDF, ring histogram)\n",
    "# 5. If specify_density = True, the unique data function MUST return: (independent_variable, dependent_variable)\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Coordination analysis \n",
    "def coordination_analysis(directory, coordination_number):\n",
    "    \n",
    "    # Label coordination number \n",
    "    mapping = {\n",
    "        2: (\"sp\", \"sp Carbon Proportion\"),\n",
    "        3: (\"sp2\", \"sp2 Carbon Proportion\"),\n",
    "        4: (\"sp3\", \"sp3 Carbon Proportion\")\n",
    "    }\n",
    "    env, y_label = mapping.get(coordination_number, (None, None))\n",
    "\n",
    "    if env is None:\n",
    "        print(\"ERROR: Coordination number should be between 2 and 4\")\n",
    "        env = f\"{coordination_number}_coordinate\"\n",
    "        y_label = f\"{coordination_number} coordinate atoms\"\n",
    "        \n",
    "    data_tag = \"coordination.txt\"\n",
    "    chart_title = f\"Coordination vs. Density\"\n",
    "    save_file_name =f\"{env}_coordination_proportion.png\"\n",
    "    x_label = \"Density (g/cm¬≥)\"\n",
    "    \n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def coord_function(data: np.ndarray):\n",
    "        return float((np.count_nonzero(data == coordination_number) / data.size))\n",
    "\n",
    "    by_density_data_analysis(directory, data_tag, coord_function, save_file_name, chart_title, x_label, y_label, specify_density=False)\n",
    "\n",
    "# Ring Size analysis\n",
    "def ring_analysis(directory, ring_size):\n",
    "        \n",
    "    data_tag = \"ring.txt\"\n",
    "    chart_title = f\"Number of {ring_size} Membered Rings vs. Density\"\n",
    "    save_file_name = f\"{ring_size}_ring_frequency.png\"\n",
    "    x_label = \"Density (g/cm¬≥)\"\n",
    "    y_label = f\"{ring_size} Membered Rings\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def ring_function(data: np.ndarray):\n",
    "        return float(data[data[:, 0] == ring_size, 1][0])\n",
    "    \n",
    "    by_density_data_analysis(directory, data_tag, ring_function, save_file_name, chart_title, x_label, y_label, specify_density=False)\n",
    "   \n",
    "# Potential energy analysis \n",
    "def potential_energy_analysis(directory):\n",
    "\n",
    "    data_tag = \"potential_energy.txt\"\n",
    "    chart_title = f\"Mean Potential Energy vs. Density\"\n",
    "    save_file_name = \"mean_PE.png\"\n",
    "    x_label = \"Density (g/cm¬≥)\"\n",
    "    y_label = 'Mean Potential Energy (eV)'\n",
    "    \n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def PE_function(data: np.ndarray):\n",
    "        return np.mean(data)\n",
    "    \n",
    "    by_density_data_analysis(directory, data_tag, PE_function, save_file_name, chart_title, x_label, y_label, specify_density=False)\n",
    "\n",
    "# Bond Length analysis\n",
    "def bond_length_analysis(directory):\n",
    "    \n",
    "    data_tag = \"bond_length.txt\"\n",
    "    chart_title = f\"Mean Bond Length vs. Density\"\n",
    "    save_file_name = \"mean_bond_length.png\"\n",
    "    x_label = \"Density (g/cm¬≥)\"\n",
    "    y_label = 'Mean Bond Length (√Ö)'\n",
    "\n",
    "    def bond_length_function (data: np.array):\n",
    "        return np.average(data[:, 0], weights=data[:, 1])\n",
    "\n",
    "    by_density_data_analysis(directory, data_tag, bond_length_function, save_file_name, chart_title, x_label, y_label, specify_density=False)\n",
    "\n",
    "# Force Analysis\n",
    "def force_analysis(directory):\n",
    "        \n",
    "    data_tag = \"forces.txt\"\n",
    "    chart_title = \"Mean Force Magnitude vs. Density\"\n",
    "    save_file_name = \"mean_force_magnitude.png\"\n",
    "    x_label = \"Density (g/cm¬≥)\"\n",
    "    y_label = \"Mean Force Magnitude (eV/√Ö)\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def force_function(data: np.ndarray):\n",
    "        return np.mean(np.linalg.norm(data, axis=1))\n",
    "    \n",
    "    by_density_data_analysis(directory, data_tag, force_function, save_file_name, chart_title, x_label, y_label, specify_density=False)\n",
    "\n",
    "# RDF Analysis\n",
    "def RDF_analysis(directory):\n",
    "    \n",
    "    data_tag = \"RDF.txt\"\n",
    "    chart_title = \"Radial Distribution Function\"\n",
    "    save_file_name = \"RDF.png\"\n",
    "    y_label = \"g(r)\"\n",
    "    x_label = \"r (√Ö)\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def RDF_function(data: np.ndarray):\n",
    "        r_array = data[:, 0]\n",
    "        g_r_array = data[:, 1]\n",
    "        return (r_array, g_r_array)\n",
    "    \n",
    "    by_density_data_analysis(directory, data_tag, RDF_function, save_file_name, chart_title, x_label, y_label, specify_density=True)\n",
    "\n",
    "# Ring Histogram Analysis\n",
    "def ring_size_distribution_analysis(directory):\n",
    "    \n",
    "    data_tag = \"ring.txt\"\n",
    "    chart_title = \"Ring Size Distribution\"\n",
    "    save_file_name = \"ring_size_distribution.png\"\n",
    "    y_label = \"Frequency\"\n",
    "    x_label = \"Ring Size\"\n",
    "\n",
    "    def ring_distribution_function(data: np.ndarray):\n",
    "        ring_size_array = data[:,0]\n",
    "        frequency_array = data[:,1]\n",
    "        return (ring_size_array, frequency_array)\n",
    "    \n",
    "    by_density_data_analysis(directory, data_tag, ring_distribution_function, save_file_name, chart_title, x_label, y_label, specify_density=True)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------ ANALYSIS PARAMETERS ------\n",
    "existing_plot_types = [\"marker\", \"line\"]\n",
    "\n",
    "set_plot_type = \"line\"\n",
    "\n",
    "set_analysis_directory = \"Analysis/Structural Analysis\"\n",
    "\n",
    "set_coordination_number = 2\n",
    "set_ring_size = 6\n",
    "# ---------------------------------\n",
    "\n",
    "coordination_analysis(directory=set_analysis_directory, coordination_number=set_coordination_number)\n",
    "\n",
    "bond_length_analysis(directory=set_analysis_directory)\n",
    "\n",
    "potential_energy_analysis(directory=set_analysis_directory)\n",
    "\n",
    "ring_analysis(directory= set_analysis_directory, ring_size=set_ring_size)\n",
    "\n",
    "force_analysis(directory=set_analysis_directory)\n",
    "\n",
    "RDF_analysis(directory=set_analysis_directory)\n",
    "\n",
    "# ring_size_distribution_analysis(directory=set_analysis_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structure_analyser_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
