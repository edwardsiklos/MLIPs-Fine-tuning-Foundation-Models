{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451afa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SIMULATION VALIDATOR ------\n",
    "# Before running analysis it's important to check that all simulations have run as expected\n",
    "# ----------------------------------\n",
    "\n",
    "expected_number_of_dump_files_per_sim = 96             # =(total number of timesteps + 1)/100\n",
    "\n",
    "# This expects file structure:\n",
    "# <unique_key>/                     \n",
    "#         ‚îî‚îÄ‚îÄ NVT/                       \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00000.dat  \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00001.dat\n",
    "from pathlib import Path\n",
    "\n",
    "root_directory = Path(\"/u/vld/scat9451/main_project/\")\n",
    "def simulation_validator(directory):\n",
    "\n",
    "    directory = Path(directory)  \n",
    "\n",
    "    # Check number of files and file types\n",
    "    errors = set()\n",
    "    warnings = set()\n",
    "    invalid_sims = 0\n",
    "\n",
    "    \n",
    "    for nvt_dir in directory.rglob(\"NVT\"):\n",
    "        if nvt_dir.is_dir():\n",
    "\n",
    "            # All files in the NVT directory\n",
    "            all_files = [f for f in nvt_dir.iterdir() if f.is_file()]\n",
    "            total = len(all_files)\n",
    "\n",
    "            unique_key = nvt_dir.parent.name\n",
    "            # Check number of files matches expected number\n",
    "            if total != expected_number_of_dump_files_per_sim:\n",
    "                print(f\"{total} files found in {unique_key}\")\n",
    "                invalid_sims += 1\n",
    "\n",
    "            # Files that start with 'dump_custom'\n",
    "            unrecognized_files = [f for f in all_files if not f.name.startswith(\"dump_custom\")]\n",
    "            if unrecognized_files:\n",
    "                print(f\"Unrecognized files found in {unique_key}\")\n",
    "                print(unrecognized_files)\n",
    "                invalid_sims += 1\n",
    "    \n",
    "    # Check for ERROR and WARNING messages in .log files\n",
    "    for log_file in directory.rglob(\"*.log\"):\n",
    "        \n",
    "        has_error = False\n",
    "        has_warning = False\n",
    "\n",
    "        unique_key = log_file.parent.name \n",
    "        \n",
    "        with log_file.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                if \"ERROR\" in line:\n",
    "                    has_error = True\n",
    "                    break  # stop reading; error takes priority\n",
    "                elif \"WARNING\" in line:\n",
    "                    has_warning = True\n",
    "\n",
    "        # Decide classification\n",
    "        if has_error:\n",
    "            errors.add(unique_key)\n",
    "            invalid_sims += 1\n",
    "        elif has_warning:\n",
    "            warnings.add(unique_key)\n",
    "        \n",
    "\n",
    "    # --- Print results nicely ---\n",
    "    print(\"\\nüîç Log scan summary\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "\n",
    "    if errors:\n",
    "        print(f\"‚ùå Simulations displaying errors ({len(errors)}):\")\n",
    "        for e in sorted(errors):\n",
    "            print(f\"   - {e}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No simulations displaying errors.\")\n",
    "\n",
    "    if warnings:\n",
    "        print(f\"\\n‚ö†Ô∏è  Simulations displaying warnings ({len(warnings)}):\")\n",
    "        for w in sorted(warnings):\n",
    "            print(f\"   - {w}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No simulations displaying warnings.\")\n",
    "\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"Total invalid simulations: {invalid_sims}\")\n",
    "\n",
    "simulation_validator(\"/u/vld/scat9451/main_project/LAMMPS_simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- ANALYSIS SCRIPT---------------\n",
    "\n",
    "# This script searches the LAMMPS_simulations directory and \n",
    "# 1. Identifies simulations that have not yet been analysed\n",
    "# 2. Creates ovito files and renderings\n",
    "# 3. Runs the following analyses as functions of density :\n",
    "#   a) % of sp, sp2, sp3 environments \n",
    "#   c) Histogram of ring sizes for a given density\n",
    "#   d) Frequency plots for n-mem rings\n",
    "#   e) Radial Distribution Functions\n",
    "#   f) Potential Energy\n",
    "#   g) Bond Length\n",
    "\n",
    "# # -------- OPTIONAL WAIT FUNCTION TO ALLOW FOR AUTOMATED RUNNING ---------\n",
    "# import subprocess\n",
    "# import time\n",
    "# WAIT_TIME = 600  # seconds\n",
    "# USER = \"scat9451\"\n",
    "\n",
    "# while True:\n",
    "#     job_status = subprocess.run([\"qstat\", \"-u\", USER], capture_output=True, text=True)\n",
    "#     if not job_status.stdout.strip():  # empty means no jobs\n",
    "#         break\n",
    "#     print(\"Active jobs found - waiting ...\")\n",
    "#     time.sleep(WAIT_TIME)\n",
    "\n",
    "# print(\"No active jobs found - proceeding ...\")\n",
    "# # ---------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import ovito\n",
    "from ovito.io import import_file\n",
    "from ovito.modifiers import CreateBondsModifier, FindRingsModifier, CoordinationAnalysisModifier, ColorCodingModifier, BondAnalysisModifier\n",
    "from ovito.vis import Viewport, TachyonRenderer, ColorLegendOverlay, BondsVis\n",
    "from ovito.qt_compat import QtCore\n",
    "\n",
    "# ------ MAKE NEW DIRECTORIES ------\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ovito_dir = analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(exist_ok=True)\n",
    "\n",
    "structural_analysis_dir = analysis_dir / \"Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(exist_ok=True)\n",
    "# ----------------------------------\n",
    "\n",
    "# ------  IMPORT SIMULATION DATA ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dictionary sorted_imported_simulation_files = {unique_key: [sorted list of dump_file path objects]} \n",
    "# 3. This can be loaded like so: \n",
    "#   a) first item: unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "#   b) loop through all items: for unique_key, dump_files in imported_simulation_files.items():\n",
    "\n",
    "# NOTE: The unique_key is generated from the grandparent of the dumpfiles\n",
    "# This function expects the following file structure, \"dump_custom.C.00000\" regex and unique_key regex:\n",
    "#\n",
    "# <unique_key>/                     \n",
    "#         ‚îî‚îÄ‚îÄ NVT/                       \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00000.dat  \n",
    "#               ‚îú‚îÄ‚îÄ dump_custom.C.00001.dat\n",
    "root_directory = Path(\"/u/vld/scat9451/main_project/\")\n",
    "def import_simulation_data(directory):\n",
    "\n",
    "    dump_file_name = re.compile(r\"^dump_custom\\.C\\.(\\d+)\\.dat$\") # Dump file regex\n",
    "    unique_key_pattern = re.compile(r\"^[A-Za-z]+_[A-Za-z0-9]+_[A-Za-z]+_\\d+_\\d+(?:\\.\\d+)?_\\d+$\") # Unique key regex\n",
    "    \n",
    "    directory = Path(directory)\n",
    "\n",
    "    imported_simulation_files = defaultdict(list) # Imported files dictionary\n",
    "\n",
    "    imported_files_counter = 0\n",
    "    skipped_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "        \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = dump_file_name.match(path.name) # Enforce dump_file file naming\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        parent = path.parent\n",
    "        \n",
    "        if parent.name != \"NVT\": # Enforce NVT file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Parent directory for {path}, {parent} is not equal to NVT\")\n",
    "            continue\n",
    "\n",
    "        grandparent = parent.parent\n",
    "        if not unique_key_pattern.match(grandparent.name): # Enforce unique_key file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Invalid unique_key name format '{grandparent.name}'\")\n",
    "            continue\n",
    "        if not grandparent.name: # Protect against missing grandparent\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: No grandparent directory for {path}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        unique_key = grandparent.name\n",
    "        numeric_index = int(m.group(1))\n",
    "\n",
    "        imported_simulation_files[unique_key].append((numeric_index, path))\n",
    "        imported_files_counter += 1\n",
    "\n",
    "    # sort each list by numeric index and drop the numeric index in final structure\n",
    "    sorted_imported_simulation_files = {}\n",
    "    for key, items in imported_simulation_files.items():\n",
    "        items.sort(key=lambda pair: pair[0])  # sort by numeric_index\n",
    "        paths_sorted = [p for _, p in items]\n",
    "        sorted_imported_simulation_files[key] = paths_sorted\n",
    "\n",
    "    if imported_files_counter:\n",
    "        print(f\"Imported {imported_files_counter} dump files\")\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} dump files due to errors\")\n",
    "\n",
    "    return sorted_imported_simulation_files\n",
    "\n",
    "imported_simulation_files = import_simulation_data(\"LAMMPS_simulations\")\n",
    "print(f\"Imported {len(imported_simulation_files)} LAMMPS simulation files\")\n",
    "\n",
    "# Sets up an empty pipeline for each successive function to use\n",
    "def empty_ovito_pipeline(imported_simulation_files):\n",
    "\n",
    "    # Clear existing pipeline\n",
    "    for p in list(ovito.scene.pipelines):\n",
    "        p.remove_from_scene()\n",
    "\n",
    "    if not imported_simulation_files:\n",
    "        raise ValueError(\"No datafiles provided to empty_ovito_pipeline()\")\n",
    "    \n",
    "    # Load the first item in the dictionary \n",
    "    unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "\n",
    "    if not dump_file:\n",
    "        raise ValueError(f\"No dump files found for simulation '{unique_key}'\")\n",
    "    \n",
    "    pipeline = import_file(dump_file)\n",
    "    \n",
    "    return pipeline\n",
    "pipeline = empty_ovito_pipeline(imported_simulation_files)\n",
    "\n",
    "# Data visualisation in Ovito\n",
    "def ovito_analysis(data_dict, pipeline):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided to ovito_analysis()\")\n",
    "\n",
    "    # ------- ANALYSIS OF IMPORTED FILES ------------\n",
    "    # BUG: Image and video renderers error with: \n",
    "    # \"RuntimeError: Visual element 'Rings' reported an error:Failed to build non-periodic representation of periodic surface mesh. Periodic domain might be too small.\" if ring mod is included.\n",
    "\n",
    "    # Bond Modifier and Visuals \n",
    "    bond_modifier = CreateBondsModifier(cutoff=1.85)\n",
    "    bond_modifier.vis.width = 0.15\n",
    "    bond_modifier.vis.coloring_mode = BondsVis.ColoringMode.Uniform\n",
    "    bond_modifier.vis.color = (0.5, 0.5, 0.5)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Coordination Modifier and Colour Coding\n",
    "    pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff=1.85))\n",
    "    colour_coding_mod = ColorCodingModifier(property=\"Coordination\",start_value=1.0,end_value=4.0,gradient=ColorCodingModifier.Viridis(),discretize_color_map=True)\n",
    "    pipeline.modifiers.append(colour_coding_mod)\n",
    "\n",
    "    # Add to Scene\n",
    "    pipeline.add_to_scene()\n",
    "\n",
    "    # Viewing settings\n",
    "    vp = Viewport()\n",
    "    vp.type = Viewport.Type.Perspective\n",
    "\n",
    "    # Coordination Legend\n",
    "    legend = ColorLegendOverlay(\n",
    "        title = \"Coordination\",\n",
    "        modifier = colour_coding_mod,\n",
    "        alignment = QtCore.Qt.AlignmentFlag.AlignHCenter | QtCore.Qt.AlignmentFlag.AlignBottom,\n",
    "        orientation = QtCore.Qt.Orientation.Horizontal,\n",
    "        font_size = 0.1,\n",
    "        format_string = '%.0f' \n",
    "        )\n",
    "    vp.overlays.append(legend)\n",
    "\n",
    "    # Note: this function only renders for the first repeat \n",
    "    def is_run_1_(run_file_name):\n",
    "        return re.match(r\".*1$\", run_file_name) is not None\n",
    "\n",
    "    # Skipped/Created file counters\n",
    "    skipped_ovito_files_counter = 0\n",
    "    skipped_png_files_counter = 0\n",
    "    skipped_avi_files_counter = 0\n",
    "    created_ovito_files_counter = 0\n",
    "    created_png_files_counter = 0\n",
    "    created_avi_files_counter = 0\n",
    "    \n",
    "\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        tachyon = TachyonRenderer(shadows=False, direct_light_intensity=1.1)\n",
    "\n",
    "\n",
    "        if is_run_1_(unique_key): # Only does analysis for run_1_\n",
    "            \n",
    "            ovito_save_file = analysis_dir / f\"{unique_key}.ovito\"\n",
    "            \n",
    "            # Ovito File Existance-Checker\n",
    "            ovito_exists = any(analysis_dir.rglob(ovito_save_file.name))\n",
    "\n",
    "            if ovito_exists:\n",
    "                skipped_ovito_files_counter += 1\n",
    "                continue\n",
    "            \n",
    "            pipeline.source.load(dump_files)\n",
    "\n",
    "            # Set particle scaling (datafile specific)\n",
    "            n_frames = pipeline.source.num_frames\n",
    "            final_frame = max(0, n_frames - 1)\n",
    "            data = pipeline.compute(frame = final_frame)\n",
    "            data.particles.vis.scaling = 0.3\n",
    "\n",
    "            # Set Zoom\n",
    "            vp.zoom_all()\n",
    "\n",
    "            ovito.scene.save(ovito_save_file)\n",
    "            created_ovito_files_counter += 1   \n",
    "\n",
    "        if is_run_1_(unique_key): # Only does analysis for run_1_\n",
    "             \n",
    "            img_save_file = analysis_dir / f\"{unique_key}.png\"\n",
    "            img_save_file_str = str(img_save_file)\n",
    "            \n",
    "            # Ovito File Existance-Checker\n",
    "            img_exists = any(analysis_dir.rglob(img_save_file.name))\n",
    "\n",
    "            if img_exists:\n",
    "                skipped_png_files_counter += 1\n",
    "                continue\n",
    "            \n",
    "            pipeline.source.load(dump_files)\n",
    "\n",
    "            # Set particle scaling (datafile specific)\n",
    "            n_frames = pipeline.source.num_frames\n",
    "            final_frame = max(0, n_frames - 1)\n",
    "            data = pipeline.compute(frame = final_frame)\n",
    "            data.particles.vis.scaling = 0.3\n",
    "\n",
    "            # Set Zoom\n",
    "            vp.zoom_all()\n",
    "            \n",
    "            vp.render_image(size=(1920,1080),\n",
    "                            filename=img_save_file_str,\n",
    "                            background=(1,1,1),\n",
    "                            frame=final_frame,\n",
    "                            renderer=tachyon)\n",
    "            created_png_files_counter += 1 \n",
    "              \n",
    "\n",
    "        if is_run_1_(unique_key): # Only does analysis for run_1_\n",
    "            \n",
    "            vid_save_file   = analysis_dir / f\"{unique_key}.avi\"\n",
    "            vid_save_file_str = str(vid_save_file)                                     \n",
    "\n",
    "            # File Existance-Checker\n",
    "            vid_exists   = any(analysis_dir.rglob(vid_save_file.name))\n",
    "            if vid_exists:\n",
    "                skipped_avi_files_counter += 1\n",
    "                continue\n",
    "\n",
    "            pipeline.source.load(dump_files)\n",
    "\n",
    "            # Set particle scaling (datafile specific)\n",
    "            n_frames = pipeline.source.num_frames\n",
    "            final_frame = max(0, n_frames - 1)\n",
    "            data = pipeline.compute(frame = final_frame)\n",
    "            data.particles.vis.scaling = 0.3\n",
    "\n",
    "            # Set Zoom\n",
    "            vp.zoom_all()\n",
    "\n",
    "            vp.render_anim(size=(1920,1080), \n",
    "                        filename=vid_save_file_str, \n",
    "                        fps=10,\n",
    "                        renderer=tachyon)\n",
    "            created_avi_files_counter += 1  \n",
    "\n",
    "\n",
    "    # Print Skipped/Created files\n",
    "    if skipped_ovito_files_counter:\n",
    "        print(f\"Skipped {skipped_ovito_files_counter} existing .ovito files\")\n",
    "    if skipped_png_files_counter:\n",
    "        print(f\"Skipped {skipped_png_files_counter} existing .png files\")\n",
    "    if skipped_avi_files_counter:\n",
    "        print(f\"Skipped {skipped_avi_files_counter} existing .avi files\")\n",
    "    \n",
    "    if created_ovito_files_counter:\n",
    "        print(f\"Created {created_ovito_files_counter} .ovito files\")\n",
    "    if created_png_files_counter:\n",
    "        print(f\"Created {created_png_files_counter} .png files\")\n",
    "    if created_avi_files_counter:\n",
    "        print(f\"Created {created_avi_files_counter} .avi files\")\n",
    "    \n",
    "    # Remove modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "\n",
    "# ------ DATA GENERATION FUNCTIONS ------\n",
    "# file_analysis(): \n",
    "#   1. uses imported_simulation_files from import_simulation_data()\n",
    "#   2. uses the pipeline from empty_ovito_pipeline(): no modifiers by default\n",
    "#   3. checks if files already exist in \"Structural Analysis\"\n",
    "#   4. loads each file in datafiles into the existing pipeline\n",
    "#   5. computes a specified data object for the given pipeline on each file and saves to a file name given by the \"unique_key\" + \"data_tag\"\n",
    "#   NOTE:\n",
    "#       a) requires \"data_tag\": e.g. \"bond_length_data.txt\" or \"RDF_data.txt\" (include file suffix, e.g. \".txt\")\n",
    "#       b) \"data_function\" refers to the ovito function that return the desired data object \n",
    "#               e.g. \"data.particles['Coordination']\" or \"data.tables['coordination-rdf'].xy()\" or \"data.particles[\"c_pea\"]\" \n",
    "#       c) requires use of the \"lambda data:\" syntax for creating a throwaway function\n",
    "#               e.g. When calling this func, use \"file_analysis_and_existance_checker(datafiles,\"ring_data\",lambda data: data.tables[\"ring-size-histogram\"].xy())\"\"\n",
    "\n",
    "def file_analysis(data_dict, pipeline, data_tag, data_function):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided\")\n",
    "    \n",
    "    # Skipped file counter\n",
    "    skipped_files_counter = 0\n",
    "    created_files_counter = 0\n",
    "\n",
    "    # ----- STRUCTURAL ANALYSIS -----\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        # File Name\n",
    "        data_file_name = analysis_dir / f\"{unique_key}_{data_tag}\"\n",
    "\n",
    "        # Structural Analysis File Existance-Checker\n",
    "        data_exists = any(analysis_dir.rglob(data_file_name.name))\n",
    "        if data_exists and not REPLACE_OLD_FILES:\n",
    "            skipped_files_counter += 1\n",
    "            continue \n",
    "\n",
    "        # Load new file into the pipeline and compute data for final frame        \n",
    "        pipeline.source.load(dump_files)\n",
    "        n_frames = pipeline.source.num_frames\n",
    "        final_frame = max(0, n_frames - 1)\n",
    "        data = pipeline.compute(frame = final_frame)\n",
    "\n",
    "        # Data\n",
    "        specific_data = data_function(data)\n",
    "        np.savetxt(data_file_name, specific_data, delimiter=\",\", fmt=\"%.6f\")\n",
    "        created_files_counter += 1\n",
    "\n",
    "    # Print Skipped/Created Files\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} existing {data_tag} files\")\n",
    "    if created_files_counter:\n",
    "        print(f\"Created {created_files_counter} {data_tag} files\")    \n",
    "\n",
    "def list_attributes(pipeline):\n",
    "    data = pipeline.compute()\n",
    "    print(\"Per Particle Attributes:\")\n",
    "    for prop in data.particles.keys():\n",
    "        print(prop)\n",
    "\n",
    "def ring_analysis(data_dict, pipeline, min_ring_size, max_ring_size, bond_length):\n",
    "    \n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "    \n",
    "    # Ring Analysis Modifier\n",
    "    ring_mod = FindRingsModifier(minimum_ring_size=min_ring_size, maximum_ring_size=max_ring_size)\n",
    "    pipeline.modifiers.append(ring_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"ring.txt\", lambda data: data.tables[\"ring-size-histogram\"].xy())\n",
    "\n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "def coordination_analysis(data_dict, pipeline, coordination_cutoff):\n",
    "    \n",
    "    # Coordination Analysis Modfier\n",
    "    coord_mod = CoordinationAnalysisModifier(cutoff=coordination_cutoff)\n",
    "    pipeline.modifiers.append(coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"coordination.txt\", lambda data: data.particles['Coordination'])\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()\n",
    "    \n",
    "def energy_analysis(data_dict, pipeline):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"potential_energy.txt\", lambda data: data.particles[\"c_pea\"])\n",
    "\n",
    "def RDF_analysis(data_dict, pipeline, RDF_cutoff, bins):\n",
    "    \n",
    "    # Coordination Analysis Modfier for RDF\n",
    "    RDF_coord_mod = CoordinationAnalysisModifier(cutoff=RDF_cutoff, number_of_bins=bins)\n",
    "    pipeline.modifiers.append(RDF_coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"RDF.txt\", lambda data: data.tables['coordination-rdf'].xy())\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()    \n",
    "\n",
    "def bond_length_analysis(data_dict, pipeline, bins, bond_length, bond_length_analysis_cutoff):\n",
    "\n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Bond Analysis Modifier\n",
    "    bond_analysis_mod = BondAnalysisModifier(bins = bins, length_cutoff=bond_length_analysis_cutoff)\n",
    "    pipeline.modifiers.append(bond_analysis_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"bond_length.txt\", lambda data: data.tables[\"bond-length-distr\"].xy())\n",
    "  \n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    " \n",
    "def force_analysis(data_dict, pipeline):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"forces.txt\", lambda data: data.particles[\"Force\"])\n",
    "\n",
    "# ----- POSSIBLE ANALYSIS ------\n",
    "# 2. Bond Angle\n",
    "# 3. Conditional analysis (i.e. for sp/sp2/sp3 individually)\n",
    "# 4. Young's Modulus\n",
    "# 5. Coordination for each frame in a given sim plotted against simulation time\n",
    "# ------------------------------\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will regenerate ALL files (apart from renders)\n",
    "REPLACE_OLD_FILES = False\n",
    "\n",
    "if REPLACE_OLD_FILES:\n",
    "    confirm = input(\"Are you sure you want to replace old files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "#list_attributes(pipeline)\n",
    "# force_analysis(imported_simulation_files, pipeline)\n",
    "# bond_length_analysis(imported_simulation_files, pipeline, bins=1000, bond_length = 1.85, bond_length_analysis_cutoff=2.0)\n",
    "# RDF_analysis(imported_simulation_files, pipeline, RDF_cutoff=6.0, bins=200)\n",
    "# ring_analysis(imported_simulation_files, pipeline, min_ring_size=3, max_ring_size=24, bond_length=1.85)\n",
    "# coordination_analysis(imported_simulation_files, pipeline, coordination_cutoff=1.85)\n",
    "# energy_analysis(imported_simulation_files, pipeline)\n",
    "# ovito_analysis(imported_simulation_files, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FILE ORGANISER -----\n",
    "import re, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Assign Directories\n",
    "cwd = Path.cwd()\n",
    "\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "ovito_dir = analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "ovito_file_data_tags = [\".ovito\", \".png\", \".avi\"]\n",
    "structural_analysis_file_data_tags = [\"bond_length.txt\", \"coordination.txt\", \"potential_energy.txt\", \"RDF.txt\", \"ring.txt\", \"forces.txt\"]\n",
    "\n",
    "structural_analysis_dir = analysis_dir / \"Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "#structural_analysis_file_tags = [\"\"] <--- This may be worth implementing if more than 2 folders are used in the future\n",
    "\n",
    "# Regex pattern for reading \"unique_key\" + \"data_tag\"\"\n",
    "data_file_name = re.compile(\n",
    "    r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "    r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "    r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "    r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "    r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "    r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "    r'(?:_(?P<data_tag>.+)|(?P<data_tag2>\\..+))$'   # e.g. ring.txt or .png (allows underscore after run_number or .avi etc...)   \n",
    ")\n",
    "\n",
    "# General function for moving a directory with overwrite function\n",
    "def directory_move(directory, destination_dir):\n",
    "\n",
    "    if not directory.exists():\n",
    "        print(f\"ERROR: {directory} does not exist\")\n",
    "        return \"missing\"\n",
    "    \n",
    "    # Make Destination Directory\n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # New Path with existance check\n",
    "    moved_dir = destination_dir / directory.name\n",
    "    if moved_dir.exists() and not OVERWRITE:\n",
    "        return \"skipped\"\n",
    "\n",
    "    # Move Directory (with failsafe)\n",
    "    try:\n",
    "        shutil.move(str(directory), str(moved_dir))\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to move {directory} --> {moved_dir}: {e}\")\n",
    "        return \"failed\"\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "# Robust, general function for sorting all files using the unique_key and directory_move()\n",
    "def sort_directory(working_directory):\n",
    "\n",
    "    sorted_files = 0\n",
    "    skipped_files = 0\n",
    "    failed_files = 0\n",
    "    missing_files = 0\n",
    "    unrecognized_data_tags = 0\n",
    "\n",
    "    for file in working_directory.rglob(\"*\"): # searches working directory for directories contained in it\n",
    "        \n",
    "        directory = Path(file)\n",
    "        \n",
    "        if not directory.is_file(): # select for files only (data files)\n",
    "            continue\n",
    "\n",
    "        m = data_file_name.match(directory.name)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        # Parse each element of the unique key\n",
    "        element_symbol = m.group(\"element_symbol\")\n",
    "        if element_symbol == \"C\":\n",
    "            element_name = \"Carbon\"\n",
    "        else:\n",
    "            print(f\"Unrecognized element symbol for {directory}. Skipping file. \\nAdd element_symbol --> element_name mapping\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        potential_name = m.group(\"potential_name\")\n",
    "        simulation_type = m.group(\"simulation_type\")\n",
    "        num_atoms = int(m.group(\"num_atoms\"))\n",
    "        density = m.group(\"density\")\n",
    "        \n",
    "        data_tag =  (m.group(\"data_tag\") or m.group(\"data_tag2\") or \"\").lower()\n",
    "\n",
    "        # Defaults to not moving files\n",
    "        destination_dir = None\n",
    "        \n",
    "        # Moves ovito files into ovito_dir\n",
    "        if data_tag in ovito_file_data_tags:   \n",
    "            # Destination: evaluated using the unique key\n",
    "            destination_dir = (\n",
    "                ovito_dir\n",
    "                / f\"Element: {element_name}\"\n",
    "                / f\"Potential: {potential_name}\"\n",
    "                / f\"Type: {simulation_type}\"\n",
    "                / f\"Atoms: {num_atoms}\"\n",
    "                / f\"Density: {density}\"\n",
    "            )\n",
    "        \n",
    "        # Moves structural data files to structural_analysis_dir \n",
    "        elif data_tag in structural_analysis_file_data_tags:\n",
    "            # Destination: evaluated using the unique key\n",
    "            destination_dir = (\n",
    "                structural_analysis_dir\n",
    "                / f\"Element: {element_name}\"\n",
    "                / f\"Potential: {potential_name}\"\n",
    "                / f\"Type: {simulation_type}\"\n",
    "                / f\"Atoms: {num_atoms}\"\n",
    "                / f\"Density: {density}\"\n",
    "            )\n",
    "        \n",
    "        if destination_dir  is None:\n",
    "            unrecognized_data_tags +=1\n",
    "            continue\n",
    "        \n",
    "        status = directory_move(directory, destination_dir)\n",
    "        \n",
    "        if status == \"success\":\n",
    "            sorted_files += 1\n",
    "        elif status == \"skipped\":\n",
    "            skipped_files += 1\n",
    "        elif status == \"failed\":\n",
    "            failed_files += 1\n",
    "        elif status == \"missing\":\n",
    "            missing_files += 1\n",
    "    \n",
    "    if sorted_files:\n",
    "        print(f\"Sorted {sorted_files} files\")\n",
    "    if skipped_files:\n",
    "        print(f\"Skipped {skipped_files} existing files\")\n",
    "    if missing_files:\n",
    "        print(f\"{missing_files} missing files\")\n",
    "    if failed_files:\n",
    "        print(f\"{failed_files} failed files\")\n",
    "    if unrecognized_data_tags:\n",
    "        print(f\"{unrecognized_data_tags} unrecognized data tags\")\n",
    "\n",
    "    if not sorted_files and not skipped_files and not missing_files and not failed_files:\n",
    "        print(f\"No matching run files found in {working_directory}. Change run file regex if required.\")\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will replace ALL existing files \n",
    "OVERWRITE = False\n",
    "\n",
    "if OVERWRITE:\n",
    "    confirm = input(\"Are you sure you want to overwrite existing files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "# Organise ovito files and structural_analysis files\n",
    "sort_directory(analysis_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3a4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_GAP17_NVT_64_mean_forces_density_plot.png created\n",
      "C_GAP17_NVT_216_mean_forces_density_plot.png created\n"
     ]
    }
   ],
   "source": [
    "# ------ GRAPHICAL ANALYSIS --------\n",
    "\n",
    "# Graphical data points are means of all repeat runs with errors given as 1 standard deviation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create Graphical Analysis Directories\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "cwd = Path.cwd()\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_dir = analysis_dir / \"Graphical Analysis\"\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# ------ FIGURE FORMATTING ------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('1_column_fig.mplstyle')\n",
    "# -------------------------------\n",
    "\n",
    "# ------ IMPORT DATA FILES ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dictionary imported_data_files = {unique_data_key : [list of (density, Path), ...]} for all densities and runs (sorted by density)\n",
    "#   a) Where unique_data_key = \"unique_key (without density or repeat)\" + \"data_tag\"\n",
    "#   b) e.g. C_GAP17_NVT_64_ring.txt\n",
    "\n",
    "def import_data_files(directory, data_tag):\n",
    "    \n",
    "    directory = Path(directory)\n",
    "\n",
    "    # Regex pattern for reading \"unique_key\" + \"data_tag\"\"\n",
    "    data_file_name = re.compile(\n",
    "    r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "    r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "    r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "    r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "    r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "    r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "    r'(?:_(?P<data_tag>.+)|(?P<data_tag2>\\..+))$'   # e.g. ring.txt or .png (allows underscore after run_number or .avi etc...)   \n",
    "    )  \n",
    "\n",
    "    imported_data_files = defaultdict(list) # Imported files dictionary\n",
    "\n",
    "    skipped_data_files_counter = 0\n",
    "    imported_data_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "    \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = data_file_name.match(path.name) # Enforce data file naming\n",
    "        if not m:\n",
    "            print(f\"ERROR: Skipped {path}. Invalid data file name\")\n",
    "            skipped_data_files_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Parse \"unique_key\" + \"data_tag\" components\n",
    "        element_symbol    = m.group(\"element_symbol\")\n",
    "        potential_name    = m.group(\"potential_name\")\n",
    "        simulation_type   = m.group(\"simulation_type\")\n",
    "        num_atoms         = m.group(\"num_atoms\")\n",
    "        density           = m.group(\"density\")\n",
    "        run_number        = m.group(\"run\")\n",
    "        file_data_tag     = m.group(\"data_tag\")\n",
    "\n",
    "\n",
    "        if file_data_tag != data_tag:\n",
    "            continue\n",
    "        \n",
    "        # Construct unique_data_key\n",
    "        unique_data_key = f\"{element_symbol}_{potential_name}_{simulation_type}_{num_atoms}\"\n",
    "\n",
    "        # Append the density to the list\n",
    "        imported_data_files[unique_data_key].append((density, path))\n",
    "        imported_data_files_counter += 1\n",
    "\n",
    "    # Sort by density for each key\n",
    "    for key, items in imported_data_files.items():\n",
    "        try:\n",
    "            items_sorted = sorted(items, key=lambda pair: pair[0])\n",
    "            imported_data_files[key] = items_sorted\n",
    "        except Exception:\n",
    "            print(f\"ERROR: Failed to sort density list for {key}\")\n",
    "\n",
    "    return imported_data_files\n",
    "\n",
    "# ------ PLOTTING DATA ------\n",
    "# Plot_type: marker (with error bars), line (with shaded regions) \n",
    "def plot(plot_type, data, x_label, y_label, chart_title, save_path):\n",
    "\n",
    "    if plot_type not in existing_plot_types:\n",
    "        raise ValueError(f\"Unknown plot_type: {plot_type!r}\")\n",
    "\n",
    "    # Ensure Parent Directory Exists\n",
    "    save_path = Path(save_path)\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig, ax = plt.subplots()  # Local figure size for each plot\n",
    "    \n",
    "    x = data[\"density\"].values\n",
    "    mean = data[\"mean\"].values\n",
    "    std = data[\"std\"].values\n",
    "    \n",
    "    if plot_type == \"marker\":\n",
    "        ax.errorbar(x, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5)\n",
    "    elif plot_type == \"line\":\n",
    "        alpha_fill = 0.25\n",
    "        ax.plot(x, mean, label=\"Mean\")\n",
    "        ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "                \n",
    "    # Labels and Titles\n",
    "    ax.set_xlabel(f\"{x_label}\")\n",
    "    ax.set_ylabel(f'{y_label}')\n",
    "    ax.set_title(f\"{chart_title}\")\n",
    "\n",
    "    # Save Plot to Graphical Analysis\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "# ---------------------------\n",
    "\n",
    "# File Reader Function\n",
    "def file_reader(path):\n",
    "    \n",
    "    try:\n",
    "        data = np.loadtxt(path, delimiter=',')\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {path}; unable to load: {e}\")\n",
    "        return None\n",
    "                \n",
    "    # Failsafe incase there's no data in the file\n",
    "    if data.size == 0:\n",
    "        print(f\"No data found in {path}\")\n",
    "        return None\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Imports data using import_data_files(directory, data_tag)\n",
    "# Manipulates data using the unique_data_function\n",
    "# Must be called within other functionsusing a data_function\n",
    "def by_density_data_analysis(directory, data_tag, unique_data_function, save_file_name, chart_title, y_label):\n",
    "    \n",
    "    imported_data_files = import_data_files(directory, data_tag)\n",
    "    if not imported_data_files:\n",
    "        return None\n",
    "    \n",
    "    # Safety check to ensure not parsing paths when trying to name graphs\n",
    "    base = Path(save_file_name).name\n",
    "\n",
    "    # Loop over all unique keys\n",
    "    for unique_data_key, entries in imported_data_files.items():\n",
    "\n",
    "        results = [] # In the form: (density, unique_data_function_output)\n",
    "        \n",
    "        # Loop over all MD runs for different densities and repeats that match the unique key\n",
    "        for density, path in entries: \n",
    "                        \n",
    "            data = file_reader(Path(path))\n",
    "            if data is None:\n",
    "                continue\n",
    "        \n",
    "            # Performs data analysis function\n",
    "            try:\n",
    "                unique_data_function_output = unique_data_function(data)\n",
    "            \n",
    "            except Exception:\n",
    "                unique_data_function_output = None\n",
    "            if unique_data_function_output is None:\n",
    "                continue\n",
    "            \n",
    "            results.append((density, float(unique_data_function_output)))\n",
    "\n",
    "        if not results:\n",
    "            print(f\"Failed to analyse data for {unique_data_key}\")\n",
    "        \n",
    "        x_label = \"Density (g/cm¬≥)\"\n",
    "        \n",
    "        save_file_dir = graph_dir / Path(unique_data_key)\n",
    "        save_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        final_name = f\"{unique_data_key}_{base}\"\n",
    "        save_path = save_file_dir / final_name\n",
    "\n",
    "        df = pd.DataFrame(results, columns=[\"density\", \"unique_data_function_output\"])\n",
    "        agg_df = df.groupby(\"density\")[\"unique_data_function_output\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index() \n",
    "\n",
    "\n",
    "        plot(set_plot_type, agg_df,x_label, y_label, chart_title,save_path)\n",
    "        \n",
    "        print(f\"{final_name} created\")\n",
    "\n",
    "# Coordination analysis \n",
    "def coordination_analysis(directory, coordination_number):\n",
    "    \n",
    "    data_tag = \"coordination.txt\"\n",
    "\n",
    "    # Label coordination number \n",
    "    if coordination_number == 2:\n",
    "        env = \"sp\"\n",
    "        y_label = \"sp Carbon Proportion\"\n",
    "    elif coordination_number == 3:\n",
    "        env = \"sp2\"\n",
    "        y_label = \"sp2 Carbon Proportion\"\n",
    "    elif coordination_number == 4:\n",
    "        env = \"sp3\"\n",
    "        y_label = \"sp3 Carbon Proportion\"\n",
    "    else:\n",
    "        print(\"ERROR: Coordination number should be between 2 and 4\")\n",
    "        env = f\"{coordination_number}_coordinate\"\n",
    "        y_label == f\"{coordination_number} coordinate atoms\"\n",
    "        return\n",
    "   \n",
    "    imported_data_files = import_data_files(directory, data_tag)\n",
    "\n",
    "    if not imported_data_files:\n",
    "        return None\n",
    "\n",
    "    # Loop over all unique keys\n",
    "    for unique_data_key, entries in imported_data_files.items():\n",
    "\n",
    "        results = [] # In the form: (density, coordination_proportion)\n",
    "        \n",
    "        # Loop over all MD runs for different densities and repeats that match the unique key\n",
    "        for density, path in entries: \n",
    "                        \n",
    "            data = file_reader(path)\n",
    "            if data is None:\n",
    "                continue\n",
    "        \n",
    "            # Calculate coordination proportions\n",
    "            coordination_proportion = (np.count_nonzero(data == coordination_number) / data.size)\n",
    "            results.append((density, coordination_proportion))\n",
    "        \n",
    "        x_label = \"Density (g/cm¬≥)\"\n",
    "        chart_title = f\"Coordination vs. Density\"\n",
    "        \n",
    "        save_file_dir = graph_dir / Path(unique_data_key)\n",
    "        save_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_file_name = save_file_dir / f\"{env}_coordination_density_plot.png\"\n",
    "\n",
    "        df = pd.DataFrame(results, columns=[\"density\", \"coordination_proportions\"])\n",
    "        coordination_df = df.groupby(\"density\")[\"coordination_proportions\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index()  \n",
    "\n",
    "\n",
    "        plot(plot_type = set_plot_type, data = coordination_df,\n",
    "            x_label = x_label, y_label = y_label, \n",
    "            chart_title = chart_title,\n",
    "            save_file_name = save_file_name)\n",
    "\n",
    "# Ring Size analysis\n",
    "def ring_analysis(directory, ring_size):\n",
    "        \n",
    "    data_tag = \"ring.txt\"\n",
    "    chart_title = f\"Number of {ring_size} Membered Rings vs. Density\"\n",
    "    save_file_name = f\"{ring_size}_ring_density_plot.png\"\n",
    "    y_label = f\"{ring_size} Membered Rings\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def ring_function(data: np.ndarray):\n",
    "        try:\n",
    "\n",
    "            num_rings_of_ring_size = data[data[:, 0] == ring_size, 1][0]\n",
    "            if num_rings_of_ring_size.size:\n",
    "                return float(num_rings_of_ring_size)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        except Exception as exc:\n",
    "            return None\n",
    "    \n",
    "    by_density_data_analysis(directory, data_tag, ring_function, save_file_name, chart_title, y_label)\n",
    "   \n",
    "# Potential energy analysis \n",
    "def potential_energy_analysis(directory):\n",
    "\n",
    "    data_tag = \"potential_energy.txt\"\n",
    "\n",
    "    imported_data_files = import_data_files(directory, data_tag)\n",
    "\n",
    "    if not imported_data_files:\n",
    "        return None\n",
    "\n",
    "    # Loop over all unique keys\n",
    "    for unique_data_key, entries in imported_data_files.items():\n",
    "\n",
    "        results = [] # In the form: (density, mean PE)\n",
    "        \n",
    "        # Loop over all MD runs for different densities and repeats that match the unique key\n",
    "        for density, path in entries: \n",
    "                        \n",
    "            data = file_reader(path)\n",
    "            if data is None:\n",
    "                continue\n",
    "        \n",
    "            # Calculate PE\n",
    "            mean_potential_energy = np.mean(data)\n",
    "            results.append((density, mean_potential_energy))\n",
    "        \n",
    "        x_label = \"Density (g/cm¬≥)\"\n",
    "        chart_title = f\"Mean Potential Energy vs. Density\"\n",
    "        \n",
    "        save_file_dir = graph_dir / Path(unique_data_key)\n",
    "        save_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_file_name = save_file_dir / \"mean_potential_energy_density_plot.png\"\n",
    "\n",
    "        df = pd.DataFrame(results, columns=[\"density\", \"mean_potential_energy\"])\n",
    "        PE_df = df.groupby(\"density\")[\"mean_potential_energy\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index()  \n",
    "\n",
    "\n",
    "        plot(plot_type = set_plot_type, data = PE_df,\n",
    "            x_label = x_label, y_label = 'Mean Potential Energy (eV)', \n",
    "            chart_title = chart_title,\n",
    "            save_file_name = save_file_name)\n",
    "\n",
    "# Bond Length analysis\n",
    "def bond_length_analysis(directory):\n",
    "    \n",
    "    data_tag = \"bond_length.txt\"\n",
    "\n",
    "    imported_data_files = import_data_files(directory, data_tag)\n",
    "\n",
    "    if not imported_data_files:\n",
    "        return None\n",
    "\n",
    "    # Loop over all unique keys\n",
    "    for unique_data_key, entries in imported_data_files.items():\n",
    "\n",
    "        results = [] # In the form: (density, mean bond length)\n",
    "        \n",
    "        # Loop over all MD runs for different densities and repeats that match the unique key\n",
    "        for density, path in entries: \n",
    "                        \n",
    "            data = file_reader(path)\n",
    "            if data is None:\n",
    "                continue\n",
    "        \n",
    "            # Split columns\n",
    "            x = data[:, 0]   # bond length\n",
    "            y = data[:, 1]   # frequency\n",
    "\n",
    "            # Calculate mean bond length w.r.t. density\n",
    "            mean_bond_length = np.average(x, weights=y)\n",
    "            results.append((density, mean_bond_length))\n",
    "        \n",
    "        x_label = \"Density (g/cm¬≥)\"\n",
    "        chart_title = f\"Mean Bond Length vs. Density\"\n",
    "        \n",
    "        save_file_dir = graph_dir / Path(unique_data_key)\n",
    "        save_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_file_name = save_file_dir / \"mean_bond_length_density_plot.png\"\n",
    "\n",
    "        df = pd.DataFrame(results, columns=[(\"density\"), \"mean_bond_length\"])\n",
    "        bond_length_df = df.groupby(\"density\")[\"mean_bond_length\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index()   \n",
    "\n",
    "\n",
    "        plot(plot_type = set_plot_type, data = bond_length_df,\n",
    "            x_label = x_label, y_label = 'Mean Bond Length (√Ö)', \n",
    "            chart_title = chart_title,\n",
    "            save_file_name = save_file_name)\n",
    "\n",
    "# Force Analysis\n",
    "def force_analysis(directory):\n",
    "        \n",
    "    data_tag = \"forces.txt\"\n",
    "    chart_title = f\"Mean Force Magnitude vs. Density\"\n",
    "    save_file_name = \"mean_forces_density_plot.png\"\n",
    "    y_label = \"mean_forces_density_plot.png\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def force_function(data: np.ndarray):\n",
    "        try:\n",
    "\n",
    "            magnitude_of_force_vectors = np.linalg.norm(data, axis=1)\n",
    "            mean_force_magnitude = np.mean(magnitude_of_force_vectors)\n",
    "            if mean_force_magnitude.size:\n",
    "                return float(mean_force_magnitude)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        except Exception as exc:\n",
    "            return None\n",
    "    \n",
    "    by_density_data_analysis(directory, data_tag, force_function, save_file_name, chart_title, y_label)\n",
    "\n",
    "# ------ RDF/DENSITY ANALYSIS ------\n",
    "# Allows comparison of different densities\n",
    "# The RDF is given as the mean of all repeats\n",
    "# Shading represents the standard deviation\n",
    "# Note: this assumes the same x values in the RDFS for the data, which is only true\n",
    "# for the same number of atoms and bins --> specified in structure generator and earlier structural analysis\n",
    "# for different analysis types it takes the first value of densities/quench_times if there is more than one\n",
    "def RDF_density_analysis(directory, melt_time, quench_time, densities):\n",
    "       \n",
    "    data_name = \"RDF\"\n",
    "\n",
    "    per_density_dataframes = {}\n",
    "\n",
    "    for density in densities:\n",
    "        datafiles = import_data_files(directory, data_name, \"single_value\", density, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            continue\n",
    "\n",
    "        RDF_results = [] # in the form (density, r_array, g_r_array)\n",
    "\n",
    "        for file, file_density, _, _ in datafiles:\n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                RDF_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if RDF_data.size == 0:\n",
    "                continue\n",
    "            \n",
    "            r_array = RDF_data[:, 0]\n",
    "            g_r_array = RDF_data[:, 1]\n",
    "\n",
    "            RDF_results.append((file_density, r_array, g_r_array))\n",
    "            \n",
    "        # skip if no files found \n",
    "        if len(RDF_results) == 0:\n",
    "            continue\n",
    "\n",
    "        # Check that the r scales are the same across all repeats\n",
    "        ref_r = RDF_results[0][1] #first r scale\n",
    "        consistent_r = True\n",
    "        for _, r_arr, _ in RDF_results[1:]:\n",
    "            if not np.allclose(ref_r, r_arr, atol=1e-6):\n",
    "                print(f\"ERROR: RDF data not consistent in r scale ‚Äî skipping density {density}\")\n",
    "                consistent_r = False\n",
    "                break\n",
    "        if not consistent_r:\n",
    "            continue  # skip this density entirely\n",
    "\n",
    "        # Expand each tuple (density, r_array, g_r_array) into rows to allow for aggregation\n",
    "        rows = []\n",
    "        for d, r_arr, g_arr in RDF_results:\n",
    "            for r_val, g_val in zip(r_arr, g_arr):\n",
    "                rows.append((d, r_val, g_val))\n",
    "        \n",
    "        # Dataframe\n",
    "        df = pd.DataFrame(rows, columns=[\"density\", \"r\", \"g_r\"])\n",
    "        RDF_df = (df.groupby([\"density\", \"r\"])[\"g_r\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index())\n",
    "    \n",
    "        # Saved to the dictionary\n",
    "        per_density_dataframes[density] = RDF_df\n",
    "\n",
    "    if not per_density_dataframes:\n",
    "        print(\"ERROR: No data found for RDF plot\")\n",
    "        return\n",
    "\n",
    "    # Plot Data\n",
    "    # Shaded areas given by +- 1 standard deviation\n",
    "    def plot_RDFs(rdf_dict, title, alpha_fill=0.25):\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlabel(\"r (√Ö)\")\n",
    "        ax.set_ylabel(\"g(r)\")\n",
    "        ax.set_title(f\"{title}\")\n",
    "        \n",
    "        # sort densities for consistent legend order\n",
    "        densities_sorted = sorted(rdf_dict.keys())\n",
    "\n",
    "        for density in densities_sorted:\n",
    "            df = rdf_dict[density]\n",
    "            r = df['r'].values\n",
    "            mean = df['mean'].values\n",
    "            std = df['std'].values\n",
    "\n",
    "            ax.plot(r, mean, label=f\"{density:.2f} g/cm¬≥\")\n",
    "            ax.fill_between(r, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "        ax.legend(title=\"Density\")\n",
    "\n",
    "        # Save image\n",
    "        dens_str = \"-\".join(f\"{d:.2f}\" for d in densities)\n",
    "        save_file_name = f\"RDF_m{melt_time}_c{quench_time}_{dens_str}gcm.png\"\n",
    "        filepath = graph_dir / save_file_name\n",
    "        plt.savefig(filepath)\n",
    "        plt.close(fig)\n",
    "        print(f\"{filepath.name} created\")\n",
    "\n",
    "    plot_RDFs(per_density_dataframes, title=f\"Radial Distribution Function\\nMelt Time = {melt_time} fs, Quench Time = {quench_time} fs\")\n",
    "#-----------------------------------\n",
    "\n",
    "# ------ RING SIZE/ DENSITY HISTOGRAM ------\n",
    "# Allows comparison of different densities\n",
    "# Shading represents the standard deviation\n",
    "def ring_histogram_density_analysis(directory, melt_time, quench_time, densities):\n",
    "    \n",
    "    data_name = \"ring\"\n",
    "\n",
    "    per_density_dataframes = {}\n",
    "\n",
    "    for density in densities:\n",
    "        datafiles = import_data_files(directory, data_name, \"single_value\", density, melt_time, quench_time)\n",
    "\n",
    "        if not datafiles:\n",
    "            return None\n",
    "\n",
    "        ring_results = [] # in the form (density, ring_size_array, frequency_array) \n",
    "\n",
    "        for file, file_density, _, _ in datafiles:\n",
    "                    \n",
    "            # Failsafe incase files don't load\n",
    "            try:\n",
    "                ring_data = np.loadtxt(file, delimiter=',')\n",
    "            except Exception as e:\n",
    "                # skip bad files\n",
    "                print(f\"Skipping {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Failsafe incase there's no data in the file\n",
    "            if ring_data.size == 0:\n",
    "                continue\n",
    "            \n",
    "            ring_size_array = ring_data[:,0]\n",
    "            frequency_array = ring_data[:,1]\n",
    "\n",
    "            ring_results.append((file_density, ring_size_array, frequency_array))\n",
    "            \n",
    "        # skip if no files found \n",
    "        if len(ring_results) == 0:\n",
    "            continue\n",
    "\n",
    "        # Expand each tuple (density, ring_size_array, frequency_array) into rows to allow for aggregation\n",
    "        rows = []\n",
    "        for d, ring_size_arr, frequency_array in ring_results:\n",
    "            for r_val, f_val in zip(ring_size_arr, frequency_array):\n",
    "                rows.append((d, r_val, f_val))        \n",
    "\n",
    "        # Dataframe\n",
    "        df = pd.DataFrame(rows, columns=[\"density\", \"ring_size\", \"frequency\"])\n",
    "        ring_df = (df.groupby([\"density\", \"ring_size\"])[\"frequency\"].agg([\"mean\", \"std\"]).fillna(0.0).reset_index())\n",
    "        \n",
    "        # Saved to the dictionary\n",
    "        per_density_dataframes[density] = ring_df\n",
    "\n",
    "    if not per_density_dataframes:\n",
    "        print(\"ERROR: No data found for ring histogram plot\")\n",
    "        return\n",
    "\n",
    "    # Plot Data\n",
    "    # Shaded areas given by +- 1 standard deviation\n",
    "    def plot_ring_hist(rdf_dict, title, alpha_fill=0.25):\n",
    "        \n",
    "        fig, ax = plt.subplots() \n",
    "        ax.set_xlabel(\"Ring Size\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"{title}\")\n",
    "        \n",
    "        # sort densities for consistent legend order\n",
    "        densities_sorted = sorted(rdf_dict.keys())\n",
    "\n",
    "        for density in densities_sorted:\n",
    "            df = rdf_dict[density]\n",
    "            r = df['ring_size'].values\n",
    "            mean = df['mean'].values\n",
    "            std = df['std'].values\n",
    "\n",
    "            ax.errorbar(r, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5, label=f\"{density} g/cm¬≥\")\n",
    "\n",
    "            #ax.fill_between(r, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "        ax.legend(title=\"Density\")\n",
    "\n",
    "        # Save image\n",
    "        dens_str = \"-\".join(f\"{d:.2f}\" for d in densities)\n",
    "        save_file_name = f\"ring_histogram_m{melt_time}_c{quench_time}_{dens_str}gcm.png\"\n",
    "        filepath = graph_dir / save_file_name\n",
    "        plt.savefig(filepath)\n",
    "        plt.close(fig)\n",
    "        print(f\"{filepath.name} created\")\n",
    "\n",
    "    plot_ring_hist(per_density_dataframes, title=f\"Ring Size Histogram\\nMelt Time = {melt_time} fs, Quench Time = {quench_time} fs\")\n",
    "# ------------------------------------------\n",
    "\n",
    "# ------ ANALYSIS PARAMETERS ------\n",
    "existing_plot_types = [\"marker\", \"line\"]\n",
    "\n",
    "set_plot_type = \"line\"\n",
    "\n",
    "set_analysis_directory = \"Analysis/Structural Analysis\"\n",
    "\n",
    "set_coordination_number = 4\n",
    "set_ring_size = 6\n",
    "# ---------------------------------\n",
    "\n",
    "#coordination_analysis(directory=set_analysis_directory, coordination_number=set_coordination_number)\n",
    "\n",
    "#bond_length_analysis(directory=set_analysis_directory)\n",
    "\n",
    "#potential_energy_analysis(directory=set_analysis_directory)\n",
    "\n",
    "# ring_analysis(directory= set_analysis_directory, ring_size=set_ring_size)\n",
    "\n",
    "force_analysis(directory=set_analysis_directory)\n",
    "\n",
    "# #Multi density plots\n",
    "# ring_histogram_density_analysis('Analysis/Structural Analysis', \n",
    "#                                 melt_time=5000, quench_time=10000, densities=(2.0,3.0))\n",
    "\n",
    "# RDF_density_analysis('Analysis/Structural Analysis', \n",
    "#                      melt_time=5000, quench_time=10000, densities=(2.0,3.5,1.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structure_analyser_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
