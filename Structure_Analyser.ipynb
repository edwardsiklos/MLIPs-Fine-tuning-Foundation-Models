{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451afa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No simulations displaying errors.\n",
      "\n",
      "Simulations displaying warnings (37):\n",
      "   - C_GAP17_NVT_64_2.5_1\n",
      "   - C_GAP17_NVT_64_2.5_2\n",
      "   - C_GAP17_NVT_64_2.5_3\n",
      "   - C_GAP17_NVT_64_2.5_4\n",
      "   - C_GAP17_NVT_64_2.5_5\n",
      "   - C_GAP17_NVT_64_3.0_1\n",
      "   - C_GAP17_NVT_64_3.0_2\n",
      "   - C_GAP17_NVT_64_3.0_3\n",
      "   - C_GAP17_NVT_64_3.0_4\n",
      "   - C_GAP17_NVT_64_3.0_5\n",
      "   - C_GAP17_NVT_64_3.5_1\n",
      "   - C_GAP17_NVT_64_3.5_2\n",
      "   - C_GAP17_NVT_64_3.5_3\n",
      "   - C_GAP17_NVT_64_3.5_4\n",
      "   - C_GAP17_NVT_64_3.5_5\n",
      "   - C_mace-mp-0b3_NVT_216_1.5_1\n",
      "   - C_mace-mp-0b3_NVT_216_1.5_2\n",
      "   - C_mace-mp-0b3_NVT_216_1.5_3\n",
      "   - C_mace-mp-0b3_NVT_216_1.5_4\n",
      "   - C_mace-mp-0b3_NVT_216_1.5_5\n",
      "   - C_mace-mp-0b3_NVT_216_2.0_1\n",
      "   - C_mace-mp-0b3_NVT_216_2.0_2\n",
      "   - C_mace-mp-0b3_NVT_216_2.0_3\n",
      "   - C_mace-mp-0b3_NVT_216_2.0_4\n",
      "   - C_mace-mp-0b3_NVT_216_2.0_5\n",
      "   - C_mace-mp-0b3_NVT_216_2.5_1\n",
      "   - C_mace-mp-0b3_NVT_216_2.5_2\n",
      "   - C_mace-mp-0b3_NVT_216_2.5_3\n",
      "   - C_mace-mp-0b3_NVT_216_2.5_4\n",
      "   - C_mace-mp-0b3_NVT_216_2.5_5\n",
      "   - C_mace-mp-0b3_NVT_216_3.0_1\n",
      "   - C_mace-mp-0b3_NVT_216_3.0_2\n",
      "   - C_mace-mp-0b3_NVT_216_3.0_3\n",
      "   - C_mace-mp-0b3_NVT_216_3.0_4\n",
      "   - C_mace-mp-0b3_NVT_216_3.0_5\n",
      "   - C_mace-mp-0b3_NVT_216_3.5_2\n",
      "   - C_mace-mp-0b3_NVT_216_3.5_3\n",
      "Total invalid simulations: 0\n",
      "\n",
      "No missing simulations\n"
     ]
    }
   ],
   "source": [
    "# ------ SIMULATION VALIDATOR ------\n",
    "# Before running analysis it's important to check that all simulations have run as expected\n",
    "# 1. Check expected number of dump files\n",
    "# 2. Checks log files for ERRORs and WARNINGs\n",
    "# 3. Check that all expected simulations exist\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "# # -------- OPTIONAL WAIT FUNCTION ---------\n",
    "# import subprocess\n",
    "# import time\n",
    "# WAIT_TIME = 600  # seconds\n",
    "# USER = \"scat9451\"\n",
    "\n",
    "# while True:\n",
    "#     job_status = subprocess.run([\"qstat\", \"-u\", USER], capture_output=True, text=True)\n",
    "#     if not job_status.stdout.strip():  # empty means no jobs\n",
    "#         break\n",
    "#     print(\"Active jobs found - waiting ...\")\n",
    "#     time.sleep(WAIT_TIME)\n",
    "\n",
    "# print(\"No active jobs found - proceeding ...\")\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "expected_number_of_dump_files_per_sim = 96             # =(total number of timesteps + 1)/100\n",
    "expected_densities = [1.5,2.0,2.5,3.0,3.5]\n",
    "expected_runs = [1,2,3,4,5]\n",
    "\n",
    "# This expects file structure:\n",
    "# <unique_key>/                     \n",
    "#         └── NVT/                       \n",
    "#               ├── dump_custom.C.00000.dat  \n",
    "#               ├── dump_custom.C.00001.dat\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def simulation_validator(directory):\n",
    "\n",
    "    directory = Path(directory)  \n",
    "\n",
    "    # Check number of files and file types\n",
    "    errors = set()\n",
    "    warnings = set()\n",
    "    invalid_sims = 0\n",
    "\n",
    "    \n",
    "    for nvt_dir in directory.rglob(\"NVT\"):\n",
    "        if nvt_dir.is_dir():\n",
    "\n",
    "            # All files in the NVT directory\n",
    "            all_files = [f for f in nvt_dir.iterdir() if f.is_file()]\n",
    "            total = len(all_files)\n",
    "\n",
    "            unique_key = nvt_dir.parent.name\n",
    "            # Check number of files matches expected number\n",
    "            if total != expected_number_of_dump_files_per_sim:\n",
    "                print(f\"{total} files found in {unique_key}\")\n",
    "                invalid_sims += 1\n",
    "\n",
    "            # Files that start with 'dump_custom'\n",
    "            unrecognized_files = [f for f in all_files if not f.name.startswith(\"dump_custom\")]\n",
    "            if unrecognized_files:\n",
    "                print(f\"Unrecognized files found in {unique_key}\")\n",
    "                print(unrecognized_files)\n",
    "                invalid_sims += 1\n",
    "    \n",
    "    # Check for ERROR and WARNING messages in .log files\n",
    "    for log_file in directory.rglob(\"*.log\"):\n",
    "        \n",
    "        has_error = False\n",
    "        has_warning = False\n",
    "\n",
    "        unique_key = log_file.parent.name \n",
    "        \n",
    "        with log_file.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                if \"ERROR\" in line:\n",
    "                    has_error = True\n",
    "                    break  # stop reading; error takes priority\n",
    "                elif \"WARNING\" in line:\n",
    "                    has_warning = True\n",
    "\n",
    "        # Decide classification\n",
    "        if has_error:\n",
    "            errors.add(unique_key)\n",
    "            invalid_sims += 1\n",
    "        elif has_warning:\n",
    "            warnings.add(unique_key)\n",
    "\n",
    "    if errors:\n",
    "        print(f\"Simulations displaying errors ({len(errors)}):\")\n",
    "        for e in sorted(errors):\n",
    "            print(f\"   - {e}\")\n",
    "    else:\n",
    "        print(\"No simulations displaying errors.\")\n",
    "\n",
    "    if warnings:\n",
    "        print(f\"\\nSimulations displaying warnings ({len(warnings)}):\")\n",
    "        for w in sorted(warnings):\n",
    "            print(f\"   - {w}\")\n",
    "    else:\n",
    "        print(\"\\nNo simulations displaying warnings.\")\n",
    "\n",
    "    print(f\"Total invalid simulations: {invalid_sims}\")\n",
    "\n",
    "    \n",
    "    # ------ Missing Simulation Check -----\n",
    "    simulations = []\n",
    "\n",
    "    unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "\n",
    "    # Generate df of simulations\n",
    "    for unique_key in directory.rglob('*'):\n",
    "\n",
    "        if not unique_key.is_dir(): # must be a directory\n",
    "            continue\n",
    "\n",
    "        # Only matches unique_key dirs\n",
    "        m = unique_key_pattern.match(unique_key.name)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        element_symbol = m.group(1)\n",
    "        potential_name = m.group(2)\n",
    "        simulation_type = m.group(3)\n",
    "        num_atoms = m.group(4)\n",
    "        density = m.group(5)\n",
    "        run = m.group(6)\n",
    "\n",
    "        simulations.append((element_symbol, potential_name, simulation_type, num_atoms, density, run))\n",
    "\n",
    "    simulations_df = pd.DataFrame(\n",
    "                    simulations,\n",
    "                    columns=['element_symbol', 'potential_name', 'simulation_type', 'num_atoms', 'density', 'run'])\n",
    "\n",
    "    simulations_df = simulations_df.astype({\n",
    "    'num_atoms': 'int64',\n",
    "    'density': 'float64',\n",
    "    'run': 'int64'\n",
    "    })\n",
    "\n",
    "    # Generate expected dataframe\n",
    "    unique_sim_params_df = simulations_df[['element_symbol', 'potential_name', 'simulation_type', 'num_atoms']].drop_duplicates()\n",
    "    expected_sims_df = pd.DataFrame(columns=['element_symbol', 'potential_name', 'simulation_type', 'num_atoms', 'density', 'run'])\n",
    "    \n",
    "    for _, row in unique_sim_params_df.iterrows():\n",
    "        for density in expected_densities:\n",
    "            for run in expected_runs:\n",
    "                expected_sims_df.loc[len(expected_sims_df)] = {'element_symbol': row['element_symbol'],\n",
    "                                                                'potential_name': row['potential_name'],\n",
    "                                                                'simulation_type': row['simulation_type'],\n",
    "                                                                'num_atoms': row['num_atoms'],\n",
    "                                                                'density': density,\n",
    "                                                                'run': run}\n",
    "\n",
    "    expected_sims_df = expected_sims_df.astype({\n",
    "    'num_atoms': 'int64',\n",
    "    'density': 'float64',\n",
    "    'run': 'int64'\n",
    "    })\n",
    "    \n",
    "    # Find diffs between expected and found\n",
    "    diff = simulations_df.merge(expected_sims_df, how = 'outer', indicator=True)\n",
    "    missing_sims = diff[diff['_merge'] == 'right only']\n",
    "     \n",
    "    if missing_sims.empty:\n",
    "        print(\"\\nNo missing simulations\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nWARNING: Missing simulations based on expected number of runs and densities\")\n",
    "        print(f\"\\n{missing_sims}\")\n",
    "\n",
    "simulation_validator(\"LAMMPS_simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- ANALYSIS SCRIPT---------------\n",
    "\n",
    "# This script searches the LAMMPS_simulations directory and \n",
    "# 1. Identifies simulations that have not yet been analysed\n",
    "# 2. Creates ovito files and renderings\n",
    "# 3. Runs the following analyses as functions of density :\n",
    "#   a) % of sp, sp2, sp3 environments \n",
    "#   c) Histogram of ring sizes for a given density\n",
    "#   d) Frequency plots for n-mem rings\n",
    "#   e) Radial Distribution Functions\n",
    "#   f) Potential Energy\n",
    "#   g) Bond Length\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import ovito\n",
    "from ovito.io import import_file\n",
    "from ovito.modifiers import CreateBondsModifier, FindRingsModifier, CoordinationAnalysisModifier, ColorCodingModifier, BondAnalysisModifier\n",
    "from ovito.vis import Viewport, TachyonRenderer, ColorLegendOverlay, BondsVis\n",
    "from ovito.qt_compat import QtCore\n",
    "\n",
    "# ------ MAKE NEW DIRECTORIES ------\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ovito_dir = analysis_dir / \"Ovito\"\n",
    "ovito_dir.mkdir(exist_ok=True)\n",
    "\n",
    "structural_analysis_dir = analysis_dir / \"Structural Analysis\"\n",
    "structural_analysis_dir.mkdir(exist_ok=True)\n",
    "# ----------------------------------\n",
    "\n",
    "# ------  IMPORT SIMULATION DATA ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dictionary sorted_imported_simulation_files = {unique_key: [sorted list of dump_file path objects]} \n",
    "# 3. This can be loaded like so: \n",
    "#   a) first item: unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "#   b) loop through all items: for unique_key, dump_files in imported_simulation_files.items():\n",
    "\n",
    "# NOTE: The unique_key is generated from the grandparent of the dumpfiles\n",
    "# This function expects the following file structure, \"dump_custom.C.00000\" regex and unique_key regex:\n",
    "#\n",
    "# <unique_key>/                     \n",
    "#         └── NVT/                       \n",
    "#               ├── dump_custom.C.00000.dat  \n",
    "#               ├── dump_custom.C.00001.dat\n",
    "root_directory = Path(\"/u/vld/scat9451/main_project/\")\n",
    "def import_simulation_data(directory):\n",
    "\n",
    "    dump_file_name = re.compile(r\"^dump_custom\\.C\\.(\\d+)\\.dat$\") # Dump file regex\n",
    "    unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "     \n",
    "    directory = Path(directory)\n",
    "\n",
    "    imported_simulation_files = defaultdict(list) # Imported files dictionary\n",
    "\n",
    "    imported_files_counter = 0\n",
    "    skipped_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "        \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = dump_file_name.match(path.name) # Enforce dump_file file naming\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        parent = path.parent\n",
    "        \n",
    "        if parent.name != \"NVT\": # Enforce NVT file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Parent directory for {path}, {parent} is not equal to NVT\")\n",
    "            continue\n",
    "\n",
    "        grandparent = parent.parent\n",
    "    \n",
    "        if not unique_key_pattern.match(grandparent.name): # Enforce unique_key file naming\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: Invalid unique_key name format '{grandparent.name}'\")\n",
    "            continue\n",
    "\n",
    "        if not grandparent.name: # Protect against missing grandparent\n",
    "            skipped_files_counter += 1\n",
    "            print(f\"ERROR: No grandparent directory for {path}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        unique_key = grandparent.name\n",
    "        numeric_index = int(m.group(1))\n",
    "\n",
    "        imported_simulation_files[unique_key].append((numeric_index, path))\n",
    "        imported_files_counter += 1\n",
    "\n",
    "    # sort each list by numeric index and drop the numeric index in final structure\n",
    "    sorted_imported_simulation_files = {}\n",
    "\n",
    "    for key, items in imported_simulation_files.items():\n",
    "        items.sort(key=lambda pair: pair[0])  # sort by numeric_index\n",
    "        paths_sorted = [p for _, p in items]\n",
    "        sorted_imported_simulation_files[key] = paths_sorted\n",
    "\n",
    "    if imported_files_counter:\n",
    "        print(f\"Imported {imported_files_counter} dump files\")\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} dump files due to errors\")\n",
    "\n",
    "    return sorted_imported_simulation_files\n",
    "\n",
    "imported_simulation_files = import_simulation_data(\"LAMMPS_simulations\")\n",
    "print(f\"Imported {len(imported_simulation_files)} LAMMPS simulation files\")\n",
    "\n",
    "# Sets up an empty pipeline for each successive function to use\n",
    "def empty_ovito_pipeline(imported_simulation_files):\n",
    "\n",
    "    # Clear existing pipeline\n",
    "    for p in list(ovito.scene.pipelines):\n",
    "        p.remove_from_scene()\n",
    "\n",
    "    if not imported_simulation_files:\n",
    "        raise ValueError(\"No datafiles provided to empty_ovito_pipeline()\")\n",
    "    \n",
    "    # Load the first item in the dictionary \n",
    "    unique_key, dump_file = next(iter(imported_simulation_files.items()))\n",
    "\n",
    "    if not dump_file:\n",
    "        raise ValueError(f\"No dump files found for simulation '{unique_key}'\")\n",
    "    \n",
    "    pipeline = import_file(dump_file)\n",
    "    \n",
    "    return pipeline\n",
    "pipeline = empty_ovito_pipeline(imported_simulation_files)\n",
    "\n",
    "# Data visualisation in Ovito\n",
    "def ovito_analysis(data_dict, pipeline):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided to ovito_analysis()\")\n",
    "\n",
    "    # ------- ANALYSIS OF IMPORTED FILES ------------\n",
    "    # BUG: Image and video renderers error with: \n",
    "    # \"RuntimeError: Visual element 'Rings' reported an error:Failed to build non-periodic representation of periodic surface mesh. Periodic domain might be too small.\" if ring mod is included.\n",
    "\n",
    "    # Bond Modifier and Visuals \n",
    "    bond_modifier = CreateBondsModifier(cutoff=1.85)\n",
    "    bond_modifier.vis.width = 0.15\n",
    "    bond_modifier.vis.coloring_mode = BondsVis.ColoringMode.Uniform\n",
    "    bond_modifier.vis.color = (0.5, 0.5, 0.5)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Coordination Modifier and Colour Coding\n",
    "    pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff=1.85))\n",
    "    colour_coding_mod = ColorCodingModifier(property=\"Coordination\",start_value=1.0,end_value=4.0,gradient=ColorCodingModifier.Viridis(),discretize_color_map=True)\n",
    "    pipeline.modifiers.append(colour_coding_mod)\n",
    "\n",
    "    # Add to Scene\n",
    "    pipeline.add_to_scene()\n",
    "\n",
    "    # Viewing settings\n",
    "    vp = Viewport()\n",
    "    vp.type = Viewport.Type.Perspective\n",
    "\n",
    "    # Coordination Legend\n",
    "    legend = ColorLegendOverlay(\n",
    "        title = \"Coordination\",\n",
    "        modifier = colour_coding_mod,\n",
    "        alignment = QtCore.Qt.AlignmentFlag.AlignHCenter | QtCore.Qt.AlignmentFlag.AlignBottom,\n",
    "        orientation = QtCore.Qt.Orientation.Horizontal,\n",
    "        font_size = 0.1,\n",
    "        format_string = '%.0f' \n",
    "        )\n",
    "    vp.overlays.append(legend)\n",
    "\n",
    "    # Note: this function only renders for the first repeat \n",
    "    def is_run_1_(run_file_name):\n",
    "        return re.match(r\".*1$\", run_file_name) is not None\n",
    "\n",
    "    # Skipped/Created file counters\n",
    "    skipped_ovito_files_counter = 0\n",
    "    skipped_png_files_counter = 0\n",
    "    skipped_avi_files_counter = 0\n",
    "    created_ovito_files_counter = 0\n",
    "    created_png_files_counter = 0\n",
    "    created_avi_files_counter = 0\n",
    "    \n",
    "\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "\n",
    "        m = unique_key_pattern.match(unique_key)\n",
    "        if not m:\n",
    "            print(f\"Invalid unique_key name format: {unique_key}\")\n",
    "            \n",
    "        element_symbol = m.group(1)\n",
    "        potential_name = m.group(2)\n",
    "        simulation_type = m.group(3)\n",
    "        num_atoms = m.group(4)\n",
    "        density = m.group(5)\n",
    "        run = m.group(6)\n",
    "\n",
    "        if element_symbol == \"C\":\n",
    "            element = \"Carbon\"\n",
    "\n",
    "        # File Name\n",
    "        ovito_file_dir = ovito_dir / f\"Element: {element}\" / f\"Potential: {potential_name}\" / f\"Type: {simulation_type}\" / f\"Atoms: {num_atoms}\" / f\"Density: {density}\" \n",
    "        ovito_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        tachyon = TachyonRenderer(shadows=False, direct_light_intensity=1.1)\n",
    "\n",
    "        # Only does analysis for run_1_\n",
    "        if not is_run_1_(unique_key):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        ovito_save_file = ovito_file_dir / f\"{unique_key}.ovito\"\n",
    "        \n",
    "        # Ovito File Existance-Checker\n",
    "        ovito_exists = any(analysis_dir.rglob(ovito_save_file.name))\n",
    "\n",
    "        if ovito_exists:\n",
    "            skipped_ovito_files_counter += 1\n",
    "            continue\n",
    "        \n",
    "        pipeline.source.load(dump_files)\n",
    "\n",
    "        # Set particle scaling (datafile specific)\n",
    "        n_frames = pipeline.source.num_frames\n",
    "        final_frame = max(0, n_frames - 1)\n",
    "        data = pipeline.compute(frame = final_frame)\n",
    "        data.particles.vis.scaling = 0.3\n",
    "\n",
    "        # Set Zoom\n",
    "        vp.zoom_all()\n",
    "\n",
    "        ovito.scene.save(ovito_save_file)\n",
    "        created_ovito_files_counter += 1   \n",
    "\n",
    "        # Create Images    \n",
    "        img_save_file = ovito_file_dir / f\"{unique_key}.png\"\n",
    "        img_save_file_str = str(img_save_file)\n",
    "        \n",
    "        # Ovito File Existance-Checker\n",
    "        img_exists = any(analysis_dir.rglob(img_save_file.name))\n",
    "\n",
    "        if img_exists:\n",
    "            skipped_png_files_counter += 1\n",
    "            continue\n",
    "        \n",
    "        vp.render_image(size=(1920,1080),\n",
    "                        filename=img_save_file_str,\n",
    "                        background=(1,1,1),\n",
    "                        frame=final_frame,\n",
    "                        renderer=tachyon)\n",
    "        created_png_files_counter += 1 \n",
    "              \n",
    "        # Create Videos\n",
    "        vid_save_file   = ovito_file_dir / f\"{unique_key}.avi\"\n",
    "        vid_save_file_str = str(vid_save_file)                                     \n",
    "\n",
    "        # File Existance-Checker\n",
    "        vid_exists   = any(analysis_dir.rglob(vid_save_file.name))\n",
    "        if vid_exists:\n",
    "            skipped_avi_files_counter += 1\n",
    "            continue\n",
    "\n",
    "        vp.render_anim(size=(1920,1080), \n",
    "                    filename=vid_save_file_str, \n",
    "                    fps=10,\n",
    "                    renderer=tachyon)\n",
    "        created_avi_files_counter += 1  \n",
    "\n",
    "\n",
    "    # Print Skipped/Created files\n",
    "    if skipped_ovito_files_counter:\n",
    "        print(f\"Skipped {skipped_ovito_files_counter} existing .ovito files\")\n",
    "    if skipped_png_files_counter:\n",
    "        print(f\"Skipped {skipped_png_files_counter} existing .png files\")\n",
    "    if skipped_avi_files_counter:\n",
    "        print(f\"Skipped {skipped_avi_files_counter} existing .avi files\")\n",
    "    \n",
    "    if created_ovito_files_counter:\n",
    "        print(f\"Created {created_ovito_files_counter} .ovito files\")\n",
    "    if created_png_files_counter:\n",
    "        print(f\"Created {created_png_files_counter} .png files\")\n",
    "    if created_avi_files_counter:\n",
    "        print(f\"Created {created_avi_files_counter} .avi files\")\n",
    "    \n",
    "    # Remove modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "\n",
    "# ------ DATA GENERATION FUNCTIONS ------\n",
    "# file_analysis(): \n",
    "#   1. uses imported_simulation_files from import_simulation_data()\n",
    "#   2. uses the pipeline from empty_ovito_pipeline(): no modifiers by default\n",
    "#   3. checks if files already exist in \"Structural Analysis\"\n",
    "#   4. loads each file in datafiles into the existing pipeline\n",
    "#   5. computes a specified data object for the given pipeline on each file and saves to a file name given by the \"unique_key\" + \"data_tag\"\n",
    "#   NOTE:\n",
    "#       a) requires \"data_tag\": e.g. \"bond_length_data.txt\" or \"RDF_data.txt\" (include file suffix, e.g. \".txt\")\n",
    "#       b) \"data_function\" refers to the ovito function that return the desired data object \n",
    "#               e.g. \"data.particles['Coordination']\" or \"data.tables['coordination-rdf'].xy()\" or \"data.particles[\"c_pea\"]\" \n",
    "#       c) requires use of the \"lambda data:\" syntax for creating a throwaway function\n",
    "#               e.g. When calling this func, use \"file_analysis_and_existance_checker(datafiles,\"ring_data\",lambda data: data.tables[\"ring-size-histogram\"].xy())\"\"\n",
    "\n",
    "def file_analysis(data_dict, pipeline, data_tag, data_function):\n",
    "\n",
    "    if not data_dict:\n",
    "        raise ValueError(\"No datafiles provided\")\n",
    "    \n",
    "    # Skipped file counter\n",
    "    skipped_files_counter = 0\n",
    "    created_files_counter = 0\n",
    "\n",
    "    # ----- STRUCTURAL ANALYSIS -----\n",
    "    for unique_key, dump_files in data_dict.items():\n",
    "\n",
    "        unique_key_pattern = re.compile(\n",
    "        r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "        r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "        r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "        r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "        r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "        r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "        )\n",
    "\n",
    "        m = unique_key_pattern.match(unique_key)\n",
    "        if not m:\n",
    "            print(f\"Invalid unique_key name format: {unique_key}\")\n",
    "            \n",
    "        element_symbol = m.group(1)\n",
    "        potential_name = m.group(2)\n",
    "        simulation_type = m.group(3)\n",
    "        num_atoms = m.group(4)\n",
    "        density = m.group(5)\n",
    "        run = m.group(6)\n",
    "\n",
    "        if element_symbol == \"C\":\n",
    "            element = \"Carbon\"\n",
    "\n",
    "        # File Name\n",
    "        data_file_dir = structural_analysis_dir / f\"Element: {element}\" / f\"Potential: {potential_name}\" / f\"Type: {simulation_type}\" / f\"Atoms: {num_atoms}\" / f\"Density: {density}\" / f\"Run: {run}\"\n",
    "        data_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "        data_file_name = data_file_dir / f\"{unique_key}_{data_tag}\"\n",
    "\n",
    "        # Structural Analysis File Existance-Checker\n",
    "        data_exists = any(analysis_dir.rglob(data_file_name.name))\n",
    "        if data_exists and not REPLACE_OLD_FILES:\n",
    "            skipped_files_counter += 1\n",
    "            continue \n",
    "\n",
    "        # Load new file into the pipeline and compute data for final frame        \n",
    "        pipeline.source.load(dump_files)\n",
    "        n_frames = pipeline.source.num_frames\n",
    "        final_frame = max(0, n_frames - 1)\n",
    "        data = pipeline.compute(frame = final_frame)\n",
    "\n",
    "        # Data\n",
    "        specific_data = data_function(data)\n",
    "        np.savetxt(data_file_name, specific_data, delimiter=\",\", fmt=\"%.6f\")\n",
    "        created_files_counter += 1\n",
    "\n",
    "    # Print Skipped/Created Files\n",
    "    if skipped_files_counter:\n",
    "        print(f\"Skipped {skipped_files_counter} existing {data_tag} files\")\n",
    "    if created_files_counter:\n",
    "        print(f\"Created {created_files_counter} {data_tag} files\")    \n",
    "\n",
    "def list_attributes(pipeline):\n",
    "    data = pipeline.compute()\n",
    "    print(\"Per Particle Attributes:\")\n",
    "    for prop in data.particles.keys():\n",
    "        print(prop)\n",
    "\n",
    "def ring_analysis(data_dict, pipeline, min_ring_size, max_ring_size, bond_length):\n",
    "    \n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "    \n",
    "    # Ring Analysis Modifier\n",
    "    ring_mod = FindRingsModifier(minimum_ring_size=min_ring_size, maximum_ring_size=max_ring_size)\n",
    "    pipeline.modifiers.append(ring_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"ring.txt\", lambda data: data.tables[\"ring-size-histogram\"].xy())\n",
    "\n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    "\n",
    "def coordination_analysis(data_dict, pipeline, coordination_cutoff):\n",
    "    \n",
    "    # Coordination Analysis Modfier\n",
    "    coord_mod = CoordinationAnalysisModifier(cutoff=coordination_cutoff)\n",
    "    pipeline.modifiers.append(coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"coordination.txt\", lambda data: data.particles['Coordination'])\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()\n",
    "    \n",
    "def energy_analysis(data_dict, pipeline):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"potential_energy.txt\", lambda data: data.particles[\"c_pea\"])\n",
    "\n",
    "def RDF_analysis(data_dict, pipeline, RDF_cutoff, bins):\n",
    "    \n",
    "    # Coordination Analysis Modfier for RDF\n",
    "    RDF_coord_mod = CoordinationAnalysisModifier(cutoff=RDF_cutoff, number_of_bins=bins)\n",
    "    pipeline.modifiers.append(RDF_coord_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"RDF.txt\", lambda data: data.tables['coordination-rdf'].xy())\n",
    "\n",
    "    # Remove Modifier\n",
    "    pipeline.modifiers.pop()    \n",
    "\n",
    "def bond_length_analysis(data_dict, pipeline, bins, bond_length, bond_length_analysis_cutoff):\n",
    "\n",
    "    # Create Bonds Modifier\n",
    "    bond_modifier = CreateBondsModifier(cutoff=bond_length)\n",
    "    pipeline.modifiers.append(bond_modifier)\n",
    "\n",
    "    # Bond Analysis Modifier\n",
    "    bond_analysis_mod = BondAnalysisModifier(bins = bins, length_cutoff=bond_length_analysis_cutoff)\n",
    "    pipeline.modifiers.append(bond_analysis_mod)\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"bond_length.txt\", lambda data: data.tables[\"bond-length-distr\"].xy())\n",
    "  \n",
    "    # Remove Modifiers\n",
    "    pipeline.modifiers.pop()\n",
    "    pipeline.modifiers.pop()\n",
    " \n",
    "def force_analysis(data_dict, pipeline):\n",
    "\n",
    "    # No modifier required\n",
    "\n",
    "    # Analysis\n",
    "    file_analysis(data_dict, pipeline, \"forces.txt\", lambda data: data.particles[\"Force\"])\n",
    "\n",
    "# ----- POSSIBLE ANALYSIS ------\n",
    "# 2. Bond Angle\n",
    "# 3. Conditional analysis (i.e. for sp/sp2/sp3 individually)\n",
    "# 4. Young's Modulus\n",
    "# 5. Coordination for each frame in a given sim plotted against simulation time\n",
    "# ------------------------------\n",
    "\n",
    "# -----------------------\n",
    "# Use carefully - will regenerate ALL files (apart from renders)\n",
    "REPLACE_OLD_FILES = False\n",
    "\n",
    "if REPLACE_OLD_FILES:\n",
    "    confirm = input(\"Are you sure you want to replace old files? (y/n): \").strip().lower()\n",
    "    if confirm != \"y\":\n",
    "        REPLACE_OLD_FILES = False\n",
    "# -----------------------\n",
    "\n",
    "# list_attributes(pipeline)\n",
    "# force_analysis(imported_simulation_files, pipeline)\n",
    "# bond_length_analysis(imported_simulation_files, pipeline, bins=1000, bond_length = 1.85, bond_length_analysis_cutoff=2.0)\n",
    "# RDF_analysis(imported_simulation_files, pipeline, RDF_cutoff=6.0, bins=200)\n",
    "# ring_analysis(imported_simulation_files, pipeline, min_ring_size=3, max_ring_size=24, bond_length=1.85)\n",
    "# coordination_analysis(imported_simulation_files, pipeline, coordination_cutoff=1.85)\n",
    "# energy_analysis(imported_simulation_files, pipeline)\n",
    "ovito_analysis(imported_simulation_files, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a3a4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imported 257 coordination.txt files\n",
      "C_GAP17_NVT_216_coordination_sp.png created\n",
      "C_GAP17_NVT_64_coordination_sp.png created\n",
      "C_mace-mp-0b3_NVT_216_coordination_sp.png created\n",
      "C_NVT_216_coordination_sp_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 coordination.txt files\n",
      "C_GAP17_NVT_216_coordination_sp2.png created\n",
      "C_GAP17_NVT_64_coordination_sp2.png created\n",
      "C_mace-mp-0b3_NVT_216_coordination_sp2.png created\n",
      "C_NVT_216_coordination_sp2_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 coordination.txt files\n",
      "C_GAP17_NVT_216_coordination_sp3.png created\n",
      "C_GAP17_NVT_64_coordination_sp3.png created\n",
      "C_mace-mp-0b3_NVT_216_coordination_sp3.png created\n",
      "C_NVT_216_coordination_sp3_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 bond_length.txt files\n",
      "C_GAP17_NVT_216_bond_length.png created\n",
      "C_GAP17_NVT_64_bond_length.png created\n",
      "C_mace-mp-0b3_NVT_216_bond_length.png created\n",
      "C_NVT_216_bond_length_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 potential_energy.txt files\n",
      "C_GAP17_NVT_216_potential_energy.png created\n",
      "C_GAP17_NVT_64_potential_energy.png created\n",
      "C_mace-mp-0b3_NVT_216_potential_energy.png created\n",
      "C_NVT_216_potential_energy_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 ring.txt files\n",
      "C_GAP17_NVT_216_ring_5.png created\n",
      "C_GAP17_NVT_64_ring_5.png created\n",
      "C_mace-mp-0b3_NVT_216_ring_5.png created\n",
      "C_NVT_216_ring_5_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 ring.txt files\n",
      "C_GAP17_NVT_216_ring_6.png created\n",
      "C_GAP17_NVT_64_ring_6.png created\n",
      "C_mace-mp-0b3_NVT_216_ring_6.png created\n",
      "C_NVT_216_ring_6_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 ring.txt files\n",
      "C_GAP17_NVT_216_ring_7.png created\n",
      "C_GAP17_NVT_64_ring_7.png created\n",
      "C_mace-mp-0b3_NVT_216_ring_7.png created\n",
      "C_NVT_216_ring_7_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 forces.txt files\n",
      "C_GAP17_NVT_216_forces.png created\n",
      "C_GAP17_NVT_64_forces.png created\n",
      "C_mace-mp-0b3_NVT_216_forces.png created\n",
      "C_NVT_216_forces_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 RDF.txt files\n",
      "C_GAP17_NVT_216_1.5_RDF.png created\n",
      "C_GAP17_NVT_216_2.0_RDF.png created\n",
      "C_GAP17_NVT_216_2.5_RDF.png created\n",
      "C_GAP17_NVT_216_3.0_RDF.png created\n",
      "C_GAP17_NVT_216_3.5_RDF.png created\n",
      "C_GAP17_NVT_64_1.5_RDF.png created\n",
      "C_GAP17_NVT_64_2.0_RDF.png created\n",
      "C_GAP17_NVT_64_2.5_RDF.png created\n",
      "C_GAP17_NVT_64_3.0_RDF.png created\n",
      "C_GAP17_NVT_64_3.5_RDF.png created\n",
      "C_mace-mp-0b3_NVT_216_1.5_RDF.png created\n",
      "C_mace-mp-0b3_NVT_216_2.0_RDF.png created\n",
      "C_mace-mp-0b3_NVT_216_2.5_RDF.png created\n",
      "C_mace-mp-0b3_NVT_216_3.0_RDF.png created\n",
      "C_mace-mp-0b3_NVT_216_3.5_RDF.png created\n",
      "C_NVT_216_1.5_RDF_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_2.0_RDF_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_2.5_RDF_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_3.0_RDF_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_3.5_RDF_GAP17_mace-mp-0b3.png created\n",
      "\n",
      "Imported 257 ring.txt files\n",
      "C_GAP17_NVT_216_1.5_ring.png created\n",
      "C_GAP17_NVT_216_2.0_ring.png created\n",
      "C_GAP17_NVT_216_2.5_ring.png created\n",
      "C_GAP17_NVT_216_3.0_ring.png created\n",
      "C_GAP17_NVT_216_3.5_ring.png created\n",
      "C_GAP17_NVT_64_1.5_ring.png created\n",
      "C_GAP17_NVT_64_2.0_ring.png created\n",
      "C_GAP17_NVT_64_2.5_ring.png created\n",
      "C_GAP17_NVT_64_3.0_ring.png created\n",
      "C_GAP17_NVT_64_3.5_ring.png created\n",
      "C_mace-mp-0b3_NVT_216_1.5_ring.png created\n",
      "C_mace-mp-0b3_NVT_216_2.0_ring.png created\n",
      "C_mace-mp-0b3_NVT_216_2.5_ring.png created\n",
      "C_mace-mp-0b3_NVT_216_3.0_ring.png created\n",
      "C_mace-mp-0b3_NVT_216_3.5_ring.png created\n",
      "C_NVT_216_1.5_ring_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_2.0_ring_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_2.5_ring_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_3.0_ring_GAP17_mace-mp-0b3.png created\n",
      "C_NVT_216_3.5_ring_GAP17_mace-mp-0b3.png created\n"
     ]
    }
   ],
   "source": [
    "# ------ GRAPHICAL ANALYSIS --------\n",
    "\n",
    "# Graphical data points are means of all repeat runs with errors given as 1 standard deviation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create Graphical Analysis Directories\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "analysis_dir = cwd / \"Analysis\"\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_dir = analysis_dir / \"Graphical Analysis\"\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "single_plot_dir = graph_dir / \"Single Plots\"\n",
    "single_plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "potential_comparison_dir = graph_dir / \"Potential Comparison Plots\"\n",
    "potential_comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ------ FIGURE FORMATTING ------\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('1_column_fig.mplstyle')\n",
    "# -------------------------------\n",
    "\n",
    "# ------ IMPORT DATA FILES ------\n",
    "# 1. Searches recursively through the specified directory\n",
    "# 2. Creates a dataframe: \n",
    "#    imported_data_files_df, with columns:\n",
    "#        element, potential, simulation type, atoms, density, run number, data tag, file_data\n",
    "# 3. Only includes files with the specified \"data_tag\" \n",
    "\n",
    "def import_data_files(directory, data_tag):\n",
    "    \n",
    "    directory = Path(directory)\n",
    "\n",
    "    # Regex pattern for reading \"unique_key\" + \"data_tag\"\"\n",
    "    data_file_name = re.compile(\n",
    "    r'^(?P<element_symbol>[A-Za-z]{1,6})_'          # e.g. C\n",
    "    r'(?P<potential_name>[^_]+)_'                   # e.g. GAP17\n",
    "    r'(?P<simulation_type>[^_]+)_'                  # e.g. NVT\n",
    "    r'(?P<num_atoms>\\d+)_'                          # e.g. 64\n",
    "    r'(?P<density>[\\d.eE+-]+)_'                     # e.g. 1.5 or 1.85e+00\n",
    "    r'(?P<run>\\d+)'                                 # e.g. 1 (run number) \n",
    "    r'(?:_(?P<data_tag>.+)|(?P<data_tag2>\\..+))$'   # e.g. ring.txt or .png (allows underscore after run_number or .avi etc...)   \n",
    "    )  \n",
    "\n",
    "    imported_data_files_rows = []\n",
    "\n",
    "    skipped_data_files_counter = 0\n",
    "    imported_data_files_counter = 0\n",
    "\n",
    "    for path in directory.rglob(\"*\"):\n",
    "    \n",
    "        if not path.is_file(): # Filters for files not directories\n",
    "            continue\n",
    "\n",
    "        m = data_file_name.match(path.name) # Enforce data file naming\n",
    "        if not m:\n",
    "            print(f\"ERROR: Skipped {path}. Invalid data file name\")\n",
    "            skipped_data_files_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Parse \"unique_key\" + \"data_tag\" components\n",
    "        element_symbol    = m.group(\"element_symbol\")\n",
    "        potential_name    = m.group(\"potential_name\")\n",
    "        simulation_type   = m.group(\"simulation_type\")\n",
    "        num_atoms         = m.group(\"num_atoms\")\n",
    "        density           = m.group(\"density\")\n",
    "        run_number        = m.group(\"run\")\n",
    "        file_data_tag     = (m.group(\"data_tag\") or m.group(\"data_tag2\") or \"\").lstrip(\"._\")\n",
    "\n",
    "\n",
    "        # Only import files with the correct data_tag\n",
    "        if file_data_tag != data_tag:\n",
    "            continue\n",
    "        \n",
    "        # Load data from each file\n",
    "        try:\n",
    "            file_data = np.loadtxt(path, delimiter = ',')\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to load {path}: {e}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Check that data files aren't empty\n",
    "        if file_data is None or getattr(file_data, \"size\", 0) == 0:\n",
    "            print(f\"No data found in {path}\")\n",
    "            continue\n",
    "\n",
    "        imported_data_files_rows.append({\"element\": element_symbol, \"potential\" : potential_name,\n",
    "                                          \"simulation_type\": simulation_type, \"num_atoms\": num_atoms, \n",
    "                                          \"density\": density, \"run_number\": run_number, \n",
    "                                          \"data_tag\": file_data_tag, \"file_data\": file_data})\n",
    "        \n",
    "        imported_data_files_counter += 1\n",
    "        \n",
    "    imported_data_files_df = pd.DataFrame(imported_data_files_rows)\n",
    "\n",
    "    print()\n",
    "    if imported_data_files_counter:\n",
    "        print(f\"Imported {imported_data_files_counter} {data_tag} files\")\n",
    "    if skipped_data_files_counter:\n",
    "        print(f\"Skipped {skipped_data_files_counter} {data_tag} files\")\n",
    "    \n",
    "    return imported_data_files_df\n",
    "#-------------------------------\n",
    "\n",
    "# ------ DATA ANALYSIS ----------\n",
    "# 1. Imports data using import_data_files()\n",
    "# 2. If independent_var=False: independent variable is defaulted to float(density)\n",
    "#       a) Applies unique_data_function to file_data to generate scalar values\n",
    "#       b) Returns dataframe with scalar values in column: dep_var\n",
    "#       c) Transforms density column into column: ind_var\n",
    "#    If independent_var=True:\n",
    "#       a) Creates 2 columns from file_data: ind_var, dep_var (explodes arrays into scalar values)\n",
    "# 4. Takes the mean and std of dep_var for each run number\n",
    "# 5. Returns analysed_data_files_df with 'mean', 'std' columns for dep var\n",
    "# Note: Assumes no analysis required when both independent and dependant variables are imported\n",
    "\n",
    "def data_analysis(df, unique_data_function, independent_var):\n",
    "\n",
    "    # Analyse data files\n",
    "    analysed_data_files_df = df\n",
    "\n",
    "    # For independent_var=True, explode the arrays into scalar values \n",
    "    if independent_var:\n",
    "        # Split file_data into ind_var and dep_var\n",
    "        pieces = []\n",
    "        meta_cols = ['element','potential','simulation_type','num_atoms','density','data_tag','run_number']\n",
    "\n",
    "        for _, row in analysed_data_files_df.iterrows():\n",
    "            arr = np.asarray(row['file_data'])\n",
    "            if arr.size == 0:\n",
    "                continue  # skip empty arrays\n",
    "            if arr.ndim != 2 or arr.shape[1] != 2:\n",
    "                print(f\"ERROR: Expected Nx2 array for {row['data_tag']}, got {arr.shape}\")\n",
    "                return None\n",
    "            \n",
    "            # create DataFrame of individual points\n",
    "            piece = pd.DataFrame(arr, columns=['ind_var', 'dep_var'])\n",
    "            \n",
    "            # attach metadata\n",
    "            for k in meta_cols:\n",
    "                piece[k] = row[k]\n",
    "            \n",
    "            pieces.append(piece)\n",
    "\n",
    "        if not pieces:\n",
    "            print(f\"No valid arrays to explode for {analysed_data_files_df['data_tag']}\")\n",
    "            return None\n",
    "\n",
    "        # concatenate into one DataFrame\n",
    "        analysed_data_files_df = pd.concat(pieces, ignore_index=True)\n",
    "  \n",
    "    \n",
    "    else:\n",
    "        # Perform unique_data_function to generate scalar values\n",
    "        try:\n",
    "            analysed_data_files_df['dep_var'] = analysed_data_files_df['file_data'].apply(unique_data_function)\n",
    "             # Ind_var is a copy of density by default (formated as float)\n",
    "            analysed_data_files_df['ind_var'] = analysed_data_files_df['density'].copy().astype(float)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed unique_data_function for {analysed_data_files_df['data_tag']}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Find mean/std of each scalar value in dep_var\n",
    "    # Groups columns by simulation runs\n",
    "    grouping_keys = [\"element\",\"potential\",\"simulation_type\",\"num_atoms\",\"density\", \"data_tag\", \"ind_var\"]\n",
    "    grouped_runs = [c for c in grouping_keys if c in analysed_data_files_df.columns]\n",
    "    if not grouped_runs:\n",
    "        raise ValueError(\"No grouping keys found in imported DataFrame. Check import_data_files output.\")\n",
    "\n",
    "    analysed_data_files_df = analysed_data_files_df.groupby(grouped_runs).agg(mean=('dep_var', 'mean'),\n",
    "                                                                            std=('dep_var', 'std')\n",
    "                                                                            ).reset_index()\n",
    "\n",
    "    return analysed_data_files_df \n",
    "# ---------------------------------\n",
    "\n",
    "# ------ PLOT ------ \n",
    "key_analysis_densities = [1.5, 2.0, 2.5, 3.0, 3.5]\n",
    "# Set number of x axis ticks \n",
    "number_of_x_axis_ticks = len(key_analysis_densities) \n",
    "\n",
    "# Additional file tag: for adding sp/sp2/sp3 or 5/6/7 ring labels\n",
    "def single_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag):\n",
    "\n",
    "    def make_single_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path):\n",
    "\n",
    "            # Local figure size for each plot            \n",
    "            fig, ax = plt.subplots()  \n",
    "            x, mean, std = group[\"ind_var\"], group[\"mean\"], group[\"std\"]\n",
    "\n",
    "\n",
    "            if plot_type == \"marker\":\n",
    "                ax.errorbar(x, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5)\n",
    "            elif plot_type == \"line\":\n",
    "                alpha_fill = 0.25\n",
    "                ax.plot(x, mean, label=\"Mean\")\n",
    "                ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(number_of_x_axis_ticks))\n",
    "\n",
    "            ax.set(xlabel=x_label, ylabel=y_label, title=chart_title)\n",
    "\n",
    "            # Save Plot \n",
    "            fig.savefig(graph_save_path)\n",
    "            plt.close(fig)  # Close figure to free memory\n",
    "\n",
    "            print(f\"{graph_save_path.name} created\")\n",
    "\n",
    "\n",
    "    if independent_var:\n",
    "        # Group by unique_data_key including density for each plot\n",
    "        group_cols = ['element','potential','simulation_type','num_atoms', 'density', 'data_tag']\n",
    "        for (e,p,s,n,d,da), group in df.groupby(group_cols):\n",
    "            \n",
    "            # Only analyse key analysis densities\n",
    "            if float(d) not in key_analysis_densities:\n",
    "                continue\n",
    "\n",
    "            # Naming and folder\n",
    "            # File Name\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = single_plot_dir / f\"Element: {e}\" / f\"Potential: {p}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / f\"Plot Type: {da}\" \n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{d}_{da}.png\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{d}_{da}_{additional_file_tag}.png\"\n",
    "            \n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            make_single_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path)\n",
    "    \n",
    "    else:\n",
    "        # Group by unique_data_key excluding density for each vs. density plot\n",
    "        group_cols = ['element','potential','simulation_type','num_atoms','data_tag']\n",
    "        for (e,p,s,n,da), group in df.groupby(group_cols):\n",
    "\n",
    "            # Naming and folder\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = single_plot_dir / f\"Element: {e}\" / f\"Potential: {p}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / f\"Plot Type: Density\"\n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{da}.png\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{p}_{s}_{n}_{da}_{additional_file_tag}.png\" \n",
    "\n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            make_single_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path)\n",
    "   \n",
    "potential_comparison_list = [\"GAP17\", \"mace-mp-0b3\"]\n",
    "specific_potential_comparison_dir_name = \"_\".join(potential_comparison_list)        \n",
    "def potential_comparison_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag):\n",
    "\n",
    "    def make_comparison_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path):\n",
    "        for potential, subdf in group.groupby('potential'):\n",
    "\n",
    "            x, mean, std = subdf[\"ind_var\"],subdf[\"mean\"], subdf[\"std\"]\n",
    "\n",
    "            if plot_type == \"marker\":\n",
    "                ax.errorbar(x, mean, yerr=std,fmt='-o', capthick=0.5, elinewidth=0.5, label = potential)\n",
    "            elif plot_type == \"line\":\n",
    "                alpha_fill = 0.25\n",
    "                ax.plot(x, mean, label=potential)\n",
    "                ax.fill_between(x, mean - std, mean + std, alpha=alpha_fill)\n",
    "\n",
    "        ax.set(xlabel=x_label, ylabel=y_label, title=chart_title)\n",
    "        ax.legend()\n",
    "\n",
    "        # Save Plot \n",
    "        fig.savefig(graph_save_path)\n",
    "        plt.close(fig)  # Close figure to free memory\n",
    "        print(f\"{graph_save_path.name} created\")\n",
    "\n",
    "    potentials_required = set(potential_comparison_list)\n",
    "\n",
    "    if independent_var:\n",
    "\n",
    "        # Group by unique_data_key including density and potential for each plot\n",
    "        group_cols = ['element','simulation_type','num_atoms', 'density', 'data_tag']\n",
    "        for (e,s,n,d,da), group in df.groupby(group_cols):\n",
    "\n",
    "            # Only analyse key analysis densities\n",
    "            if float(d) not in key_analysis_densities: \n",
    "                continue\n",
    "\n",
    "            # Only compare plots if there is data for all specified densities\n",
    "            available = set(group['potential'].unique())\n",
    "            if not potentials_required.issubset(available):\n",
    "                continue\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(number_of_x_axis_ticks))\n",
    "\n",
    "            # Naming and folder\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = potential_comparison_dir / f\"Element: {e}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / specific_potential_comparison_dir_name / f\"Plot Type: {da}\"  \n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{d}_{da}_{specific_potential_comparison_dir_name}.png\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{d}_{da}_{additional_file_tag}_{specific_potential_comparison_dir_name}.png\"\n",
    "            \n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            make_comparison_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Group by unique_data_key excluding density and potential for each vs. density plot\n",
    "        group_cols = ['element','simulation_type','num_atoms','data_tag']\n",
    "        for (e,s,n,da), group in df.groupby(group_cols):\n",
    "\n",
    "            # Only compare plots if there is data for all specified densities\n",
    "            available = set(group['potential'].unique())\n",
    "            if not potentials_required.issubset(available):\n",
    "                continue\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(number_of_x_axis_ticks))\n",
    "            \n",
    "            # Naming and folder\n",
    "            da = Path(da).stem\n",
    "            graph_file_dir = potential_comparison_dir / f\"Element: {e}\" / f\"Type: {s}\" / f\"Atoms: {n}\" / specific_potential_comparison_dir_name / f\"Plot Type: Density\" \n",
    "            graph_file_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if not additional_file_tag:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{da}_{specific_potential_comparison_dir_name}.png\"\n",
    "            else:\n",
    "                graph_file_name = f\"{e}_{s}_{n}_{da}_{additional_file_tag}_{specific_potential_comparison_dir_name}.png\"\n",
    "            \n",
    "            graph_save_path = graph_file_dir / graph_file_name\n",
    "\n",
    "            make_comparison_plot(group, plot_type, x_label, y_label, chart_title, graph_save_path)\n",
    "# ------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Instructions: \n",
    "# 1. Assign data_tag, chat_title, save_file_name and y_label (RAW TEXT ONLY, NO PATHS)\n",
    "# 2. Create a function that computes a desired value per structure file and returns this value\n",
    "# 3. Call by_density_data_analysis, using your new function as its unique_data_function\n",
    "# 4. Set specify_density=True to plot for only 1 density (e.g. RDF, ring histogram)\n",
    "# 5. If specify_density = True, the unique data function MUST return: (independent_variable, dependent_variable)\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function Wrapper\n",
    "def import_analyse_plot(directory, data_tag, unique_data_function, \n",
    "                        plot_type, x_label, y_label, chart_title, \n",
    "                        independent_var, additional_file_tag):\n",
    "    \n",
    "    # Import Data Files\n",
    "    df = import_data_files(directory, data_tag)\n",
    "    if df.empty:\n",
    "        print(f\"No data files imported for data_analysis()\")\n",
    "        return None\n",
    "\n",
    "    # Analyse Data Files\n",
    "    df = data_analysis(df, unique_data_function, independent_var)\n",
    "\n",
    "    # Plot\n",
    "    single_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag)\n",
    "    potential_comparison_plot(df, plot_type, x_label, y_label, chart_title, independent_var, additional_file_tag)\n",
    "\n",
    "# Coordination analysis \n",
    "def coordination_analysis(directory, coordination_number):\n",
    "    \n",
    "    # Label coordination number \n",
    "    mapping = {\n",
    "        2: (\"sp\", \"sp Carbon Proportion\"),\n",
    "        3: (\"sp2\", \"sp2 Carbon Proportion\"),\n",
    "        4: (\"sp3\", \"sp3 Carbon Proportion\")\n",
    "    }\n",
    "    env, y_label = mapping.get(coordination_number, (None, None))\n",
    "\n",
    "    if env is None:\n",
    "        print(\"ERROR: Coordination number should be between 2 and 4\")\n",
    "        env = f\"{coordination_number}_coordinate\"\n",
    "        y_label = f\"{coordination_number} coordinate atoms\"\n",
    "        \n",
    "    data_tag = \"coordination.txt\"\n",
    "    chart_title = f\"Coordination vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    \n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def coord_function(data: np.ndarray):\n",
    "        return float((np.count_nonzero(data == coordination_number) / data.size))\n",
    "\n",
    "    import_analyse_plot(directory, data_tag, coord_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False, additional_file_tag= env)\n",
    "\n",
    "# Ring Size analysis\n",
    "def ring_analysis(directory, ring_size):\n",
    "        \n",
    "    data_tag = \"ring.txt\"\n",
    "    chart_title = f\"Number of {ring_size} Membered Rings vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = f\"{ring_size} Membered Rings\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def ring_function(data: np.ndarray):\n",
    "        return float(data[data[:, 0] == ring_size, 1][0])\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, ring_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False, additional_file_tag=ring_size)\n",
    "   \n",
    "# Potential energy analysis \n",
    "def potential_energy_analysis(directory):\n",
    "\n",
    "    data_tag = \"potential_energy.txt\"\n",
    "    chart_title = f\"Mean Potential Energy vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = 'Mean Potential Energy (eV)'\n",
    "    \n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def PE_function(data: np.ndarray):\n",
    "        return np.mean(data)\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, PE_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False, additional_file_tag=None)\n",
    "\n",
    "# Bond Length analysis\n",
    "def bond_length_analysis(directory):\n",
    "    \n",
    "    data_tag = \"bond_length.txt\"\n",
    "    chart_title = f\"Mean Bond Length vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = 'Mean Bond Length (Å)'\n",
    "\n",
    "    def bond_length_function (data: np.array):\n",
    "        return np.average(data[:, 0], weights=data[:, 1])\n",
    "\n",
    "    import_analyse_plot(directory, data_tag, bond_length_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False,additional_file_tag=None)\n",
    "\n",
    "# Force Analysis\n",
    "def force_analysis(directory):\n",
    "        \n",
    "    data_tag = \"forces.txt\"\n",
    "    chart_title = \"Mean Force Magnitude vs. Density\"\n",
    "    x_label = \"Density (g/cm³)\"\n",
    "    y_label = \"Mean Force Magnitude (eV/Å)\"\n",
    "\n",
    "    # unique_data_function must be a callable that takes the loaded numpy array and returns a scalar\n",
    "    def force_function(data: np.ndarray):\n",
    "        return np.mean(np.linalg.norm(data, axis=1))\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, force_function, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=False,additional_file_tag=None)\n",
    "\n",
    "# RDF Analysis\n",
    "def RDF_analysis(directory):\n",
    "    \n",
    "    data_tag = \"RDF.txt\"\n",
    "    chart_title = \"Radial Distribution Function\"\n",
    "    y_label = \"g(r)\"\n",
    "    x_label = \"r (Å)\"\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, None, \n",
    "                        \"line\", x_label, y_label, chart_title, \n",
    "                        independent_var=True,additional_file_tag=None)\n",
    "\n",
    "# Ring Histogram Analysis\n",
    "def ring_size_distribution_analysis(directory):\n",
    "    \n",
    "    data_tag = \"ring.txt\"\n",
    "    chart_title = \"Ring Size Distribution\"\n",
    "    y_label = \"Frequency\"\n",
    "    x_label = \"Ring Size\"\n",
    "    \n",
    "    import_analyse_plot(directory, data_tag, None, \n",
    "                        \"marker\", x_label, y_label, chart_title, \n",
    "                        independent_var=True,additional_file_tag=None)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# AttributeError: 'NoneType' object has no attribute 'groupby'\n",
    "# From mis-assigneding independent-var = False when it is actually true\n",
    "\n",
    "# ------ ANALYSIS PARAMETERS ------\n",
    "set_analysis_directory = \"Analysis/Structural Analysis\"\n",
    "\n",
    "for i in range (2,5):\n",
    "    coordination_analysis(directory=set_analysis_directory, coordination_number=i)\n",
    "\n",
    "bond_length_analysis(directory=set_analysis_directory)\n",
    "\n",
    "potential_energy_analysis(directory=set_analysis_directory)\n",
    "\n",
    "for i in range (5,8):\n",
    "    ring_analysis(directory= set_analysis_directory, ring_size=i)\n",
    "\n",
    "force_analysis(directory=set_analysis_directory)\n",
    "\n",
    "RDF_analysis(directory=set_analysis_directory)\n",
    "\n",
    "ring_size_distribution_analysis(directory=set_analysis_directory)\n",
    "\n",
    "# ---------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structure_analyser_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
